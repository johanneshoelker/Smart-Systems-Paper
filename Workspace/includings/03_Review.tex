\section{Representation Learning for Time Series Data}\label{review}
In this chapter the found literature is put into context. Starting with classical literature about the fundamental findings followed by actual trends in the Area of Representation Learning with different Representation Learning Strategies listed and compared.
\subsection{Historical view}
In this chapter the fundamental literature about Representation Learning is going to be discussed.
% Sensors and comparable applications produce values which vary over time. Sometimes the values vary in an unforeseen way and for a short time window they may be completely random. We have to step back and observe longer time periods.
% Sometimes it is possible for a human to see some patterns in the data when observing a long time window. In the measuring of a solar plant it is obvious that the sun rises and sets, depending on the voltage of the panels. Starting with 0 V at night the voltage is rising before noon, having its peak at midday and descending in the afternoon. This is one representation in the data. But there could be more represenations hidden, which are not likely to see. The shadow of a tree wandering over the panels happening every day or a one time event like the snow covering the plant. \\\\
Variations in data are not always visible for a human and even less possible to label them accordingly. Like \cite{bengio_representation_2013} mentioned it is important for artificial intelligence to detect these representations in data by machines. A machine should be able to extract information hidden in the low-level sensor measurings and continue working with the representations instead of the raw data. This is according to the paper the main requirement for a good representation, to be able using it as an input to a supervised predictor.

% TODO
Since the paper came out in 2013, several representation learning techniques were developed and some of them are directly applicable for time series data. In \cite{sun_survey_2021} the importance of machine learning in sensor data is emphasized. They summarize deep learning techniques on data-driven soft-sensors. Soft-sensors represent hard to measure variables by adapting available sensor data.% Their observation of industry processes is a rapidly changing field which demands data processing for a huge amount of data.
\subsection{Representation Learning Strategies}
The different RL strategies are listed, explained and compared. The strategies are organized by their underlying concept. We begin with straight-forward methods which are based on one concept and increase the complexity throughout the chapter. In the end methods which use combinations of different concepts are presented.
\subsubsection{MLP}
% INRAD
Using a simple Multi Layer Perceptron (MLP) is a straight-forward way to learn representations and to detect anomalies in time series data \cite{nielsen_neural_2015}. The input variable for the MLP are time points and the output variable represents the value at these time points. The model is trained to learn this mapping. With the trained model, the values in a live scenario are predicted and the difference to the actual values is measured. If this representation error exceeds a certain threshold, an anomaly is found \cite{jeong_time-series_2022}.
The model is trained with data including anomalies so it is not suitable for Zero-Shot Learning. It is theoretically possible to adapt the model for Zero-Shot Anomaly Detection but no further publications based on MLP are found.
\subsubsection{Contrastive Learning}
% Paper Debiased Contrastive Learning
Learning representations in time series data is done in a several different ways. One solution according to \citeA{zhang_debiased_2024} is debiased contrastive learning. By comparing pairs of data points and rating the similarities as distances between the two, contrastive learning gets less dependant on labeled data. The data can be more general and the extracted representations are more robust. The pairs of data points are labeled as positive and negative pairs with a distance according to their similarities. With this distance they are put into a feature space where they form groups of data points. To minimize the bias between representations multigranularity augmented view generation and expert knowledge are used during training. The proposed framework is applied on industrial fault detection. The two data sets consist of various vibration signals of industrial machines and stiction sensors. The effectiveness of the proposed framework is demonstrated through its application to these datasets, where it shows improved performance in fault detection compared to traditional methods \cite{zhang_debiased_2024}.

% CARLA
Contrastive Representation Learning is also used to tackle anomaly detection in time series data by \citeA{darban_carla_2024}. They use CL combined with synthetic anomaly injection. CL enables them to capture patterns in time series data and the framework shows good results on common real world datasets. Similar to the previous paper, dissimilar pairs, the anomalies, build distant data points and similar data points are close to each other. In order to train the model artificial anomalies are injected which build distant pairs. In the next stage the classification is done by  the proximity of the neighbours in the representation space. Additionally anchor points representing the nearest and furthest neighbour are given from each representation. Their methodology is called CARLA and is also not tested for Zero-Shot Learning.%TODO implementation

% CL-TAD
The article by \citeA{ngu_cl-tad_2023} introduces CL-TAD, a novel method for time series anomaly detection that leverages contrastive learning and reconstruction-based techniques to address the challenges of temporal dynamics, label scarcity, and data diversity in real-world applications. The method comprises two main components: positive sample generation and contrastive-learning-based representation learning. Positive samples are generated by reconstructing masked parts of the time series data, helping the model learn the underlying normal patterns. These samples, along with the original data, are then fed into a contrastive learning framework, which contrasts pairs of similar (positive) and dissimilar (negative) samples to learn robust representations. This process helps the model map similar data points closer together in the feature space while distancing dissimilar points, making it easier to detect deviations indicative of anomalies. Extensive experiments on nine benchmark datasets show that CL-TAD outperforms ten other recent methods in detecting anomalies, highlighting its effectiveness in handling diverse and complex time series data \cite{ngu_cl-tad_2023}.
While CL-TAD is not explicitly designed as a zero-shot learning method, its use of contrastive learning and reconstruction-based techniques suggests that it could have potential in zero-shot anomaly detection scenarios. However, this would depend on the model's ability to generalize from the learned normal patterns to detect unseen anomalies. Further empirical studies would be needed to validate its performance in zero-shot learning scenarios.

%
To succeed on Zero-Shot Anomaly Detection, the method of One-Class Classification (OCC) can solve the problem. %TODO Source
By gathering all "normal" values into a single class the outliers are directly detected if they are outside of it. The paper by \citeA{lee_time_2023} presents an approach for detecting anomalies using OCC in industrial time series data, which typically lacks labels for supervised learning. Thecombine OCC with contrastive learning to define a new objective function that can simultaneously learn from both models. This method enhances feature extraction while preserving temporal characteristics. The paper demonstrates the method's effectiveness through high anomaly detection performance on datasets with similar normal and anomalous data forms, highlighting its potential in industrial applications \cite{lee_time_2023}.

% TS2Vec
\citeA{yue_ts2vec_2022} introduce TS2Vec, a framework for learning robust and universal time series representations at multiple semantic levels through hierarchical contrastive learning. This approach utilizes timestamp masking and random cropping to create augmented context views, enhancing position-agnostic and comprehensive representations. By combining instance-wise and temporal contrastive losses, TS2Vec captures unique characteristics of different time series instances and dynamic temporal patterns within each series. Extensive experiments show that TS2Vec outperforms state-of-the-art methods in classification, forecasting, and anomaly detection tasks across 125 UCR and 29 UEA datasets, achieving average improvements of 2.4\% and 3.0\% in classification accuracy, respectively. The framework's efficiency in training time further underscores its practical utility. TS2Vec demonstrates its versatility by excelling in multiple time series analysis tasks, making it a significant contribution to the field. The framework's hierarchical contrastive learning at various scales encapsulates rich and meaningful patterns in time series data \cite{yue_ts2vec_2022}.

% Anomaly Clip
The ANOMALY CLIP method developed by \citeA{zhou_anomalyclip_2024} is designed for anomaly detection in images using CLIP, which maps images and text into a shared space to identify anomalies without prior examples. It works well for spatial data like images, but adapting it to time series data would be challenging because time series involve temporal dynamics, requiring models that capture patterns over time, like LSTMs or TCNs. CLIP is trained on visual and textual data, making it less suitable for handling sequences. The prompt-based approach, integral to ANOMALY CLIP, relies on visual-textual comparisons, which don't directly translate to time series. However, the concept of zero-shot, object-agnostic anomaly detection could inspire similar methods for time series, possibly using different embedding techniques and prediction models. Direct application to time series data is impractical, but analogous approaches could be developed with significant adjustments. Overall, aNOMALY CLIP is not directly applicable to time series but offers ideas that could be adapted \cite{zhou_anomalyclip_2024}

% TimeAutoAD
Another paper by \citeA{jiao_timeautoad_2022} introduces an autonomous system for anomaly detection in multivariate time series data using Contrastive Learning. The proposed TimeAutoAD automates model configuration and hyperparameter optimization, addressing challenges such as limited labeled anomaly data. It uses self-supervised contrastive learning to enhance the model's ability to differentiate normal and anomalous time series by generating pseudo-negative samples. The method is tested on real-world datasets, demonstrating improved performance over existing anomaly detection techniques, especially in scenarios where training data may be contaminated \cite{jiao_timeautoad_2022}.

The paper does not explicitly address zero-shot anomaly detection, which refers to detecting anomalies without having seen any labeled anomalies during training. However, the method is designed to function in a self-supervised manner, meaning it generates pseudo-negative samples from normal data to train the model, which suggests that it could be applied to scenarios where anomaly labels are not available. This approach allows the model to differentiate between normal and abnormal behaviors based on the learned representations, potentially making it useful for zero-shot anomaly detection in multivariate time series data. Nonetheless, further validation would be needed to confirm its effectiveness specifically for zero-shot tasks.
\subsubsection{Autoencoder}
%UAE-TENN
\citeA{nivarthi_multi-task_2023} are the first to use a Unified Autoencoder (UAE) for time series data, namely the power forecast of wind and solar plants. They contribute to the challenge of predicting the possible outcome of renewable energy in a newly created plant, either wind or solar. To do so a UAE is combined with a Task Embedding Neural Network (TENN) They examine the usability divided in Single-Task, Multi-Task and Zero-Shot Learning. The method was first published in \citeA{nivarthi_unified_2022}. It is then extended by convolutional layers instead of the fully connected neural network layers (UCAE-TENN) and also Long Short-Term Memory layers (ULAE-TENN).

% MAEDAY
Realising few-shot anomaly detection of images is done by \citeA{schwartz_maeday_2024}. The method MAEDAY can detect objects newly added to the frames. To achieve this a masked autoencoder is used who recreates the former image but without the anomaly. The difference between the initial and reconstructed images is calculated and the object then visible.
This method is useful for its ability to detect anomalies with very few examples, making it a powerful tool in scenarios where labeled data is rare. \citeA{schwartz_maeday_2024} demonstrate the effectiveness of MAEDAY in various applications, showcasing its potential for real-world anomaly detection tasks.

%
To detect anomalies in healthcare data a variational recurrent autoencoder is used by \citeA{pereira_learning_2019}. % TODO check if correct
The focus is on electrocardiogram (ECG) datasets. Their method tackles the challenge of finding anomalies in unlabelled time series data. They created an unsupervised framework where the model learns to represent the data and detect anomalies without needing labeled examples.
The VRAE model works by learning to reconstruct the input sequences. During training, they add noise to the input data, and the model tries to reconstruct the original, uncorrupted data. This helps the model learn more robust representations of the data. To detect anomalies, they cluster these learned representations and use the Wasserstein distance to identify outliers. Their approach was tested on the ECG5000 dataset and showed that it could effectively detect unusual heartbeats, performing better than previous methods that required labeled data.

%
Another approach using VRAE involves creating synthetic anomalies to improve the detection process. In their method, they use a two-level hierarchical latent space representation. First, they distill feature descriptors of normal data points into more robust representations using autoencoders (AEs). These representations are then refined using a variational autoencoder (VAE) that creates a family of distributions. From these distributions, they select those that lie on the outskirts of the normal data as generators of synthetic anomalies.
By generating these synthetic anomalies, they train binary classifiers to distinguish between normal and abnormal data. Their hierarchical structure for feature distillation and fusion helps create robust representations, enabling effective anomaly detection without needing actual anomalies during training. Their method performs well on several benchmarks for anomaly detection  \cite{ramirez_rivera_anomaly_2022}.

%
\citeA{pranavan_contrastive_2022} present a novel approach for anomaly detection in multi-variate time series data using Contrastive Predictive Coding (CPC). Their method, named Time-series Representational Learning through Contrastive Predictive Coding (TRL-CPC), aims to effectively capture the temporal dependencies and correlations across multiple variables in time series data.
The TRL-CPC framework consists of an encoder, an auto-regressive model, and a non-linear transformation model. These components are jointly optimized to learn the representations of multi-variate time series data by predicting future segments from past segments. The core idea is to maximize the mutual information between the encoded representations of past and future segments, thereby learning robust representations.
To detect anomalies, TRL-CPC calculates the prediction error between actual future segments and the predicted segments generated by the CPC model. Anomalies are identified where this prediction error exceeds a certain threshold, enabling unsupervised anomaly detection based on the structure of the data itself.
Experimental results show that TRL-CPC outperforms traditional anomaly detection methods on several benchmark datasets, highlighting its effectiveness in capturing complex temporal dependencies and identifying anomalies in multi-variate time series data \cite{pranavan_contrastive_2022}.

\citeA{zhang_debiased_2024} point out, that AE-based methods have remaining challenges.
\subsubsection{Stochastic Recurrent Neural Network}
\citeA{su_robust_2019} propose a method called OmniAnomaly for robust anomaly detection in multivariate time series data using a Stochastic Recurrent Neural Network (SRNN). This approach addresses the challenge of detecting anomalies in complex, high-dimensional time series data, which is common in applications such as network monitoring, industrial systems, and healthcare.
Their method utilizes an SRNN to model the temporal dependencies and stochasticity in multivariate time series data. By incorporating stochastic units into the recurrent neural network, the model can capture the underlying uncertainty and variability in the data. This allows for more accurate detection of anomalies, as the model can differentiate between normal fluctuations and genuine anomalies.
The key advantage of this method is its robustness to noisy and high-dimensional data. The SRNN learns to represent the normal patterns in the time series and identifies deviations from these patterns as anomalies. The model is evaluated on several benchmark datasets and demonstrates superior performance compared to state-of-the-art methods in terms of both precision and recall \cite{su_robust_2019}.
\subsubsection{Transformer}
% MOMENT
To overcome the challenge of poorly available time series data sets \cite{ma_survey_2023}, the model family MOMENT tries to learn general patterns on a pile of time series data \cite{goswami_moment_2024}. The pile is a collection of different datasets which they assembled for their pretraining. According to the paper minimal finetuning is needed to perform well on time series tasks like anomaly detection. They published the model and made the usage easily accessible with its own python library. The constructed time serie pile consists of a widespread list of domains including weather measurements, sensor values and power consumption datasets. They also included data not connected with the previous like the tongue and finger movement of humans. The different tasks which the model is evaluated on are forecasting (long and short horizon), classification, anomaly detection and imputation. Except for short-horizon forecasting all tasks are managed well.

% LLMs
Based on the transformer architecture, Large Language Models (LLM) are developed and used increasingly in different applications. Known as chatbots they can help in language specific tasks. Beside that they can be used in anomaly detection and forecasting. \citeA{su_large_2024} examine a literature review on how LLMs perform on anomaly detection tasks concerning time series data. LLMs in anomaly detection are specifically useful when the time series data is in the form of words. THis can be the case in log analysis. Logs are generated over time and hold a lot of information which can singify errors and system failures. They conclude that LLMs have potential in detecting anomalies but challenges remain. The occurrence of hallucinations and the need for computational efficiency to name a few \cite{su_large_2024}.

A method detecting anomalies in tabular data is presented by \citeA{li_anomaly_2024}. In order to perform tasks that LLMs are not directly build for they generate synthetic datasets. Using these datasets LLMs and specifically GPT-4 have comparable performance with transductive learning methods \cite[p. 6]{li_anomaly_2024}.
\subsubsection{Shapelet Learning}
% Shapelets: Shapelets are short, representative subsequences extracted from time series data.
% Shapelet Learning: Extraction, Evaluation and Selection of shapelets. Medling
\citeA{beggel_time_2019} address the problem of detecting anomalies in time series data using a novel unsupervised method based on shapelet learning. This approach is particularly useful in scenarios where labeling data is difficult and expensive.
Their method learns representative features that describe the shape of time series data from the normal class and simultaneously learns to accurately detect anomalies. The objective function encourages the learning of a feature representation in which normal time series lie within a compact hypersphere, while anomalous observations lie outside the decision boundary. This is achieved through a block-coordinate descent procedure.
The advantage of their approach is that it can efficiently detect anomalies in unseen test data without retraining the model, by reusing the learned feature representation. Experimental results on multiple benchmark datasets demonstrate the robustness and reliability of the method in detecting anomalous time series, outperforming competing methods when the training data contains anomalies \cite{beggel_time_2019}.

% Alshaer
In contrast, \citeA{alshaer_detecting_2020} propose a method combining matrix profiles with shapelet learning to handle streaming time series data. The matrix profile efficiently identifies potential anomalies in real-time, and shapelet learning characterizes these anomalies accurately. This approach is particularly suited for environments requiring immediate anomaly detection, such as finance, healthcare, and industrial monitoring \cite{alshaer_detecting_2020}.

% shapelet summary
While both methods utilize shapelet learning, Beggel et al. focus on static datasets and robust feature representation, whereas Alshaer et al. emphasize real-time detection in dynamic, streaming environments.
% TODO search for GAN based methods
\subsubsection{combinations}

TODO \cite{aota_zero-shot_2023}  %TODO

TODO \cite{li_zero-shot_2023}  %TODO
