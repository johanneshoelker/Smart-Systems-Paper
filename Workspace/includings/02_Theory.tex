\section{Definitions and Conventions}\label{theory}
\subsection{Representation Learning}
Representation Learning mainly tries to detect interconnections in data, which represent meanings relevant for further data analysis. There are several representation learning techniques to detect patterns and to store them in different ways. In \citeA{bishop_pattern_2006} representation learning occurs in several machine learning areas. In neural networks representations in data are learned in every hidden layer. In that case the representations are not symbolic representations that we as humans see. Cognitive repesentations can in that sense be seperated into neural, spatial and symbolic \cite{gardenfors_conceptual_2000}.\\\\
To extract symbolic or spatial features which are more comprehensible for us a knowledge discovery process with different methods of machine learning and data mining methods are used \cite[p. 4]{lavrac_representation_2021}. RL techniques are devided into Propositionalization as symbolic representations and Embeddings as numeric representations.\\\\
In the book of \citeA{goodfellow_deep_2016} this general detailed description of representation learning is given. They sum up that a representations should make the subsequent learning tasks easier. This implicates that to find the best fitting representation and the underlying representation learning technique, we need to know the task it should perform afterwards.\\\\
% Contrastive learning
One solution to learn representations is contrastive learning. Pairs of data points are labeled as similar and dissimilar. These data points are put into a feature space where the distance between the two represents their similarity. With a contrastive loss function and a label of similarity between two points, the model is trained by putting the similar data points together and separating dissimilar points. Using this method groups of similar data points are formed. \\\\

\subsubsection{Evaluation}\label{theory:evaluation}
% \cite{lavric} 1.4.1
This chapter describes how to evaluate the performance of an RL approach.\\
\cite{bengio_representation_2013} describes what makes a representation "good". They list the following factors:
\begin{itemize}
  \item Smoothness
  \item Multiple Explanatory Factors
  \item A hierarchical organization of explanatory factors
  \item Semi-supervised learning
  \item Shared factors across tasks
  \item Manifolds
  \item Natural clustering
  \item Temporal and spatial coherence
  \item Sparsity
  \item Simplicity of factor dependencies
\end{itemize}
"We want to find properties of the data but at the same time we don't want to loose information about the input" \cite[S. 525]{goodfellow_deep_2016}\\

\subsection{Zero Shot Learning}
%Zero-shot learning involves training a model on certain classes and then testing its ability to recognize new, unseen classes without any retraining. In the context of anomaly detection, this means the model should be able to detect types of anomalies it has not encountered during the training phase.
Zero Shot Learning is an extreme form of transfer learning \cite[S. 536]{goodfellow_deep_2016}. While transfer learning is the concept of transferring the knowledge and weights gained at one task using them at solving another task, Zero-Shot Learning means there are no samples for the other task. The transformation of knowledge can help solving tasks where there are few or no samples available. The gained knowledge is normally stored as representations in the data. Representations which are abstract enough to not see a specific item but information about items which can be applicated to groups of items. THis also means that Zero-Shot Learning is only possible because addition information has been discovered during training \cite[S. 536]{goodfellow_deep_2016}. \\\\
\citeA{palatucci_zero-shot_2009} were the first to implement a successful Zero-Shot Detection followed by \citeA{socher_zero-shot_2013} who used semantic word vector representations to classify words in groups and to sort new words with an accuracy of 90\% with a fully unsupervised mdoel.\\
% Quote Definition for ZSL in \cite[S. 1532]{nivarthi_unified_2022}\\%TODO
\subsection{Anomaly Detection}
Several definitons of anomalies in data can be found in literature. In this paper the definition of \citeA[S. 54]{gruhl_novelty_2022} is used. They seperate anomaly and novelty detection as different tasks. Anomalies can be understood as outliers from the regular class. But these anomalies can vary in their cause. If there is a specific cause and the anomalies occur in its own cluster, they form a novelty. If instead the outliers randomly occur with no specific root cause, they are called noise. The cause for noise then is of a different kind and cannot be classified.\\\\
Instead of dividing anomalies by cause the shape of anomalies can vary in several ways. In real measurement data any shape is possible which can be a mix of the following. This it totally unpredictable \cite{schwartz_maeday_2024}. For training purposes anomaly injection is crucial. Then the anomalies are simulated as point anomalies or subsequence anomalies. Point anomalies occur once and can be global or contextual. Subsequence anomalies on the other hand change the values in a given time window or on long term. They can be divided in seasonal, shapelet and trend anomalies. Seasonal and shapelet change the values in a limited time window, trend anomalies are changing all following values \cite[p. 9]{darban_carla_2024}.\\\\ %Seasonal in time direction, shapelet the amplitudes
In this paper we want to focus on single time events, which are in any case anomalies. Potentially being caused by an unknown process, they cannot be classified \cite{gruhl_novelty_2022}. This defines our goal as an anomally detection task.\\

% ## Theoretischer Rahmen
% Definitionen und Konzepte: Detaillierte Erklärungen zu Representation Learning, Zero Shot Learning und Anomaly Detection.
% Methoden und Techniken: Überblick über die verschiedenen Ansätze und Methoden im Representation Learning (z.B. Autoencoder, CNNs, Word Embeddings).
