\section{Representation Learning Methods}\label{review}
In this chapter any found paper proposing a RL strategy used for time series data with adaptability on anomaly detection tasks is presented. For the literature research the inclusion criteria as described in \ref{criteria} are applied.

The different RL strategies explained focusing on the compliance of the exclusion criteria. The strategies are organized by their underlying concept. We begin with straight-forward methods which are based on one concept and increase the complexity throughout the chapter. In the end methods that use combinations of different concepts are presented.
\subsubsection{MLP}
% INRAD
Using a simple MLP is a straight-forward way to learn representations and to detect anomalies in time series data \cite{nielsen_neural_2015}. The input variable for the MLP are time points and the output variable represents the value at these time points. The model is trained to learn this mapping. With the trained model, the values in a live scenario are predicted and the difference to the actual values is measured. If this representation error exceeds a certain threshold, an anomaly is found. A method called INRAD, Implicit Neural Representation of Time-Series Data is using this concept. The method takes multiple variables as input and the model is trained with data including anomalies. It is not suitable for Zero-Shot Learning \cite{jeong_time-series_2022}.
\subsubsection{RNN}
\cite{su_robust_2019} propose a method called OmniAnomaly for anomaly detection in multivariate time series data using a Stochastic Recurrent Neural Network (SRNN). This approach addresses the challenge of detecting anomalies in complex, high-dimensional time series data.
Their method utilizes a SRNN to model the temporal dependencies in multivariate time series data.
The key advantage of this method is its robustness to noisy and high-dimensional data. The SRNN learns to represent the normal patterns in the time series and identifies deviations from these patterns as anomalies. It relies on training with data that contains normal patterns, which the model uses to detect anomalies based on deviations from these patterns. Since OmniAnomaly depends on having access to representative normal data to learn patterns, it is not suitable for zero-shot scenarios.
\subsubsection{CNN}
Methods based on Convolutional Neural Networks (CNN) are normally used to classify images but in recent papers they are used to detect anomalies in images. \cite{aota_zero-shot_2023} develop a Texture Anomaly Detection and achieve a high performance in Zero Shot Learning. They compare Zero-Shot against Many-Shot Learning in their work. Several image anomaly detection tools can be found (\cite{sabokrou_deep-anomaly__2018}, \cite{aota_zero-shot_2023}). But CNNs perform on time series data as well.

The main idea of using CNNs is to predict a value based on the input frame. If the distance between the predicted and the actual value exceeds a predefined threshold, the anomaly can be detected.

% CyberAttacks Kravchick
This idea is used to detect cyberattacks in industrial control systems. The study by \cite{kravchik_detecting_2018} uses a dataset from a Secure Water Treatment testbed to identify cyber anomalies by measuring the statistical deviation between predicted and observed values. They explore different deep learning architectures, including CNNs and recurrent networks, and find that 1D CNNs perform particularly well for time series prediction tasks. Their approach successfully detects the majority of cyber attacks with minimal false positives, highlighting the effectiveness of CNNs in real-time anomaly detection in multivariate time series \cite{kravchik_detecting_2018}. However, the paper does not discuss the usability on zero-shot learning. In the same area a method detecting unknown cyber-attacks is presented by \cite{zhang_unknown_2020} who use an Autoencoder which is discussed later on.

% TCN
\cite{he_temporal_2019} use Temporal Convolutional Networks (TCN). TCNs restrict the output to be dependent on past and present time steps only. This enables them to capture temporal dependencies. By training on normal patterns, the network learns to predict future values. Significant deviations between these predictions and actual observations indicate potential anomalies. Since the model only learns the normal data, it is able to work in Zero Shot scenarios. The inclusion of a multivariate Gaussian model for error handling and the multi-scale feature mixture method enhances the robustness and accuracy of the anomaly detection process.

% SLMR
Another paper introduces a mask-based self-supervised representation learning approach to extract both short-term local dependencies and long-term global trends. By integrating forecasting and reconstruction-based models, the method effectively captures temporal contexts and feature correlations. An attention mechanism ensures feature importance, leading to better anomaly detection performance on various datasets. The method is designed for multivariate time series anomaly detection but does not explicitly address zero-shot learning scenarios.
\cite{miao_unsupervised_2022}

\subsubsection{Contrastive Learning}
% Paper Debiased Contrastive Learning
Learning representations in time series data is done in several different ways. One solution according to \cite{zhang_debiased_2024} is contrastive learning. By comparing pairs of data points and rating the similarities as distances between the two, CL gets less dependent on labeled data. The data can be more general and the extracted representations are more robust. The pairs of data points are labeled as positive and negative pairs with a distance according to their similarities. With this distance they are put into a feature space where they form groups of data points. To minimize the bias between representations multigranularity augmented view generation and expert knowledge are used during training. The proposed framework is applied on industrial fault detection. The two data sets consist of various vibration signals of industrial machines and stiction sensors with multiple variables. The effectiveness of the proposed framework is demonstrated through its application to these datasets, where it shows improved performance in fault detection compared to traditional methods.

% CARLA
CL is also used for anomaly detection in time series data by \cite{darban_carla_2024}. They use CL combined with synthetic anomaly injection. CL enables them to capture patterns in time series data and the framework shows good results on common real world datasets. Like in the previous paper, dissimilar pairs, the anomalies, build distant data points and similar data points are close to each other. In order to train the model artificial anomalies are injected which build distant pairs. In the next stage the classification is done by  the proximity of the neighbours in the representation space. Additionally anchor points representing the nearest and furthest neighbour are given from each representation. Their methodology is called CARLA and is not tested for Zero-Shot Learning. An implementation by the authors can be found \footnote{\fussy\tiny https://github.com/zamanzadeh/CARLA}.

% CL-TAD
CL-TAD, a method for time series anomaly detection that leverages contrastive learning and reconstruction-based techniques addresses the challenges of temporal dynamics, label scarcity, and data diversity in real-world applications. The method comprises two main components: positive sample generation and contrastive-learning-based representation learning. Positive samples are generated by reconstructing masked parts of the time series data, helping the model learn the underlying normal patterns. These samples, along with the original data, are then fed into a contrastive learning framework, which contrasts pairs of similar and dissimilar samples to learn representations. This process helps the model map similar data points closer together in the feature space while distancing dissimilar points, making it easier to detect deviations indicative of anomalies \cite{ngu_cl-tad_2023}.
While CL-TAD is not explicitly designed as a zero-shot learning method, its use of contrastive learning and reconstruction-based techniques suggests that it could have potential in zero-shot anomaly detection scenarios. A tutorial for implementation can be found \footnote{\fussy\tiny https://github.com/nguhcv/cl-tad/tree/main}.

% OCC based methods
% COCA
To succeed on Zero-Shot Anomaly Detection, One-Class Classification (OCC) can be useful.
By gathering all "normal" values into a single class the outliers are directly detected if they are outside of it.
The COCA (Contrastive One-Class Anomaly Detection) method combines contrastive learning with OCC to improve anomaly detection in multivariate time-series data. By treating original and reconstructed representations as positive pairs, it optimizes a contrastive one-class loss function that enhances the detection of anomalies while preventing common issues like hypersphere collapse. Although COCA is designed for self-supervised anomaly detection, its ability to learn from unlabeled data suggests potential applicability in zero-shot learning scenarios, though this has not been explicitly tested \cite{wang_deep_2023}. An implementation script is provided by the authors \footnote{\fussy\tiny https://github.com/ruiking04/COCA}.

%
The paper by \cite{lee_time_2023} presents an approach for detecting anomalies also using OCC in industrial time series data, which typically lacks labels for supervised learning. They combine OCC with contrastive learning to define a new objective function that can simultaneously learn from both models. This method enhances feature extraction while preserving temporal characteristics. The paper demonstrates the method's effectiveness through high anomaly detection performance on datasets with similar normal and anomalous data forms, highlighting its potential in industrial applications.

Unlike traditional OCC methods that map all normal instances into a single hypersphere, the method presented by \cite{chen_time-series_2023} focuses on local contextual information. By pulling each normal instance towards its recent context window, it aims to better detect context-based anomalies. The model incorporates a deterministic contrastive loss, which improves the network's ability to distinguish between normal and abnormal data.

% TS2Vec
\cite{yue_ts2vec_2022} introduce TS2Vec, a framework for learning robust and universal time series representations at multiple semantic levels through hierarchical contrastive learning. This approach utilizes timestamp masking and random cropping to create augmented context views, enhancing position-agnostic and comprehensive representations. By combining instance-wise and temporal contrastive losses, TS2Vec captures characteristics of different time series instances and dynamic temporal patterns within each series. The method is tested on Zero Shot Learning and is applicable for multivariate time series data.

% TimeAutoAD
Another paper introduces an autonomous system for anomaly detection in multivariate time series data also using Contrastive Learning. The proposed TimeAutoAD automates model configuration and hyperparameter optimization, addressing challenges such as limited labeled anomaly data. It uses self-supervised contrastive learning to enhance the model's ability to differentiate normal and anomalous time series by generating pseudo-negative samples. The method is tested on real-world datasets, demonstrating improved performance over existing anomaly detection techniques, especially in scenarios where training data may be contaminated. The paper does not explicitly address zero-shot anomaly detection \cite{jiao_timeautoad_2022}.

% ContrastAD
The ContrastAD framework presented in \cite{li_contrastive_2023} is a self-supervised method for time series anomaly detection that leverages contrastive learning with temporal transformations. The key innovation is the use of anomaly-induced transformations to create representations that differentiate between normal and abnormal data. This approach targets both point anomalies and contextual anomalies in high-dimensional time series, which are often missed by other methods. By learning distinct representations for normal and anomalous data in the latent space, ContrastAD improves performance on noisy and complex datasets. However, the method is not trained or validated on entirely new types of anomalies.

% DCdetector
Another model called DCdetector presented in \cite{yang_dcdetector_2023} is a multi-scale dual attention contrastive learning framework designed for time-series anomaly detection. It utilizes a dual attention asymmetric design to create a permutation-invariant representation, guiding the learning process with pure contrastive loss. This approach enhances the model's ability to discriminate between normal and anomalous data. Extensive experiments demonstrate that DCdetector achieves state-of-the-art performance across multiple benchmark datasets. While the paper focuses on its effectiveness in anomaly detection, it does not explicitly address or test the model's applicability to zero-shot learning scenarios \cite{yang_dcdetector_2023} In the article a repository containing bash scripts for implementation can be found \footnote{\fussy\tiny https://github.com/DAMO-DI-ML/KDD2023-DCdetector}.

%Contrastive Predictive Coding Methods:
\cite{pranavan_contrastive_2022} presents a novel approach for anomaly detection in multi-variate time series data using Contrastive Predictive Coding (CPC). Their method, named Time-series Representational Learning through Contrastive Predictive Coding (TRL-CPC), aims to capture the temporal dependencies and correlations across multiple variables in time series data.
The TRL-CPC framework consists of an encoder, an auto-regressive model, and a non-linear transformation model. These components are optimized to learn the representations of multi-variate time series data by predicting future segments from past segments. The core idea is to maximize the mutual information between the encoded representations of past and future segments, thereby learning robust representations.
To detect anomalies, TRL-CPC calculates the prediction error between actual future segments and the predicted segments generated by the CPC model. Anomalies are identified where this prediction error exceeds a certain threshold, enabling unsupervised anomaly detection based on the structure of the data itself \cite{pranavan_contrastive_2022}.

% TiCTok
CPC is also used by the method TiCTok presented in \cite{kang_tictok_2023}. The model proposes an approach to multivariate time-series anomaly detection by combining contrastive tokenization with a time-series token encoder. This encoder converts raw time-series data into latent embeddings that capture wide-ranging temporal information. The model employs contrastive learning to produce representations, which help distinguish between normal and anomalous data. Additionally, TiCTok introduces a new anomaly scoring method based on the contrastive loss used during training. In their paper they do not test the model on Zero Shot Learning scenarios.

% MGCLAD
Another paper introduces Multiview Graph Contrastive Learning for detecting anomalies in multivariate time-series data, particularly in IoT systems. The method constructs graph structures to model both temporal context and signal dependencies, while an adaptive data augmentation strategy generates graph views for contrastive learning. This approach enhances representation quality and improves performance in anomaly detection tasks except Zero Shot Learning \cite{qin_multiview_2023}. The method is available on Github \footnote{\fussy\tiny https://github.com/shuxin-qin/MGCLAD}.

% TriAD
The paper by \cite{sun_unraveling_2023} introduces TriAD (Tri-domain Anomaly Detector), a self-supervised learning method for time-series anomaly detection. TriAD models features across three domains: temporal, frequency, and residual. Without relying on labeled anomalies. Unlike traditional contrastive learning, TriAD uses inter-domain and intra-domain contrastive losses to learn shared attributes among normal data and distinguish them from anomalies. The approach is designed to handle anomalies of varying lengths and shapes \cite{sun_unraveling_2023}. The authors provide a link to their implementation \footnote{\fussy\tiny https://github.com/pseudo-Skye/TriAD}.

In summary methods using contrastive learning are mostly not tested on Zero-Shot Learning. Their robustness against noisy data is not ensured.

\subsubsection{Autoencoder}
Autoencoders on the other hand are robust to unclean training datasets \cite[p. 2487]{abdulaal_practical_2021}. Due to their architecture noise is filtered from the input.

\cite{provotar_unsupervised_2019} introduces an anomaly detection method using an autoencoder architecture based on Long Short-Term Memory (LSTM) networks. The core idea is that an LSTM autoencoder learns to compress and reconstruct the input time series data. During training, the model learns the normal patterns in the data by minimizing the reconstruction error. When fed new data, the model attempts to reconstruct it, and any significant reconstruction error (i.e. deviation between the original and reconstructed data) signals an anomaly. This approach is particularly effective because LSTMs are well-suited to capture temporal dependencies, making them ideal for time series data. The model works without requiring labeled datasets, making it an unsupervised solution. The authors test the model on both synthetic and real-world data, such as sound event detection. It can effectively detect outliers based on reconstruction error but does not have the capacity for ZSL.

% TSMAE
Time Series Memory-Augmented Autoencoder (TSMAE), a anomaly detection method for IoT time-series data integrates a memory module to suppress overgeneralization by storing normal patterns and reconstructing time-series data with Long Short-Term Memory (LSTM) networks to capture temporal dependencies. Anomalies are detected by comparing the reconstruction error between the original and generated sequences, enhancing anomaly detection effectiveness. This method is tested on multivariate time series data, but no explicit mention of zero-shot testing was found \cite{gao_tsmae_2023}.

%UAE-TENN
\cite{nivarthi_multi-task_2023} are the first to use a Unified Autoencoder (UAE) for time series data, namely the power forecast of wind and solar plants. They contribute to the challenge of predicting the possible outcome of renewable energy in a newly created plant, either wind or solar. To do so a UAE is combined with a Task Embedding Neural Network (TENN). They examine the usability divided in Single-Task, Multi-Task and Zero-Shot Learning. The method was first published in \cite{nivarthi_unified_2022}. It is then extended by convolutional layers instead of the fully connected neural network layers (UCAE-TENN) and also Long Short-Term Memory layers (ULAE-TENN).

% MST-VAE
\cite{pham_mst-vae_2022} proposes a Multi-Scale Temporal Variational Autoencoder (MST-VAE) for anomaly detection in multivariate time series data. MST-VAE combines short and long-scale convolutional kernels within a 1D CNN and a Variational Autoencoder to capture both short-term and long-term temporal patterns. The method is not directly tested in a Zero-Shot scenario. An implemention is provided by the authors \footnote{\fussy\tiny https://github.com/tuananhphamds/MST-VAE}.

% MSCVAE
Another method using VAE introduces a Multi-Scale Convolutional Variational Autoencoder (MSCVAE) for unsupervised anomaly detection in multivariate time-series data. It constructs multi-scale attribute matrices to capture system states at different time steps and uses a convolutional variational autoencoder to extract spatial features, while an attention-based ConvLSTM captures temporal patterns. Anomalies are detected by comparing the original and reconstructed matrices, and a novel ERR-based threshold strategy is employed for optimal performance. The article does not mention Zero-Shot Learning and neither provides an open source implementation \cite{yookampon_robust_2022}.

% VRAE based
To detect anomalies in healthcare data a variational recurrent autoencoder (VRAE) is used in \cite{pereira_learning_2019}. They created an unsupervised framework where the model learns to represent the data and detect anomalies without needing labeled examples.
The model works by learning to reconstruct the input sequences. During training, they add noise to the input data, and the model tries to reconstruct the original, uncorrupted data. This helps the model learn more robust representations of the data. To detect anomalies, they cluster these learned representations and calculate the distance to identify outliers. Their approach was tested on the ECG5000 dataset and showed that it could effectively detect unusual heartbeats, performing better than previous methods that required labeled data. The model is designed to capture temporal dependencies, making it applicable for MVTSD.

%
Another approach using VRAE involves creating synthetic anomalies to improve the detection process. A two-level hierarchical latent space representation is used. First, they distill feature descriptors of normal data points into more robust representations using AEs. These representations are then refined using a VAE that creates a family of distributions. From these distributions, they select those that lie on the outskirts of the normal data as generators of synthetic anomalies.
By generating these synthetic anomalies, they train binary classifiers to distinguish between normal and abnormal data. Their hierarchical structure for feature distillation and fusion helps create robust representations, enabling effective anomaly detection without needing actual anomalies during training \cite{ramirez_rivera_anomaly_2022}.

% VRQRAE
Kieu et al. propose Variational Quasi-Recurrent Autoencoders (VQRAEs) for unsupervised time series anomaly detection, using robust divergences to improve resilience against noisy data. VQRAEs utilize Quasi-Recurrent Neural Networks (QRNNs) for efficient temporal dependency capture, and a bi-directional version (BiVQRAEs) enhances accuracy by processing data in both forward and backward directions. However the method was not explicitly tested on Zero-Shot Learning \cite{kieu_anomaly_2022} \footnote{\fussy\tiny https://github.com/tungk/Bi-VQRAE}.


% Unknwon CyberAttacks
The paper by \cite{zhang_unknown_2020} addresses the challenge of detecting unknown cyberattacks by applying zero-shot learning. The proposed method maps the features of known attacks to a semantic space using a sparse autoencoder and restores them to the feature space by minimizing reconstruction errors, effectively creating a mapping between features and semantic attributes. This technique enables the model to detect previously unseen attacks by generalizing from known attack features. The research highlights the feasibility and effectiveness of zero-shot learning for cybersecurity applications.

% FuSAGNet
A method called Fused Sparse Autoencoder and Graph Net (FuSAGNet) is used for anomaly detection in multivariate time series data, specifically targeting cyber-physical systems. The method combines a Sparse Autoencoder (SAE) to learn sparse latent representations and a Graph Neural Network (GNN) to forecast future time series behavior. It captures both temporal dependencies and complex inter-feature relationships by learning graph structures from the data. This joint optimization approach aims to improve anomaly detection performance by fusing reconstruction and forecasting tasks. The method was empirically tested on three real-world datasets related to industrial systems but not in Zero-Shot Learning scenarios \cite{han_learning_2022} \footnote{\fussy\tiny https://github.com/sihohan/FuSAGNet}.

% deep AOC
Deep Autoencoding One-Class (AOC), a method for time-series anomaly detection that combines autoencoder-based reconstruction and one-class classification in a single-stage approach to better capture normal patterns. The method is evaluated on public datasets, outperforming baseline models and proving effective for detecting various types of anomalies in both univariate and multivariate time-series data. However, zero-shot learning is not tested or addressed in the proposed framework \cite{mou_deep_2023} \footnote{\fussy\tiny https://github.com/alsike22/AOC}.

% RANSynCoders
Another paper presents a method named RANSynCoders for detecting anomalies in asynchronous multivariate time series by combining spectral analysis with autoencoders. It leverages spectral analysis to synchronize the features, followed by multiple autoencoders using random feature subsets to detect anomalies via majority voting. This approach improves performance on high-dimensional, asynchronous data by reducing false positives and providing better localization of anomalies. The method in the paper does not specifically mention being tested in zero-shot scenarios \cite{abdulaal_practical_2021}. An implemention of the method is provided by the authors \footnote{\fussy\tiny https://github.com/eBay/RANSynCoders}.

% TCN-AE
Another proposed method is using a Temporal Convolutional Network Autoencoder (TCN-AE) designed for unsupervised anomaly detection in multivariate time series data. It employs dilated convolutions to capture long-range dependencies and compresses time series into representations using an autoencoder. The method detects anomalies by evaluating reconstruction errors, as anomalies are expected to have significantly higher reconstruction errors compared to normal patterns \cite{thill_temporal_2021}. A Repository with a minimal working example can be found provided by the author but no implementation on new datasets is provided \footnote{\fussy\tiny https://github.com/MarkusThill/bioma-tcn-ae}.



% MEGA
A framework called Multiscale Wavelet Graph AutoEncoder (MEGA) presented in \cite{wang_multiscale_2023} for anomaly detection in multivariate time series. It integrates Discrete Wavelet Transform (DWT) to decompose time series into different frequency components, reconstructing them to highlight anomalies across scales. Additionally, a dynamic graph convolution network is employed to model inter-variable relationships at different scales, enhancing the detection of anomalies caused by changes in variable dependencies.
This method is tested in a zero-shot scenario and explicitly takes multivariate time series as input. The code can be found provided by the authors \footnote{\fussy\tiny https://github.com/jingwang2020/MEGA}.

% DAEMON
The method proposed in \cite{chen_adversarial_2023} introduces DAEMON, an adversarial autoencoder framework designed for unsupervised multivariate time-series anomaly detection and interpretation. The model employs two adversarial training processes: one to align the hidden variable’s posterior distribution with a prior and another to minimize the difference between original and reconstructed data. This improves robustness and avoids overfitting while anomalies are detected based on reconstruction errors. There is no mentioning of testing in a zero-shot scenario.
Replicating the results of the paper is possible due to the open source availability mentioned in the paper
\footnote{\fussy\tiny https://github.com/Sherlock-C/DAEMON \label{foot_daemon}}.


% GRU-AE
The method proposed in \cite{gong_autoencoder-based_2022} utilizes a Gated Recurrent Unit-based Autoencoder (GRU-AE) to detect anomalies in time-series data by reconstructing sequences and identifying large reconstruction errors as anomalies. It incorporates an attention mechanism to enhance performance and applies multi-timestamp stacking to reduce time steps for better training efficiency. The model is tested on real-world datasets from cellular networks, detecting anomalies at both single-day and multi-day scales. The method is applied to multivariate time-series data, but no mention of a zero-shot scenario is made.

%
The method proposed in \cite{wang_attention-based_2022} presents an attention-based encoder-decoder network for anomaly detection in time-series data, which focuses on learning representations in both principal and residual spaces without needing reconstruction. The attention mechanism is applied to rescale convolutional layers, highlighting the most contributive segments of the data for better representation learning. This approach improves anomaly detection by calculating belief scores in both spaces using kernel density estimation (KDE). The method is tested on multivariate time series and in a zero-shot scenario, specifically for detecting new faults.

% MOMENT
To overcome the challenge of poorly available time series data sets \cite{ma_survey_2023}, the model family MOMENT tries to learn general patterns on a pile of time series data. The pile is a collection of different datasets which they assembled for their pretraining. According to the paper minimal finetuning is needed to perform on multivariate time series tasks like anomaly detection. They published the model and made the usage easily accessible with its own python library. The time serie datasets the model is trained on consist of domains including weather measurements, sensor values and power consumption datasets. They also included tongue and finger movement of humans. The different tasks which the model is evaluated on are forecasting (long and short horizon), classification, anomaly detection and imputation. Except for short-horizon forecasting all tasks are managed well. However it cannot detect anomalies in vertically shifted time series \cite{goswami_moment_2024}. The method is available as a Jupyter Notebook on Github \footnote{\fussy\tiny https://github.com/moment-timeseries-foundation-model/moment/blob/main/tutorials/anomaly\_detection.ipynb \label{foot_moment}}.

% \cite{zhang_debiased_2024} point out, that AE-based methods have remaining challenges.
% TODO what are the challenges of autoencoders?

\subsubsection{Transformer}
% AnomalyTransformer
\cite{xu_anomaly_2022} use a Transformer architecture to detect anomalies on three different datasets. TODO



% DATN TODO
\cite{wu_decompose_2023}

% TiSAT TODO
\cite{doshi_tisat_2022}

% TCF-Trans TODO
\cite{peng_tcf-trans_2023}

% DCT-GAN TODO
\cite{li_dct-gan_2023}

% GRU-AE TODO
\cite{shin_time_2023}


% LLMs
Based on the transformer architecture, Large Language Models (LLM) are developed and used increasingly in different applications. Known as chatbots they can help in language specific tasks. Beside that they can be used in anomaly detection and forecasting. \cite{su_large_2024} examine a literature review on how LLMs perform on anomaly detection tasks concerning time series data. LLMs in anomaly detection are specifically useful when the time series data is in the form of words. This can be the case in log analysis. Logs are generated over time and hold a lot of information which can singify errors and system failures. They conclude that LLMs have potential in detecting anomalies but challenges remain. The occurrence of hallucinations and the need for computational efficiency to name a few.

A method detecting anomalies in tabular data using LLMs is presented by \cite{li_anomaly_2024}. In order to perform tasks that LLMs are not directly build for they generate synthetic datasets. Using these datasets LLMs and specifically GPT-4 have comparable performance with transductive learning methods \cite[p. 6]{li_anomaly_2024}.

% Fourier
Using Fourier Analysis and the transformer architecture \cite{ye_multivariate_2023} detect anomalies in time series data. The encoder of a transformer is used to capture temporal features of time series. They detect frequencies by using Fourier Analysis.

\subsubsection{Shapelet Learning}
% Shapelets: Shapelets are short, representative subsequences extracted from time series data.
% Shapelet Learning: Extraction, Evaluation and Selection of shapelets. Medling
\cite{beggel_time_2019} address the problem of detecting anomalies in time series data using a novel unsupervised method based on shapelet learning. This approach is particularly useful in scenarios where labeling data is difficult and expensive.
Their method learns representative features that describe the shape of time series data from the normal class and simultaneously learns to accurately detect anomalies. The objective function encourages the learning of a feature representation in which normal time series lie within a compact hypersphere, while anomalous observations lie outside the decision boundary. This is achieved through a block-coordinate descent procedure.
The advantage of their approach is that it can efficiently detect anomalies in unseen test data without retraining the model, by reusing the learned feature representation. Experimental results on multiple benchmark datasets demonstrate the robustness and reliability of the method in detecting anomalous time series, outperforming competing methods when the training data contains anomalies.

% Alshaer
In contrast, \cite{alshaer_detecting_2020} propose a method combining matrix profiles with shapelet learning to handle streaming time series data. The matrix profile efficiently identifies potential anomalies in real-time, and shapelet learning characterizes these anomalies accurately. This approach is particularly suited for environments requiring immediate anomaly detection, such as finance, healthcare, and industrial monitoring.

% shapelet summary
While both methods utilize shapelet learning, Beggel et al. focus on static datasets and robust feature representation, whereas Alshaer et al. emphasize real-time detection in dynamic, streaming environments.
% TODO search for GAN based methods

% SL based TODO
\cite{liang_shapelet-based_2024}

% IPS TODO
\cite{li_ips_2022}

\subsubsection{Other}
% Clustering
\cite{li_clustering-based_2021} propose a method for detecting anomalies in multivariate time series by combining clustering and reconstruction techniques. The authors use a sliding window approach to generate subsequences from the multivariate time series, then apply extended fuzzy clustering to reveal the underlying structure of the subsequences. By reconstructing these subsequences with optimal cluster centers, the method detects anomalies based on how well the reconstructed data fits the original subsequences. A confidence index quantifies the level of detected anomalies. The approach is shown to effectively detect anomalies related to amplitude and shape changes in various application domains, such as healthcare and finance, and is optimized using Particle Swarm Optimization.

% TODO include benchmark methods from \cite[p. 19]{darban_carla_2024}

% LSTM based TODO
\cite{niu_lstm-based_2020}
% Moving Memory Dynamic Filter TODO
\cite{duan_unsupervised_2021}
