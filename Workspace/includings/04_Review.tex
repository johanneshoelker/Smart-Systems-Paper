\section{Representation Learning Methods}\label{review}
In this chapter any found paper proposing a RL strategy used for time series data with adaptability on anomaly detection tasks is presented. Here the inclusion criteria as described in \ref{criteria} are applied.

The different RL strategies are listed, explained and compared. The strategies are organized by their underlying concept. We begin with straight-forward methods which are based on one concept and increase the complexity throughout the chapter. In the end methods that use combinations of different concepts are presented.
\subsubsection{MLP}
% INRAD
Using a simple MLP is a straight-forward way to learn representations and to detect anomalies in time series data \cite{nielsen_neural_2015}. The input variable for the MLP are time points and the output variable represents the value at these time points. The model is trained to learn this mapping. With the trained model, the values in a live scenario are predicted and the difference to the actual values is measured. If this representation error exceeds a certain threshold, an anomaly is found. A method called INRAD, Implicit Neural Representation of Time-Series Data is using this concept. The method takes multiple variables as input and the model is trained with data including anomalies. It is not suitable for Zero-Shot Learning \cite{jeong_time-series_2022}.
\subsubsection{RNN}
\cite{su_robust_2019} propose a method called OmniAnomaly for anomaly detection in multivariate time series data using a Stochastic Recurrent Neural Network (SRNN). This approach addresses the challenge of detecting anomalies in complex, high-dimensional time series data.
Their method utilizes a SRNN to model the temporal dependencies in multivariate time series data.
The key advantage of this method is its robustness to noisy and high-dimensional data. The SRNN learns to represent the normal patterns in the time series and identifies deviations from these patterns as anomalies. It relies on training with data that contains normal patterns, which the model uses to detect anomalies based on deviations from these patterns. Since OmniAnomaly depends on having access to representative normal data to learn patterns, it is not suitable for zero-shot scenarios.
\subsubsection{CNN}
Methods based on Convolutional Neural Networks (CNN) are normally used to classify images but in recent papers they are used to detect anomalies in images. \cite{aota_zero-shot_2023} develop a Texture Anomaly Detection and achieve a high performance in Zero Shot Learning. They compare Zero-Shot against Many-Shot Learning in their work. Several image anomaly detection tools can be found (\cite{sabokrou_deep-anomaly__2018}, \cite{aota_zero-shot_2023}). But CNNs perform on time series data as well.

The main idea of using CNNs is to predict a value based on the input frame. If the distance between the predicted and the actual value exceeds a predefined threshold, the anomaly can be detected.

% CyberAttacks Kravchick
This idea is used to detect cyberattacks in industrial control systems. The study by \cite{kravchik_detecting_2018} uses a dataset from a Secure Water Treatment testbed to identify cyber anomalies by measuring the statistical deviation between predicted and observed values. They explore different deep learning architectures, including CNNs and recurrent networks, and find that 1D CNNs perform particularly well for time series prediction tasks. Their approach successfully detects the majority of cyber attacks with minimal false positives, highlighting the effectiveness of CNNs in real-time anomaly detection in multivariate time series \cite{kravchik_detecting_2018}. However, the paper does not discuss the usability on zero-shot learning. In the same area a method detecting unknown cyber-attacks is presented by \cite{zhang_unknown_2020} who use an Autoencoder which is discussed later on.

% TCN
\cite{he_temporal_2019} use Temporal Convolutional Networks (TCN). TCNs restrict the output to be dependent on past and present time steps only. This enables them to capture temporal dependencies. By training on normal patterns, the network learns to predict future values. Significant deviations between these predictions and actual observations indicate potential anomalies. Since the model only learns the normal data, it is able to work in Zero Shot scenarios. The inclusion of a multivariate Gaussian model for error handling and the multi-scale feature mixture method enhances the robustness and accuracy of the anomaly detection process.

% SLMR
Another paper introduces a mask-based self-supervised representation learning approach to extract both short-term local dependencies and long-term global trends. By integrating forecasting and reconstruction-based models, the method effectively captures temporal contexts and feature correlations. An attention mechanism ensures feature importance, leading to better anomaly detection performance on various datasets. The method is designed for multivariate time series anomaly detection but does not explicitly address zero-shot learning scenarios.
\cite{miao_unsupervised_2022}

\subsubsection{Contrastive Learning}
% Paper Debiased Contrastive Learning
Learning representations in time series data is done in several different ways. One solution according to \cite{zhang_debiased_2024} is contrastive learning. By comparing pairs of data points and rating the similarities as distances between the two, CL gets less dependent on labeled data. The data can be more general and the extracted representations are more robust. The pairs of data points are labeled as positive and negative pairs with a distance according to their similarities. With this distance they are put into a feature space where they form groups of data points. To minimize the bias between representations multigranularity augmented view generation and expert knowledge are used during training. The proposed framework is applied on industrial fault detection. The two data sets consist of various vibration signals of industrial machines and stiction sensors with multiple variables. The effectiveness of the proposed framework is demonstrated through its application to these datasets, where it shows improved performance in fault detection compared to traditional methods.

% CARLA
CL is also used for anomaly detection in time series data by \cite{darban_carla_2024}. They use CL combined with synthetic anomaly injection. CL enables them to capture patterns in time series data and the framework shows good results on common real world datasets. Like in the previous paper, dissimilar pairs, the anomalies, build distant data points and similar data points are close to each other. In order to train the model artificial anomalies are injected which build distant pairs. In the next stage the classification is done by  the proximity of the neighbours in the representation space. Additionally anchor points representing the nearest and furthest neighbour are given from each representation. Their methodology is called CARLA and is not tested for Zero-Shot Learning.

% CL-TAD
The article by \cite{ngu_cl-tad_2023} introduces CL-TAD, a novel method for time series anomaly detection that leverages contrastive learning and reconstruction-based techniques to address the challenges of temporal dynamics, label scarcity, and data diversity in real-world applications. The method comprises two main components: positive sample generation and contrastive-learning-based representation learning. Positive samples are generated by reconstructing masked parts of the time series data, helping the model learn the underlying normal patterns. These samples, along with the original data, are then fed into a contrastive learning framework, which contrasts pairs of similar (positive) and dissimilar (negative) samples to learn representations. This process helps the model map similar data points closer together in the feature space while distancing dissimilar points, making it easier to detect deviations indicative of anomalies. Extensive experiments on nine benchmark datasets show that CL-TAD outperforms ten other recent methods in detecting anomalies, highlighting its effectiveness in handling diverse and complex time series data \cite{ngu_cl-tad_2023}.
While CL-TAD is not explicitly designed as a zero-shot learning method, its use of contrastive learning and reconstruction-based techniques suggests that it could have potential in zero-shot anomaly detection scenarios. However, this would depend on the model's ability to generalize from the learned normal patterns to detect unseen anomalies. Further empirical studies would be needed to validate its performance in zero-shot learning scenarios.

% OCC based methods
% COCA
To succeed on Zero-Shot Anomaly Detection, the method of One-Class Classification (OCC) can solve the problem. %TODO Source
By gathering all "normal" values into a single class the outliers are directly detected if they are outside of it.
The COCA (Contrastive One-Class Anomaly Detection) method combines contrastive learning with OCC to improve anomaly detection in time-series data. By treating original and reconstructed representations as positive pairs, it optimizes a contrastive one-class loss function that enhances the detection of anomalies while preventing common issues like hypersphere collapse. Although COCA is designed for self-supervised anomaly detection, its ability to learn from unlabeled data suggests potential applicability in zero-shot learning scenarios, though this has not been explicitly tested \cite{wang_deep_2023}.

%
The paper by \cite{lee_time_2023} presents an approach for detecting anomalies also using OCC in industrial time series data, which typically lacks labels for supervised learning. They combine OCC with contrastive learning to define a new objective function that can simultaneously learn from both models. This method enhances feature extraction while preserving temporal characteristics. The paper demonstrates the method's effectiveness through high anomaly detection performance on datasets with similar normal and anomalous data forms, highlighting its potential in industrial applications.

Unlike traditional OCC methods that map all normal instances into a single hypersphere, the method presented by \cite{chen_time-series_2023} focuses on local contextual information. By pulling each normal instance towards its recent context window, it aims to better detect context-based anomalies. To prevent representation collapse, the model incorporates a deterministic contrastive loss, which improves the network's ability to distinguish between normal and abnormal data.

% TS2Vec
\cite{yue_ts2vec_2022} introduce TS2Vec, a framework for learning robust and universal time series representations at multiple semantic levels through hierarchical contrastive learning. This approach utilizes timestamp masking and random cropping to create augmented context views, enhancing position-agnostic and comprehensive representations. By combining instance-wise and temporal contrastive losses, TS2Vec captures unique characteristics of different time series instances and dynamic temporal patterns within each series. Extensive experiments show that TS2Vec outperforms state-of-the-art methods in classification, forecasting, and anomaly detection tasks across 125 UCR and 29 UEA datasets, achieving average improvements of 2.4\% and 3.0\% in classification accuracy, respectively. The framework's efficiency in training time further underscores its practical utility. TS2Vec demonstrates its versatility by excelling in multiple time series analysis tasks, making it a significant contribution to the field. The framework's hierarchical contrastive learning at various scales encapsulates rich and meaningful patterns in time series data.

% TimeAutoAD
Another paper introduces an autonomous system for anomaly detection in multivariate time series data using Contrastive Learning. The proposed TimeAutoAD automates model configuration and hyperparameter optimization, addressing challenges such as limited labeled anomaly data. It uses self-supervised contrastive learning to enhance the model's ability to differentiate normal and anomalous time series by generating pseudo-negative samples. The method is tested on real-world datasets, demonstrating improved performance over existing anomaly detection techniques, especially in scenarios where training data may be contaminated \cite{jiao_timeautoad_2022}.

The paper does not explicitly address zero-shot anomaly detection, which refers to detecting anomalies without having seen any labeled anomalies during training. However, the method is designed to function in a self-supervised manner, meaning it generates pseudo-negative samples from normal data to train the model, which suggests that it could be applied to scenarios where anomaly labels are not available. This approach allows the model to differentiate between normal and abnormal behaviors based on the learned representations, potentially making it useful for zero-shot anomaly detection in multivariate time series data. Nonetheless, further validation would be needed to confirm its effectiveness specifically for zero-shot tasks.

% ContrastAD
The ContrastAD framework presented by \cite{li_contrastive_2023} is a self-supervised method for time series anomaly detection that leverages contrastive learning with temporal transformations. The key innovation is the use of anomaly-induced transformations to create representations that differentiate between normal and abnormal data. This approach targets both point anomalies and contextual anomalies in high-dimensional time series, which are often missed by other methods. By learning distinct representations for normal and anomalous data in the latent space, ContrastAD improves performance on noisy and complex datasets. However, the method is not trained or validated on entirely new types of anomalies.

% DCdetector
Another model called DCdetector presented by \cite{yang_dcdetector_2023} is a multi-scale dual attention contrastive learning framework designed for time-series anomaly detection. It utilizes a dual attention asymmetric design to create a permutation-invariant representation, guiding the learning process with pure contrastive loss. This approach enhances the model's ability to discriminate between normal and anomalous data. Extensive experiments demonstrate that DCdetector achieves state-of-the-art performance across multiple benchmark datasets. While the paper focuses on its effectiveness in anomaly detection, it does not explicitly address or test the model's applicability to zero-shot learning scenarios \cite{yang_dcdetector_2023}.

%Contrastive Predictive Coding Methods:
%
\cite{pranavan_contrastive_2022} present a novel approach for anomaly detection in multi-variate time series data using Contrastive Predictive Coding (CPC). Their method, named Time-series Representational Learning through Contrastive Predictive Coding (TRL-CPC), aims to effectively capture the temporal dependencies and correlations across multiple variables in time series data.
The TRL-CPC framework consists of an encoder, an auto-regressive model, and a non-linear transformation model. These components are jointly optimized to learn the representations of multi-variate time series data by predicting future segments from past segments. The core idea is to maximize the mutual information between the encoded representations of past and future segments, thereby learning robust representations.
To detect anomalies, TRL-CPC calculates the prediction error between actual future segments and the predicted segments generated by the CPC model. Anomalies are identified where this prediction error exceeds a certain threshold, enabling unsupervised anomaly detection based on the structure of the data itself.
Experimental results show that TRL-CPC outperforms traditional anomaly detection methods on several benchmark datasets, highlighting its effectiveness in capturing complex temporal dependencies and identifying anomalies in multi-variate time series data \cite{pranavan_contrastive_2022}.

% TiCTok
CPC is also used by the method TiCTok presented by \cite{kang_tictok_2023}. The model proposes a novel approach to time-series anomaly detection by combining contrastive tokenization with a time-series token encoder. This encoder converts raw time-series data into latent embeddings that capture wide-ranging temporal information. The model employs contrastive learning to produce high-quality representations, which help distinguish between normal and anomalous data. Additionally, TiCTok introduces a new anomaly scoring method based on the contrastive loss used during training. Experimental results indicate that TiCTok performs effectively across multiple benchmark datasets, achieving results that are either superior or comparable to existing state-of-the-art methods. In their paper they do not test the model on Zero Shot Learning scenarios like the previous method.

% MGCLAD
A paper by \cite{qin_multiview_2023} introduces Multiview Graph Contrastive Learning for detecting anomalies in multivariate time-series data, particularly in IoT systems. The method constructs graph structures to model both temporal context and signal dependencies, while an adaptive data augmentation strategy generates graph views for contrastive learning. This approach enhances representation quality and improves performance in anomaly detection tasks, outperforming existing methods on multiple real-world datasets

% TriAD
The paper by \cite{sun_unraveling_2023} introduces TriAD (Tri-domain Anomaly Detector), a self-supervised learning method for time-series anomaly detection. TriAD models features across three domains—temporal, frequency, and residual—without relying on labeled anomalies. Unlike traditional contrastive learning, TriAD uses inter-domain and intra-domain contrastive losses to learn shared attributes among normal data and distinguish them from anomalies. The approach is designed to handle anomalies of varying lengths and shapes, and it achieves significant improvements over state-of-the-art deep learning models in anomaly detection tasks \cite{sun_unraveling_2023}.

% TODO summary on cl methods telling that unseen training datasets are a problem


\subsubsection{Autoencoder}
"Autoencoders are robust to unclean training datasets" \cite[p. 2487]{abdulaal_practical_2021}.

A paper by \cite{provotar_unsupervised_2019} introduces an anomaly detection method using an autoencoder architecture based on Long Short-Term Memory (LSTM) networks. The core idea is that an LSTM autoencoder learns to compress and reconstruct the input time series data. During training, the model learns the normal patterns in the data by minimizing the reconstruction error. When fed new data, the model attempts to reconstruct it, and any significant reconstruction error (i.e., deviation between the original and reconstructed data) signals an anomaly. This approach is particularly effective because LSTMs are well-suited to capture temporal dependencies, making them ideal for time series data. The model works without requiring labeled datasets, making it an unsupervised solution. The authors test the model on both synthetic and real-world data, such as sound event detection, showing its applicability across different domains. The LSTM autoencoder method in this paper focuses on unsupervised anomaly detection, where it is trained to recognize anomalies by learning the normal patterns in time series data. It can effectively detect outliers based on reconstruction error but does not inherently have the capacity for ZSL.

%UAE-TENN
\cite{nivarthi_multi-task_2023} are the first to use a Unified Autoencoder (UAE) for time series data, namely the power forecast of wind and solar plants. They contribute to the challenge of predicting the possible outcome of renewable energy in a newly created plant, either wind or solar. To do so a UAE is combined with a Task Embedding Neural Network (TENN) They examine the usability divided in Single-Task, Multi-Task and Zero-Shot Learning. The method was first published in \cite{nivarthi_unified_2022}. It is then extended by convolutional layers instead of the fully connected neural network layers (UCAE-TENN) and also Long Short-Term Memory layers (ULAE-TENN).

% MAEDAY
Realising few-shot anomaly detection of images is done by \cite{schwartz_maeday_2024}. The method MAEDAY can detect objects newly added to the frames. To achieve this a masked autoencoder is used who recreates the former image but without the anomaly. The difference between the initial and reconstructed images is calculated and the object then visible.
This method is useful for its ability to detect anomalies with very few examples, making it a powerful tool in scenarios where labeled data is rare. \cite{schwartz_maeday_2024} demonstrate the effectiveness of MAEDAY in various applications, showcasing its potential for real-world anomaly detection tasks.
% TODO why important for time series data?

% VRAE based
To detect anomalies in healthcare data a variational recurrent autoencoder is used by \cite{pereira_learning_2019}.
The model is trained on electrocardiogram (ECG) datasets. Their method tackles the challenge of finding anomalies in unlabelled time series data. They created an unsupervised framework where the model learns to represent the data and detect anomalies without needing labeled examples.
The model is based on Variational Recurrent Autoencoders (VRAE) and works by learning to reconstruct the input sequences. During training, they add noise to the input data, and the model tries to reconstruct the original, uncorrupted data. This helps the model learn more robust representations of the data. To detect anomalies, they cluster these learned representations and calculate the distance to identify outliers. Their approach was tested on the ECG5000 dataset and showed that it could effectively detect unusual heartbeats, performing better than previous methods that required labeled data. The model is designed to capture temporal dependencies, making it applicable for MVTSD.

%
Another approach using VRAE involves creating synthetic anomalies to improve the detection process. In their method, they use a two-level hierarchical latent space representation. First, they distill feature descriptors of normal data points into more robust representations using autoencoders (AEs). These representations are then refined using a variational autoencoder (VAE) that creates a family of distributions. From these distributions, they select those that lie on the outskirts of the normal data as generators of synthetic anomalies.
By generating these synthetic anomalies, they train binary classifiers to distinguish between normal and abnormal data. Their hierarchical structure for feature distillation and fusion helps create robust representations, enabling effective anomaly detection without needing actual anomalies during training. Their method performs well on several benchmarks for anomaly detection  \cite{ramirez_rivera_anomaly_2022}.

% Unknwon CyberAttacks
The paper by \cite{zhang_unknown_2020} addresses the challenge of detecting unknown cyberattacks by applying zero-shot learning. The proposed method maps the features of known attacks to a semantic space using a sparse autoencoder and restores them to the feature space by minimizing reconstruction errors, effectively creating a mapping between features and semantic attributes. This technique enables the model to detect previously unseen attacks by generalizing from known attack features. The method was tested on the NSL-KDD dataset, achieving an accuracy of 88.3\%, outperforming traditional approaches in detecting unknown attacks. The research highlights the feasibility and effectiveness of zero-shot learning for cybersecurity applications.
% TODO how to implement on other time series data tasks?
% \cite{zhang_debiased_2024} point out, that AE-based methods have remaining challenges.
% TODO what are the challenges of autoencoders?

% TODO
\cite{han_learning_2022}

% TODO
\cite{pham_mst-vae_2022}

% TODO
\cite{mou_deep_2023}

% VRQRAE TODO
\cite{kieu_anomaly_2022}

% TCN-AE TODO
\cite{thill_temporal_2021}

% MEGA TODO
\cite{wang_multiscale_2023}

% DAEMONTODO
\cite{chen_adversarial_2023}

% TSMAE TODO
\cite{gao_tsmae_2023}

% GRU-AE TODO
\cite{gong_autoencoder-based_2022}

% TODO evtl auch transformer
\cite{wang_attention-based_2022}

% MOMENT
To overcome the challenge of poorly available time series data sets \cite{ma_survey_2023}, the model family MOMENT tries to learn general patterns on a pile of time series data. The pile is a collection of different datasets which they assembled for their pretraining. According to the paper minimal finetuning is needed to perform well on time series tasks like anomaly detection. They published the model and made the usage easily accessible with its own python library. The time serie datasets the model is trained on consist of domains including weather measurements, sensor values and power consumption datasets. Mulitvariate! They also included tongue and finger movement of humans. The different tasks which the model is evaluated on are forecasting (long and short horizon), classification, anomaly detection and imputation. Except for short-horizon forecasting all tasks are managed well. However it cannot detect anomalies in vertically shifted time series \cite{goswami_moment_2024}.

\subsubsection{Transformer}
% AnomalyTransformer
\cite{xu_anomaly_2022} use a Transformer architecture to detect anomalies on three different datasets. TODO



% DATN TODO
\cite{wu_decompose_2023}

% TiSAT TODO
\cite{doshi_tisat_2022}

% TCF-Trans TODO
\cite{peng_tcf-trans_2023}

% DCT-GAN TODO
\cite{li_dct-gan_2023}

% GRU-AE TODO
\cite{shin_time_2023}


% LLMs
Based on the transformer architecture, Large Language Models (LLM) are developed and used increasingly in different applications. Known as chatbots they can help in language specific tasks. Beside that they can be used in anomaly detection and forecasting. \cite{su_large_2024} examine a literature review on how LLMs perform on anomaly detection tasks concerning time series data. LLMs in anomaly detection are specifically useful when the time series data is in the form of words. This can be the case in log analysis. Logs are generated over time and hold a lot of information which can singify errors and system failures. They conclude that LLMs have potential in detecting anomalies but challenges remain. The occurrence of hallucinations and the need for computational efficiency to name a few.

A method detecting anomalies in tabular data using LLMs is presented by \cite{li_anomaly_2024}. In order to perform tasks that LLMs are not directly build for they generate synthetic datasets. Using these datasets LLMs and specifically GPT-4 have comparable performance with transductive learning methods \cite[p. 6]{li_anomaly_2024}.

% Fourier
Using Fourier Analysis and the transformer architecture \cite{ye_multivariate_2023} detect anomalies in time series data. The encoder of a transformer is used to capture temporal features of time series. They detect frequencies by using Fourier Analysis.

\subsubsection{Shapelet Learning}
% Shapelets: Shapelets are short, representative subsequences extracted from time series data.
% Shapelet Learning: Extraction, Evaluation and Selection of shapelets. Medling
\cite{beggel_time_2019} address the problem of detecting anomalies in time series data using a novel unsupervised method based on shapelet learning. This approach is particularly useful in scenarios where labeling data is difficult and expensive.
Their method learns representative features that describe the shape of time series data from the normal class and simultaneously learns to accurately detect anomalies. The objective function encourages the learning of a feature representation in which normal time series lie within a compact hypersphere, while anomalous observations lie outside the decision boundary. This is achieved through a block-coordinate descent procedure.
The advantage of their approach is that it can efficiently detect anomalies in unseen test data without retraining the model, by reusing the learned feature representation. Experimental results on multiple benchmark datasets demonstrate the robustness and reliability of the method in detecting anomalous time series, outperforming competing methods when the training data contains anomalies.

% Alshaer
In contrast, \cite{alshaer_detecting_2020} propose a method combining matrix profiles with shapelet learning to handle streaming time series data. The matrix profile efficiently identifies potential anomalies in real-time, and shapelet learning characterizes these anomalies accurately. This approach is particularly suited for environments requiring immediate anomaly detection, such as finance, healthcare, and industrial monitoring.

% shapelet summary
While both methods utilize shapelet learning, Beggel et al. focus on static datasets and robust feature representation, whereas Alshaer et al. emphasize real-time detection in dynamic, streaming environments.
% TODO search for GAN based methods

% SL based TODO
\cite{liang_shapelet-based_2024}

% IPS TODO
\cite{li_ips_2022}

\subsubsection{Other}
% Clustering
\cite{li_clustering-based_2021} propose a method for detecting anomalies in multivariate time series by combining clustering and reconstruction techniques. The authors use a sliding window approach to generate subsequences from the multivariate time series, then apply extended fuzzy clustering to reveal the underlying structure of the subsequences. By reconstructing these subsequences with optimal cluster centers, the method detects anomalies based on how well the reconstructed data fits the original subsequences. A confidence index quantifies the level of detected anomalies. The approach is shown to effectively detect anomalies related to amplitude and shape changes in various application domains, such as healthcare and finance, and is optimized using Particle Swarm Optimization.

% TODO include benchmark methods from \cite[p. 19]{darban_carla_2024}

% LSTM based TODO
\cite{niu_lstm-based_2020}
% Moving Memory Dynamic Filter TODO
\cite{duan_unsupervised_2021}
