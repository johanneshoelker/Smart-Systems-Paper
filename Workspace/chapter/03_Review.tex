\chapter{Representation Learning for Time Series Data}\label{review}
In this chapter the found literature is put into context. Starting with classical literature about the fundamental findings followed by actual trends in the Area of Representation Learning. Finally the different Representation Learning Strategies are listed and compared.
\section{Historical view}
In this chapter the fundamental literature about the topic is going to be discussed. \\
Sensors and comparable applications produce time series data points which on a closer look may not make sense. They can vary in an unforeseen way and for a short time window they may be completely random. We have to step back and observe longer time periods which could be days or weeks. Or, for very dense measuring it is shorter but there are way more data points to handle.\\
Sometimes it is possible for a human to see some patterns in the data when observing a long time window. Take for example the measuring of a solar plant. On a daily basis it is obvious to see the sun rising and setting, depending on the voltage of the panels. Starting at 0 at night the voltage is rising before noon and descending in the afternoon. This is one representation in the data. But there could be more represenations hidden, which are not likely to see. The shadow of a tree wandering over the panels happening every day or a one time event like the snow covering the plant. \\
These variations in data are not always visible for a human and even less possible to label them accordingly. Like \cite{bengio_representation_2013} mentioned it is important for artificial intelligence to detect these representations in data by machines. A machine should be able to extract information hidden in the low-level sensor measurings and continue working with the representations instead of the raw data. This is according to the paper the main requirement for a good representation, to be able using it as an input to a supervised predictor.\\
Since the paper came out in 2013, several representation learning techniques were developed and some of them are directly applicable for time series data. In \cite{sun_survey_2021} the importance of machine learning in sensor data is emphasized. They sum up several deep learning techniques on data-driven soft-sensors. Soft-sensors represent hard to measure variables by adapting available sensor data. Their observation of industry processes is a rapidly changing field which demands data processing for a huge amount of data.
\section{Trends}
Based on the presented fundamental literature the up-to-date papers are presented in the chapter.
%         Aktuelle Entwicklungen: Überblick über die neuesten Forschungsergebnisse und Trends.
\section{Representation Learning Strategies}
The different RL strategies are listed, explained and compared.\\
% Paper Debiased Contrastive Learning
Learning representations in time series data is tackled in a variety of ways. One solution according to \citeA{zhang_debiased_2024} is debiased contrastive learning. By comparing pairs of data points and rating the similarities as distances between the two, contrastive learning gets less dependant on labeled data. The data can be more general and the extracted representations are more robust. The pairs of data points are labeled as positive and negative pairs with a distance according to their similarities. With this distance they are put into a feature space where they form groups of data points. To minimize the bias between representations multigranularity augmented view generation and expert knowledge are used during training. \\
Contrastive Representation Learning is also used to tackle anomaly detection in time series data by \citeA{darban_carla_2024}. They use CL combined with synthetic anomaly injection. CL enables them to capture patterns in time series data and the framework shows good results on common real world datasets.\\
\citeA{nivarthi_multi-task_2023} are the first to use a Unified Autoencoder (UAE) for time series data, namely the power forecast of wind and solar plants. They contribute to the challenge of predicting the possible outcome of renewable energy in a newly created plant, either wind or solar. To do so a UAE is combined with a Task Embedding Neural Network (TENN) They examine the usability divided in Single-Task, Multi-Task and Zero-Shot Learning. The method was first published in \citeA{nivarthi_unified_2022}. It is then extended by convolutional layers instead of the fully connected neural network layers (UCAE-TENN)
and also Long Short-Term Memory layers (ULAE-TENN).\\
\subsection{MOMENT}
To overcome the challenge of poorly available time series data sets \cite{ma_survey_2023}, the model family MOMENT tries to learn general patterns on a pile of time series data \cite{goswami_moment_2024}. The pile is a collection of different datasets which they assembled for their pretraining. According to the paper minimal finetuning is needed to perform well on time series tasks like anomaly detection. They published the model and made the usage easily accessible with its own python library. The constructed time serie pile consists of a widespread list of domains including Weather measurements, sensor values and power consumption datasets. They also included data not connected with the previous like the tongue and finger movement of humans. The different tasks which the model is evaluated on are forecasting (long and short horizon), classification, anomaly detection and imputation. Except for short-horizon forecasting all tasks are managed well. \\
\subsection{CARLA}
