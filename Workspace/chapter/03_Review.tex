\chapter{Representation Learning for Time Series Data}\label{review}
In this chapter the found literature is put into context. Starting with classical literature about the fundamental findings followed by actual trends in the Area of Representation Learning. Finally the different Representation Learning Strategies are listed and compared.
\section{Historical view}
In this chapter the fundamental literature about the topic is going to be discussed. \\
Sensors and comparable applications produce time series data points which on a closer look may not make sense. They can vary in an unforeseen way and for a short time window they may be completely random. We have to step back and observe longer time periods which could be days or weeks. Or, for very dense measuring it is shorter but there are way more data points to handle.\\
Sometimes it is possible for a human to see some patterns in the data when observing a long time window. Take for example the measuring of a solar plant. On a daily basis it is obvious to see the sun rising and setting, depending on the voltage of the panels. Starting at 0 at night the voltage is rising before noon and descending in the afternoon. This is one representation in the data. But there could be more represenations hidden, which are not likely to see. The shadow of a tree wandering over the panels happening every day or a one time event like the snow covering the plant. \\
These variations in data are not always visible for a human and even less possible to label them accordingly. Like \cite{bengio_representation_2013} mentioned it is important for artificial intelligence to detect these representations in data by machines. A machine should be able to extract information hidden in the low-level sensor measurings and continue working with the representations instead of the raw data. This is according to the paper the main requirement for a good representation, to be able using it as an input to a supervised predictor.\\
Since the paper came out in 2013, several representation learning techniques were developed and some of them are directly applicable for time series data. In \cite{sun_survey_2021} the importance of machine learning in sensor data is emphasized. They sum up several deep learning techniques on data-driven soft-sensors. Soft-sensors represent hard to measure variables by adapting available sensor data. Their observation of industry processes is a rapidly changing field which demands data processing for a huge amount of data.
\section{Trends}
Based on the presented fundamental literature the up-to-date papers are presented in the chapter.
%         Aktuelle Entwicklungen: Überblick über die neuesten Forschungsergebnisse und Trends.
\section{Representation Learning Strategies}
The different RL strategies are listed, explained and compared.\\
TODO\\
% Paper Debiased Contrastive Learning
Learning representations in time series data is tackled in a variety of ways. One solution according to \citeA{zhang_debiased_2024} is debiased contrastive learning. By comparing pairs of data points and rating the similarities as distances between the two, contrastive learning gets less dependant on labeled data. The data can be more general and the extracted representations are more robust. The pairs of data points are labeled as positive and negative pairs with a distance according to their similarities. With this distance they are put into a feature space where they form groups of data points. To minimize the bias between representations multigranularity augmented view generation and expert knowledge are used during training. \\
\citeA{nivarthi_multi-task_2023} are the first to use a Unified Autoencoder (UAE) for time series data, namely the power forecast of wind and solar plants. They face the challenge of predicting the possible outcome of renewable energy in a newly created plant, either wind or solar. They examine the usability divided in Single-Task, Multi-Task and Zero-Shot Learning
