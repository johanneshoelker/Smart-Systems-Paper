
@misc{yue_ts2vec_2022,
	title = {{TS2Vec}: {Towards} {Universal} {Representation} of {Time} {Series}},
	shorttitle = {{TS2Vec}},
	url = {http://arxiv.org/abs/2106.10466},
	abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classiﬁcation tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves signiﬁcant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
	language = {en},
	urldate = {2024-08-03},
	publisher = {arXiv},
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	month = feb,
	year = {2022},
	note = {arXiv:2106.10466 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{li_anomaly_2024,
	title = {Anomaly {Detection} of {Tabular} {Data} {Using} {LLMs}},
	url = {http://arxiv.org/abs/2406.16308},
	abstract = {Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zeroshot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective datagenerating processes to simulate synthetic batchlevel anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Li, Aodong and Zhao, Yunhan and Qiu, Chen and Kloft, Marius and Smyth, Padhraic and Rudolph, Maja and Mandt, Stephan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16308 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{fung_model_2024,
	title = {Model {Selection} of {Zero}-shot {Anomaly} {Detectors} in the {Absence} of {Labeled} {Validation} {Data}},
	url = {http://arxiv.org/abs/2310.10461},
	abstract = {Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data—without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors with a generated synthetic validation set. Our proposed anomaly generation method assumes access to only a small support set of normal images and requires no training or finetuning. Once generated, our synthetic validation set is used to create detection tasks that compose a validation framework for model selection. In an empirical study, we find that SWSA often selects models that match selections made with a groundtruth validation set, resulting in higher AUROCs than baseline methods. We also find that SWSA selects prompts for CLIP-based anomaly detection that outperform baseline prompt selection strategies on all datasets, including the challenging MVTec-AD and VisA datasets.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Fung, Clement and Qiu, Chen and Li, Aodong and Rudolph, Maja},
	month = feb,
	year = {2024},
	note = {arXiv:2310.10461 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhou_anomalyclip_2024,
	title = {{AnomalyCLIP}: {Object}-agnostic {Prompt} {Learning} for {Zero}-shot {Anomaly} {Detection}},
	shorttitle = {{AnomalyCLIP}},
	url = {http://arxiv.org/abs/2310.18961},
	abstract = {Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Zhou, Qihang and Pang, Guansong and Tian, Yu and He, Shibo and Chen, Jiming},
	month = mar,
	year = {2024},
	note = {arXiv:2310.18961 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jiao_timeautoad_2022,
	title = {{TimeAutoAD}: {Autonomous} {Anomaly} {Detection} {With} {Self}-{Supervised} {Contrastive} {Loss} for {Multivariate} {Time} {Series}},
	volume = {9},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4697, 2334-329X},
	shorttitle = {{TimeAutoAD}},
	url = {https://ieeexplore.ieee.org/document/9705079/},
	doi = {10.1109/TNSE.2022.3148276},
	abstract = {Multivariate time series (MTS) data are becoming increasingly ubiquitous in networked systems, e.g., IoT systems and 5G networks. Anomaly detection in MTS refers to identifying time series which exhibit different behaviors from normal status. Building such a system, however, is challenging due to a few reasons: i) labels for anomaly cases are usually unavailable or very rare; ii) most existing approaches rely on manual model-design and hyperparameter tuning, which may cost a huge amount of labor effort. To this end, we propose an autonomous anomaly detection technique for multivariate time series data (TimeAutoAD) based on a novel self-supervised contrastive loss. Speciﬁcally, we ﬁrst present an automatic anomaly detection pipeline to optimize the model conﬁguration and hyperparameters automatically. Next, we introduce three different strategies to augment the training data for generating pseudo negative time series and employ a self-supervised contrastive loss to distinguish the original time series and the generated time series. In this way, the representation learning capability of TimeAutoAD can be greatly enhanced and the anomaly detection performance can thus be improved. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoAD not only outperforms state-of-the-art anomaly detection approaches but also exhibits robustness when training data are contaminated.},
	language = {en},
	number = {3},
	urldate = {2024-08-01},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Jiao, Yang and Yang, Kai and Song, Dongjing and Tao, Dacheng},
	month = may,
	year = {2022},
	pages = {1604--1619},
}

@article{ngu_cl-tad_2023,
	title = {{CL}-{TAD}: {A} {Contrastive}-{Learning}-{Based} {Method} for {Time} {Series} {Anomaly} {Detection}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {{CL}-{TAD}},
	url = {https://www.mdpi.com/2076-3417/13/21/11938},
	doi = {10.3390/app132111938},
	abstract = {Anomaly detection has gained increasing attention in recent years, but detecting anomalies in time series data remains challenging due to temporal dynamics, label scarcity, and data diversity in real-world applications. To address these challenges, we introduce a novel method for anomaly detection in time series data, called CL-TAD (Contrastive-Learning-based method for Times series Anomaly Detection), which employs a contrastive-learning-based representation learning technique. Inspired by the successes of reconstruction-based approaches and contrastive learning approaches, the proposed method seeks to leverage these approaches for time series anomaly detection. The CL-TAD method is comprised of two main components: positive sample generation and contrastivelearning-based representation learning. The former component generates positive samples by trying to reconstruct the original data from masked samples. These positive samples, in conjunction with the original data, serve as input for the contrastive-learning-based representation learning component. The representations of input original data and their masked data are used to detect anomalies later on. Experimental results have demonstrated that the CL-TAD method achieved the best performance on ﬁve datasets out of nine benchmark datasets over 10 other recent methods. By leveraging the reconstruction learning and contrastive learning techniques, our method offers a promising solution for effectively detecting anomalies in time series data by handling the issues raised by label scarcity and data diversity, delivering high performance.},
	language = {en},
	number = {21},
	urldate = {2024-08-01},
	journal = {Applied Sciences},
	author = {Ngu, Huynh Cong Viet and Lee, Keon Myung},
	month = oct,
	year = {2023},
	pages = {11938},
}

@misc{jeong_time-series_2022,
	title = {Time-{Series} {Anomaly} {Detection} with {Implicit} {Neural} {Representation}},
	url = {http://arxiv.org/abs/2201.11950},
	abstract = {Detecting anomalies in multivariate time-series data is essential in many real-world applications. Recently, various deep learning-based approaches have shown considerable improvements in timeseries anomaly detection. However, existing methods still have several limitations, such as long training time due to their complex model designs or costly tuning procedures to ﬁnd optimal hyperparameters (e.g., sliding window length) for a given dataset. In our paper, we propose a novel method called Implicit Neural Representationbased Anomaly Detection (INRAD). Speciﬁcally, we train a simple multi-layer perceptron that takes time as input and outputs corresponding values at that time. Then we utilize the representation error as an anomaly score for detecting anomalies. Experiments on ﬁve real-world datasets demonstrate that our proposed method outperforms other state-of-the-art methods in performance, training speed, and robustness.},
	language = {en},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Jeong, Kyeong-Joong and Shin, Yong-Min},
	month = jan,
	year = {2022},
	note = {arXiv:2201.11950 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@book{gardenfors_conceptual_2000,
	address = {Cambridge, Mass},
	title = {Conceptual spaces: the geometry of thought},
	isbn = {978-0-262-07199-4},
	shorttitle = {Conceptual spaces},
	publisher = {MIT Press},
	author = {Gärdenfors, Peter},
	year = {2000},
	keywords = {Artificial intelligence, Cognitive science},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@misc{darban_carla_2024,
	title = {{CARLA}: {Self}-supervised {Contrastive} {Representation} {Learning} for {Time} {Series} {Anomaly} {Detection}},
	shorttitle = {{CARLA}},
	url = {http://arxiv.org/abs/2308.09296},
	abstract = {One main challenge in time series anomaly detection (TSAD) is the lack of labelled data in many real-life scenarios. Most of the existing anomaly detection methods focus on learning the normal behaviour of unlabelled time series in an unsupervised manner. The normal boundary is often defined tightly, resulting in slight deviations being classified as anomalies, consequently leading to a high false positive rate and a limited ability to generalise normal patterns. To address this, we introduce a novel end-to-end self-supervised ContrAstive Representation Learning approach for time series Anomaly detection (CARLA). While existing contrastive learning methods assume that augmented time series windows are positive samples and temporally distant windows are negative samples, we argue that these assumptions are limited as augmentation of time series can transform them to negative samples, and a temporally distant window can represent a positive sample. Our contrastive approach leverages existing generic knowledge about time series anomalies and injects various types of anomalies as negative samples. Therefore, CARLA not only learns normal behaviour but also learns deviations indicating anomalies. It creates similar representations for temporally closed windows and distinct ones for anomalies. Additionally, it leverages the information about representations' neighbours through a self-supervised approach to classify windows based on their nearest/furthest neighbours to further enhance the performance of anomaly detection. In extensive tests on seven major real-world time series anomaly detection datasets, CARLA shows superior performance over state-of-the-art self-supervised and unsupervised TSAD methods. Our research shows the potential of contrastive representation learning to advance time series anomaly detection.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Darban, Zahra Zamanzadeh and Webb, Geoffrey I. and Pan, Shirui and Aggarwal, Charu C. and Salehi, Mahsa},
	month = apr,
	year = {2024},
	note = {arXiv:2308.09296 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{gruhl_novelty_2022,
	title = {Novelty {Detection} for {Multivariate} {Data} {Streams} with {Probalistic} {Models}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, open access},
	url = {https://kobra.uni-kassel.de/handle/123456789/13902},
	doi = {10.17170/KOBRA-202205106160},
	language = {en},
	urldate = {2024-07-15},
	author = {Gruhl, Christian M.},
	collaborator = {{Universität Kassel}},
	year = {2022},
	note = {Publisher: Universität Kassel},
	keywords = {620, Anomalieerkennung, Autonomer Agent, Daten, Datenstrom, Maschinelles Lernen, Sensor, Wahrscheinlichkeitsmaß, anomalies, anomaly detection, concept drift, data streams, machine learning, novelties, novelty detection, outliers, probabilistic modeling},
}

@article{palatucci_zero-shot_2009,
	title = {Zero-shot {Learning} with {Semantic} {Output} {Codes}},
	abstract = {We consider the problem of zero-shot learning, where the goal is to learn a classiﬁer f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we deﬁne the notion of a semantic output code classiﬁer (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classiﬁer and study its theoretical properties in a PAC framework, showing conditions under which the classiﬁer can accurately predict novel classes. As a case study, we build a SOC classiﬁer for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.},
	language = {en},
	author = {Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
	year = {2009},
}

@misc{socher_zero-shot_2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	url = {http://arxiv.org/abs/1301.3666},
	abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by ﬁrst using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually deﬁned semantic features for either words or images.},
	language = {en},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
	month = mar,
	year = {2013},
	note = {arXiv:1301.3666 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
