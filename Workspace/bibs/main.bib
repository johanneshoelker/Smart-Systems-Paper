
@article{zhuang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	volume = {109},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9134370/},
	doi = {10.1109/JPROC.2020.3004555},
	language = {en},
	number = {1},
	urldate = {2024-07-03},
	journal = {Proceedings of the IEEE},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jan,
	year = {2021},
	pages = {43--76},
}

@misc{liang_shapelet-based_2024,
	title = {A {Shapelet}-based {Framework} for {Unsupervised} {Multivariate} {Time} {Series} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2305.18888},
	doi = {10.14778/3632093.3632103},
	abstract = {Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly designed to achieve our goal, and a data augmentation library is employed to improve the generalization. We conduct extensive experiments using tens of real-world datasets to assess the representation quality on many downstream tasks, including classification, clustering, and anomaly detection. The results demonstrate the superiority of our method against not only URL competitors, but also techniques specially designed for downstream tasks. Our code has been made publicly available at https://github.com/real2fish/CSL.},
	language = {en},
	urldate = {2024-09-07},
	author = {Liang, Zhiyu and Zhang, Jianfeng and Liang, Chen and Wang, Hongzhi and Liang, Zheng and Pan, Lujia},
	month = aug,
	year = {2024},
	note = {arXiv:2305.18888 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a deﬁnition of MTL and then classify different MTL algorithms into ﬁve categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	language = {en},
	number = {12},
	urldate = {2024-07-03},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	pages = {5586--5609},
}

@misc{ma_survey_2023,
	title = {A {Survey} on {Time}-{Series} {Pre}-{Trained} {Models}},
	url = {http://arxiv.org/abs/2305.10716},
	abstract = {Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difﬁcult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Speciﬁcally, we ﬁrst brieﬂy introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work. The source code is available at https://github.com/qianlima-lab/time-series-ptms.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Ma, Qianli and Liu, Zhen and Zheng, Zhenjing and Huang, Ziyang and Zhu, Siying and Yu, Zhongzhong and Kwok, James T.},
	month = may,
	year = {2023},
	note = {arXiv:2305.10716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{chen_adversarial_2023,
	address = {Singapore Singapore},
	title = {Adversarial {Autoencoder} for {Unsupervised} {Time} {Series} {Anomaly} {Detection} and {Interpretation}},
	isbn = {978-1-4503-9407-9},
	url = {https://dl.acm.org/doi/10.1145/3539597.3570371},
	doi = {10.1145/3539597.3570371},
	abstract = {In many complex systems, devices are typically monitored and generating massive multivariate time series. However, due to the complex patterns and little useful labeled data, it is a great challenge to detect anomalies from these time series data. Existing methods either rely on less regularizations, or require a large number of labeled data, leading to poor accuracy in anomaly detection. To overcome the limitations, in this paper, we propose an adversarial autoencoder anomaly detection and interpretation framework named DAEMON, which performs robustly for various datasets. The key idea is to use two discriminators to adversarially train an autoencoder to learn the normal pattern of multivariate time series, and thereafter use the reconstruction error to detect anomalies. The robustness of DAEMON is guaranteed by the regularization of hidden variables and reconstructed data using the adversarial generation method. An unsupervised approach used to detect anomalies is proposed. Moreover, in order to help operators better diagnose anomalies, DAEMON provides anomaly interpretation by computing the gradients of anomalous data. An extensive empirical study on real data offers evidence that the framework is capable of outperforming state-of-the-art methods in terms of the overall F1-score and interpretation accuracy for time series anomaly detection.},
	language = {en},
	urldate = {2024-09-06},
	booktitle = {Proceedings of the {Sixteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Xuanhao and Deng, Liwei and Zhao, Yan and Zheng, Kai},
	month = feb,
	year = {2023},
	pages = {267--275},
}

@article{garg_evaluation_2022,
	title = {An {Evaluation} of {Anomaly} {Detection} and {Diagnosis} in {Multivariate} {Time} {Series}},
	volume = {33},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9525836/},
	doi = {10.1109/TNNLS.2021.3105827},
	abstract = {Several techniques for multivariate time series anomaly detection have been proposed recently, but a systematic comparison on a common set of datasets and metrics is lacking. This article presents a systematic and comprehensive evaluation of unsupervised and semisupervised deep-learning-based methods for anomaly detection and diagnosis on multivariate time series data from cyberphysical systems. Unlike previous works, we vary the model and post-processing of model errors, i.e., the scoring functions independently of each other, through a grid of ten models and four scoring functions, comparing these variants to state-of-the-art methods. In time-series anomaly detection, detecting anomalous events is more important than detecting individual anomalous time points. Through experiments, we ﬁnd that the existing evaluation metrics either do not take events into account or cannot distinguish between a good detector and trivial detectors, such as a random or an all-positive detector. We propose a new metric to overcome these drawbacks, namely, the composite F-score (Fc1), for evaluating time-series anomaly detection. Our study highlights that dynamic scoring functions work much better than static ones for multivariate time series anomaly detection, and the choice of scoring functions often matters more than the choice of the underlying model. We also ﬁnd that a simple, channel-wise model—the univariate fully connected auto-encoder, with the dynamic Gaussian scoring function emerges as a winning candidate for both anomaly detection and diagnosis, beating state-of-the-art algorithms.},
	language = {en},
	number = {6},
	urldate = {2024-08-25},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Garg, Astha and Zhang, Wenyu and Samaran, Jules and Savitha, Ramasamy and Foo, Chuan-Sheng},
	month = jun,
	year = {2022},
	pages = {2508--2517},
}

@misc{miao_unsupervised_2022,
	title = {An {Unsupervised} {Short}- and {Long}-{Term} {Mask} {Representation} for {Multivariate} {Time} {Series} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2208.09240},
	abstract = {Anomaly detection of multivariate time series is meaningful for system behavior monitoring. This paper proposes an anomaly detection method based on unsupervised Short- and Long-term Mask Representation learning(SLMR). The main idea is to extract short-term local dependency patterns and long-term global trend patterns by using multi-scale residual dilated convolution and Gated Recurrent Unit(GRU) respectively. Furthermore, our approach can comprehend temporal contexts and feature correlations by combining spatial-temporal masked selfsupervised representation learning and sequence split. It considers the importance of features is diﬀerent, and we introduce the attention mechanism to adjust the contribution of each feature. Finally, a forecastingbased model and a reconstruction-based model are integrated to focus on single timestamp prediction and latent representation of time series. Experiments show that the performance of our method outperforms other state-of-the-art models on three real-world datasets. Further analysis shows that our method is good at anomaly localization.},
	language = {en},
	urldate = {2024-09-08},
	publisher = {arXiv},
	author = {Miao, Qiucheng and Xu, Chuanfu and Zhan, Jun and Zhu, Dong and Wu, Chengkun},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{ramirez_rivera_anomaly_2022,
	title = {Anomaly {Detection} {Based} on {Zero}-{Shot} {Outlier} {Synthesis} and {Hierarchical} {Feature} {Distillation}},
	volume = {33},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9228891/},
	doi = {10.1109/TNNLS.2020.3027667},
	abstract = {Anomaly detection suffers from unbalanced data since anomalies are quite rare. Synthetically generated anomalies are a solution to such ill or not fully deﬁned data. However, synthesis requires an expressive representation to guarantee the quality of the generated data. In this article, we propose a two-level hierarchical latent space representation that distills inliers’ feature descriptors [through autoencoders (AEs)] into more robust representations based on a variational family of distributions (through a variational AE) for zero-shot anomaly generation. From the learned latent distributions, we select those that lie on the outskirts of the training data as synthetic-outlier generators. Also, we synthesize from them, i.e., generate negative samples without seen them before, to train binary classiﬁers. We found that the use of the proposed hierarchical structure for feature distillation and fusion creates robust and general representations that allow us to synthesize pseudo outlier samples. Also, in turn, train robust binary classiﬁers for true outlier detection (without the need for actual outliers during training). We demonstrate the performance of our proposal on several benchmarks for anomaly detection.},
	language = {en},
	number = {1},
	urldate = {2024-07-30},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Ramirez Rivera, Adin and Khan, Adil and Bekkouch, Imad Eddine Ibrahim and Sheikh, Taimoor Shakeel},
	month = jan,
	year = {2022},
	pages = {281--291},
}

@inproceedings{kieu_anomaly_2022,
	title = {Anomaly {Detection} in {Time} {Series} with {Robust} {Variational} {Quasi}-{Recurrent} {Autoencoders}},
	url = {https://ieeexplore.ieee.org/document/9835268/?arnumber=9835268},
	doi = {10.1109/ICDE53745.2022.00105},
	abstract = {We propose variational quasi-recurrent autoencoders (VQRAEs) to enable robust and efficient anomaly detection in time series in unsupervised settings. The proposed VQRAEs employs a judiciously designed objective function based on robust divergences, including a, ß, and, -divergence, making it possible to separate anomalies from normal data without the reliance on anomaly labels, thus achieving robustness and fully unsupervised training. To better capture temporal dependencies in time series data, VQRAEs are built upon quasi-recurrent neural networks, which employ convolution and gating mechanisms to avoid the inefficient recursive computations used by classic recurrent neural networks. Further, VQRAEs can be extended to bi-directional Bi VQRAEs that utilize bi-directional information to further improve the accuracy. The above design choices make VQRAEs not only robust and thus accurate, but also efficient at detecting anomalies in streaming settings. Experiments on five real-world time series offer insight into the design properties of VQRAEs and demonstrate that VQRAEs are capable of outperforming state-of-the-art methods.},
	urldate = {2024-09-06},
	booktitle = {2022 {IEEE} 38th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Kieu, Tung and Yang, Bin and Guo, Chenjuan and Cirstea, Razvan-Gabriel and Zhao, Yan and Song, Yale and Jensen, Christian S.},
	month = may,
	year = {2022},
	note = {ISSN: 2375-026X},
	keywords = {Bidirectional control, Computational modeling, Convolution, Stochastic processes, Time series analysis, Training, Transportation},
	pages = {1342--1354},
}

@article{schmidl_anomaly_2022,
	title = {Anomaly detection in time series: a comprehensive evaluation},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Anomaly detection in time series},
	url = {https://dl.acm.org/doi/10.14778/3538598.3538602},
	doi = {10.14778/3538598.3538602},
	abstract = {Detecting anomalous subsequences in time series data is an important task in areas ranging from manufacturing processes over finance applications to health care monitoring. An anomaly can indicate important events, such as production faults, delivery bottlenecks, system defects, or heart flicker, and is therefore of central interest. Because time series are often large and exhibit complex patterns, data scientists have developed various specialized algorithms for the automatic detection of such anomalous patterns. The number and variety of anomaly detection algorithms has grown significantly in the past and, because many of these solutions have been developed independently and by different research communities, there is no comprehensive study that systematically evaluates and compares the different approaches. For this reason, choosing the best detection technique for a given anomaly detection task is a difficult challenge.},
	language = {en},
	number = {9},
	urldate = {2024-08-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schmidl, Sebastian and Wenig, Phillip and Papenbrock, Thorsten},
	month = may,
	year = {2022},
	pages = {1779--1797},
}

@misc{li_anomaly_2024,
	title = {Anomaly {Detection} of {Tabular} {Data} {Using} {LLMs}},
	url = {http://arxiv.org/abs/2406.16308},
	abstract = {Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zeroshot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective datagenerating processes to simulate synthetic batchlevel anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Li, Aodong and Zhao, Yunhan and Qiu, Chen and Kloft, Marius and Smyth, Padhraic and Rudolph, Maja and Mandt, Stephan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16308 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_anomaly_2022,
	title = {Anomaly {Transformer}: {Time} {Series} {Anomaly} {Detection} with {Association} {Discrepancy}},
	shorttitle = {Anomaly {Transformer}},
	url = {http://arxiv.org/abs/2110.02642},
	abstract = {Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufﬁcient to reason about the intricate dynamics. Recently, Transformers have shown great power in uniﬁed modeling of pointwise representation and pairwise association, and we ﬁnd that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difﬁcult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies’ associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-theart results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space \& earth exploration, and water treatment.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Xu, Jiehui and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
	month = jun,
	year = {2022},
	note = {arXiv:2110.02642 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{wang_attention-based_2022,
	address = {Mexico City, Mexico},
	title = {Attention-based {Representation} {Learning} for {Time} {Series} with {Principal} and {Residual} {Space} {Monitoring}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-042-9},
	url = {https://ieeexplore.ieee.org/document/9926721/},
	doi = {10.1109/CASE49997.2022.9926721},
	abstract = {The encoder-decoder network is one of the most common deep learning models for time series representation learning and anomaly detection. However, it is hard to reconstruct time series, which is complex, correlated, and lacking in common patterns. In this paper, we apply the attention mechanism to rescale convolution layers and learn representation in the principal and the residual space. To avoid the reconstruction process, we define the residual space by the omitted segments according to the attention score in the encoder. We introduce the temporal information inside the token level and use sparse penalty to improve representation learning. We apply the proposed model to anomaly classification and fault detection experiments on two datasets, i.e. multivariate bearing fault dataset and UCRArchive profile dataset. The result shows that the representation learned by the proposed model is more likely to cluster by category, especially in the residual space. Compared to the baselines and state-of-the-art models, the proposed model has higher accuracy and recall in the limitedlabeled situation, which illustrates the stability of the learned representation and its superiority in the downstream tasks.},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	publisher = {IEEE},
	author = {Wang, Botao and Tsung, Fugee and Yan, Hao},
	month = aug,
	year = {2022},
	pages = {1833--1839},
}

@inproceedings{gong_autoencoder-based_2022,
	title = {Autoencoder-{Based} {Anomaly} {Detection} for {Time} {Series} {Data} in {Complex} {Systems}},
	url = {https://ieeexplore.ieee.org/document/10090260/?arnumber=10090260},
	doi = {10.1109/APCCAS55924.2022.10090260},
	abstract = {In this paper, we present a new anomaly detection method for time-series data in complex systems such as power grid and cellular networks. The proposed anomaly detection method is developed following unsupervised learning, where an AutoEncoder based on Gated Recurrent Units (GRU-AE) is trained to reconstruct a time-series of interest, and anomalies are detected via detecting exceptionally large reconstruction errors. A multi-timestamp stacking method is adopted to reduce the number of time steps in the GRU-AE to facilitate the training of the model and a new training scheme with random shuffling is proposed to prevent overfitting. The proposed GRU-AE based detector is applied in multiple time scales to detect different types of anomalies. Numerical results obtained via time-series data from real cellular network demonstrate the performance of the proposed method.},
	urldate = {2024-09-06},
	booktitle = {2022 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems} ({APCCAS})},
	author = {Gong, Xundong and Liao, Shibo and Hu, Fei and Hu, Xiaoqing and Liu, Chunshan},
	month = nov,
	year = {2022},
	keywords = {AutoEncoder, Cellular networks, Detectors, Numerical models, Power grids, Stacking, Time series analysis, Training, anomaly detection, multi-timestamp stacking, random shuffling, time series},
	pages = {428--433},
}

@misc{darban_carla_2024,
	title = {{CARLA}: {Self}-supervised {Contrastive} {Representation} {Learning} for {Time} {Series} {Anomaly} {Detection}},
	shorttitle = {{CARLA}},
	url = {http://arxiv.org/abs/2308.09296},
	abstract = {One main challenge in time series anomaly detection (TSAD) is the lack of labelled data in many real-life scenarios. Most of the existing anomaly detection methods focus on learning the normal behaviour of unlabelled time series in an unsupervised manner. The normal boundary is often defined tightly, resulting in slight deviations being classified as anomalies, consequently leading to a high false positive rate and a limited ability to generalise normal patterns. To address this, we introduce a novel end-to-end self-supervised ContrAstive Representation Learning approach for time series Anomaly detection (CARLA). While existing contrastive learning methods assume that augmented time series windows are positive samples and temporally distant windows are negative samples, we argue that these assumptions are limited as augmentation of time series can transform them to negative samples, and a temporally distant window can represent a positive sample. Our contrastive approach leverages existing generic knowledge about time series anomalies and injects various types of anomalies as negative samples. Therefore, CARLA not only learns normal behaviour but also learns deviations indicating anomalies. It creates similar representations for temporally closed windows and distinct ones for anomalies. Additionally, it leverages the information about representations' neighbours through a self-supervised approach to classify windows based on their nearest/furthest neighbours to further enhance the performance of anomaly detection. In extensive tests on seven major real-world time series anomaly detection datasets, CARLA shows superior performance over state-of-the-art self-supervised and unsupervised TSAD methods. Our research shows the potential of contrastive representation learning to advance time series anomaly detection.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Darban, Zahra Zamanzadeh and Webb, Geoffrey I. and Pan, Shirui and Aggarwal, Charu C. and Salehi, Mahsa},
	month = apr,
	year = {2024},
	note = {arXiv:2308.09296 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{ngu_cl-tad_2023,
	title = {{CL}-{TAD}: {A} {Contrastive}-{Learning}-{Based} {Method} for {Time} {Series} {Anomaly} {Detection}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {{CL}-{TAD}},
	url = {https://www.mdpi.com/2076-3417/13/21/11938},
	doi = {10.3390/app132111938},
	abstract = {Anomaly detection has gained increasing attention in recent years, but detecting anomalies in time series data remains challenging due to temporal dynamics, label scarcity, and data diversity in real-world applications. To address these challenges, we introduce a novel method for anomaly detection in time series data, called CL-TAD (Contrastive-Learning-based method for Times series Anomaly Detection), which employs a contrastive-learning-based representation learning technique. Inspired by the successes of reconstruction-based approaches and contrastive learning approaches, the proposed method seeks to leverage these approaches for time series anomaly detection. The CL-TAD method is comprised of two main components: positive sample generation and contrastivelearning-based representation learning. The former component generates positive samples by trying to reconstruct the original data from masked samples. These positive samples, in conjunction with the original data, serve as input for the contrastive-learning-based representation learning component. The representations of input original data and their masked data are used to detect anomalies later on. Experimental results have demonstrated that the CL-TAD method achieved the best performance on ﬁve datasets out of nine benchmark datasets over 10 other recent methods. By leveraging the reconstruction learning and contrastive learning techniques, our method offers a promising solution for effectively detecting anomalies in time series data by handling the issues raised by label scarcity and data diversity, delivering high performance.},
	language = {en},
	number = {21},
	urldate = {2024-08-01},
	journal = {Applied Sciences},
	author = {Ngu, Huynh Cong Viet and Lee, Keon Myung},
	month = oct,
	year = {2023},
	pages = {11938},
}

@article{li_clustering-based_2021,
	title = {Clustering-based anomaly detection in multivariate time series data},
	volume = {100},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494620308577},
	doi = {10.1016/j.asoc.2020.106919},
	abstract = {Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix. We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering, the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.},
	language = {en},
	urldate = {2024-08-24},
	journal = {Applied Soft Computing},
	author = {Li, Jinbo and Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
	month = mar,
	year = {2021},
	pages = {106919},
}

@book{gardenfors_conceptual_2000,
	address = {Cambridge, Mass},
	title = {Conceptual spaces: the geometry of thought},
	isbn = {978-0-262-07199-4},
	shorttitle = {Conceptual spaces},
	publisher = {MIT Press},
	author = {Gärdenfors, Peter},
	year = {2000},
	keywords = {Artificial intelligence, Cognitive science},
}

@misc{pranavan_contrastive_2022,
	title = {Contrastive predictive coding for {Anomaly} {Detection} in {Multi}-variate {Time} {Series} {Data}},
	url = {http://arxiv.org/abs/2202.03639},
	abstract = {Anomaly detection in multi-variate time series (MVTS) data is a huge challenge as it requires simultaneous representation of long term temporal dependencies and correlations across multiple variables. More often, this is solved by breaking the complexity through modeling one dependency at a time. In this paper, we propose a Time-series Representational Learning through Contrastive Predictive Coding (TRL-CPC) towards anomaly detection in MVTS data. First, we jointly optimize an encoder, an auto-regressor and a non-linear transformation function to effectively learn the representations of the MVTS data sets, for predicting future trends. It must be noted that the context vectors are representative of the observation window in the MTVS. Next, the latent representations for the succeeding instants obtained through non-linear transformations of these context vectors, are contrasted with the latent representations of the encoder for the multi-variables such that the density for the positive pair is maximized. Thus, the TRL-CPC helps to model the temporal dependencies and the correlations of the parameters for a healthy signal pattern. Finally, ﬁtting the latent representations are ﬁt into a Gaussian scoring function to detect anomalies. Evaluation of the proposed TRL-CPC on three MVTS data sets against SOTA anomaly detection methods shows the superiority of TRL-CPC.},
	language = {en},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Pranavan, Theivendiram and Sim, Terence and Ambikapathi, Arulmurugan and Ramasamy, Savitha},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03639 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{li_contrastive_2023,
	address = {Gold Coast, Australia},
	title = {Contrastive {Time} {Series} {Anomaly} {Detection} by {Temporal} {Transformations}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66548-867-9},
	url = {https://ieeexplore.ieee.org/document/10191358/},
	doi = {10.1109/IJCNN54540.2023.10191358},
	abstract = {Detecting anomalies in time series data is challenging due to their complex and volatile temporal features. Some anomalies only show deviating patterns to their local context instead of the overall distribution. Additionally, the biased sample distribution between normal and abnormal classes hinders the efficient usage of the available data labels. Self-supervised approaches are practically efficient for anomaly detection, in which only normal data is used during the training. However, they often fail to detect contextual anomalies in high-dimensional time series data, while the representation learning of such complex data patterns is sub-optimal.},
	language = {en},
	urldate = {2024-08-27},
	booktitle = {2023 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Li, Bin and Müller, Emmanuel},
	month = jun,
	year = {2023},
	pages = {1--8},
}

@inproceedings{yang_dcdetector_2023,
	title = {{DCdetector}: {Dual} {Attention} {Contrastive} {Representation} {Learning} for {Time} {Series} {Anomaly} {Detection}},
	shorttitle = {{DCdetector}},
	url = {http://arxiv.org/abs/2306.10347},
	doi = {10.1145/3580305.3599295},
	abstract = {Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior discrimination abilities. Extensive experiments show that DCdetector achieves state-of-the-art results on multiple time series anomaly detection benchmark datasets. Code is publicly available at this URL1.},
	language = {en},
	urldate = {2024-08-27},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Yang, Yiyuan and Zhang, Chaoli and Zhou, Tian and Wen, Qingsong and Sun, Liang},
	month = aug,
	year = {2023},
	note = {arXiv:2306.10347 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {3033--3045},
}

@article{li_dct-gan_2023,
	title = {{DCT}-{GAN}: {Dilated} {Convolutional} {Transformer}-{Based} {GAN} for {Time} {Series} {Anomaly} {Detection}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {{DCT}-{GAN}},
	url = {https://ieeexplore.ieee.org/document/9626552/?arnumber=9626552},
	doi = {10.1109/TKDE.2021.3130234},
	abstract = {Time series anomaly detection (TSAD) is an essential problem faced in several fields, e.g., fault detection, fraud detection, and intrusion detection, etc. Although TSAD is a crucial problem in anomaly detection, few solutions in anomaly detection are suitable for it at present. Recently, some researchers use GAN-based methods such as TAnoGAN and TadGAN to solve TSAD problem. However, problems such as model collapse, low generalization capability and poor accuracy still exist. In this article, we proposed a Dilated Convolutional Transformer-based GAN (DCT-GAN) to enhance accuracy and improve generalization capability of the model. Specifically, DCT-GAN utilize several generators and a single discriminator to alleviate the mode collapse problem. Each generator consists of a dilated convolutional neural network and a Transformer block to obtain fine-grained and coarse-grained information of the time series, which is a useful component to improve generalization capability. We also use weight-based mechanism to balance these generators. Experiments verify the effectiveness of our method and each part of DCT-GAN.},
	number = {4},
	urldate = {2024-09-07},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Yifan and Peng, Xiaoyan and Zhang, Jia and Li, Zhiyong and Wen, Ming},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Anomaly detection, Convolutional neural networks, Data models, Generative adversarial networks, Generators, Time series analysis, Transformers, dilated convolutional neural network, generative adversarial network, time series analysis, transformer, weight-based mechanism},
	pages = {3632--3644},
}

@article{zhang_debiased_2024,
	title = {Debiased {Contrastive} {Learning} for {Time}-{Series} {Representation} {Learning} and {Fault} {Detection}},
	volume = {20},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/10443248/},
	doi = {10.1109/TII.2024.3359409},
	abstract = {Building reliable fault detection systems through deep neural networks is an appealing topic in industrial scenarios. In these contexts, the representations extracted by neural networks on available labeled timeseries data can reﬂect system states. However, this endeavor remains challenging due to the necessity of labeled data. Self-supervised contrastive learning (SSCL) is one of the effective approaches to deal with this challenge, but existing SSCL-based models suffer from sampling bias and representation bias problems. This article introduces a debiased contrastive learning framework for time-series data and applies it to industrial fault detection tasks. This framework ﬁrst develops the multigranularity augmented view generation method to generate augmented views at different granularities. It then introduces the momentum clustering contrastive learning strategy and the expert knowledge guidance mechanism to mitigate sampling bias and representation bias, respectively. Finally, the experiments on a public bearing fault detection dataset and a widely used valve stiction detection dataset show the effectiveness of the proposed feature learning framework.},
	language = {en},
	number = {5},
	urldate = {2024-07-09},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Zhang, Kexin and Cai, Rongyao and Zhou, Chunlin and Liu, Yong},
	month = may,
	year = {2024},
	pages = {7641--7653},
}

@article{wu_decompose_2023,
	title = {Decompose {Auto}-{Transformer} {Time} {Series} {Anomaly} {Detection} for {Network} {Management}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/354},
	doi = {10.3390/electronics12020354},
	abstract = {Time series anomaly detection through unsupervised methods has been an active research area in recent years due to its enormous potential for networks management. The representation and reconstruction of time series have made extraordinary progress in existing works. However, time series is known to be complex in terms of their temporal dependency and stochasticity, which makes anomaly detection difficult. To this end, we propose a novel approach based on a decomposition auto-transformer networks(DATN) for time series anomaly detection. The time series is decomposed into seasonal and trend components, and renovated as a basic inner block deep model. With this design, transformers can decompose complex time series in a progressive manner. We also design an auto-transfomer block that determines dependencies and representation aggregation at the sub-series level based on series seasonal and trend components. Moreover, the complex transformer decoder is replaced by a simple linear decoder, which makes the model more efficient. Extensive experiments on various public benchmarks demonstrate that our method has achieved state-of-the-art performance.},
	language = {en},
	number = {2},
	urldate = {2024-09-07},
	journal = {Electronics},
	author = {Wu, Bo and Fang, Chao and Yao, Zhenjie and Tu, Yanhui and Chen, Yixin},
	month = jan,
	year = {2023},
	pages = {354},
}

@inproceedings{mou_deep_2023,
	title = {Deep {Autoencoding} {One}-{Class} time {Series} {Anomaly} {Detection}},
	url = {https://ieeexplore.ieee.org/document/10095724/?arnumber=10095724},
	doi = {10.1109/ICASSP49357.2023.10095724},
	abstract = {Time-series Anomaly Detection(AD) is widely used in monitoring and security applications in various industries and has become a hot spot in the field of deep learning. Normality-representation-based methods perform well in certain scenarios but may ignore some aspects of the overall normality. Feature-extraction-based methods always take a process of pre-training, whose target differs from AD, leading to a decline in AD performance. In this paper, we propose a new AD method called deep Autoencoding One-Class (AOC), which learns features with AutoEncoder(AE). Meanwhile, the normal context vectors from AE are constrained into a hypersphere small enough, similar to one-class methods. With an objective function that optimizes the two assumptions simultaneously, AOC learns various aspects of normality, which is more effective for AD. Experiments on public datasets show that our method outperforms existing baseline approaches.},
	urldate = {2024-09-06},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Mou, Xudong and Wang, Rui and Wang, Tiejun and Sun, Jie and Li, Bo and Wo, Tianyu and Liu, Xudong},
	month = jun,
	year = {2023},
	note = {ISSN: 2379-190X},
	keywords = {Autoen-coder, Deep learning, Industries, Linear programming, Merging, One-class classification, Security, Signal processing, Time Series Anomaly Detection, Time series analysis},
	pages = {1--5},
}

@misc{wang_deep_2023,
	title = {Deep {Contrastive} {One}-{Class} {Time} {Series} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2207.01472},
	abstract = {The accumulation of time-series data and the absence of labels make time-series Anomaly Detection (AD) a selfsupervised deep learning task. Single-normality-assumptionbased methods, which reveal only a certain aspect of the whole normality, are incapable of tasks involved with a large number of anomalies. Speciﬁcally, Contrastive Learning (CL) methods distance negative pairs, many of which consist of both normal samples, thus reducing the AD performance. Existing multi-normality-assumption-based methods are usually two-staged, ﬁrstly pre-training through certain tasks whose target may diﬀer from AD, limiting their performance. To overcome the shortcomings, a deep Contrastive One-Class Anomaly detection method of time series (COCA) is proposed by authors, following the normality assumptions of CL and one-class classiﬁcation. It treats the original and reconstructed representations as the positive pair of negative-sample-free CL, namely “sequence contrast”. Next, invariance terms and variance terms compose a contrastive one-class loss function in which the loss of the assumptions is optimized by invariance terms simultaneously and the “hypersphere collapse” is prevented by variance terms. In addition, extensive experiments on two realworld time-series datasets show the superior performance of the proposed method achieves state-of-the-art.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Wang, Rui and Liu, Chongwei and Mou, Xudong and Gao, Kai and Guo, Xiaohui and Liu, Pin and Wo, Tianyu and Liu, Xudong},
	month = apr,
	year = {2023},
	note = {arXiv:2207.01472 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@article{sabokrou_deep-anomaly__2018,
	title = {Deep-anomaly\_ {Fully} convolutional neural network for fast anomaly detection in crowded scenes},
	abstract = {The detection of abnormal behaviour in crowded scenes has to deal with many challenges. This paper presents an eﬃcient method for detection and localization of anomalies in videos. Using fully convolutional neural networks (FCNs) and temporal data, a pre-trained supervised FCN is transferred into an unsupervised FCN ensuring the detection of (global) anomalies in scenes. High performance in terms of speed and accuracy is achieved by investigating the cascaded detection as a result of reducing computation complexities. This FCN-based architecture addresses two main tasks, feature representation and cascaded outlier detection. Experimental results on two benchmarks suggest that the proposed method outperforms existing methods in terms of accuracy regarding detection and localization.},
	language = {en},
	journal = {Computer Vision and Image Understanding},
	author = {Sabokrou, Mohammad},
	year = {2018},
}

@inproceedings{alshaer_detecting_2020,
	address = {Baltimore, MD, USA},
	title = {Detecting {Anomalies} from {Streaming} {Time} {Series} using {Matrix} {Profile} and {Shapelets} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72819-228-4},
	url = {https://ieeexplore.ieee.org/document/9288261/},
	doi = {10.1109/ICTAI50040.2020.00066},
	abstract = {Detecting anomalies in streaming time series data with no prior labels is considered a challenging issue, especially, when anomalies may vary with time. There is a need to deal with time series streams by identifying the anomalous patterns. These patterns can be described by representative features extracted from the data, which expresses abnormal behavior. This work addresses the challenge of performing online and continuous learning over time series data. In this paper, a solution based on the Matrix Proﬁle algorithm and representation learning approach is developed. In light of that, we will show how the integration of these widely used approaches in the streaming context is quite important for learning and detecting anomalies in realtime.},
	language = {en},
	urldate = {2024-07-30},
	booktitle = {2020 {IEEE} 32nd {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	publisher = {IEEE},
	author = {Alshaer, Mohammad and Garcia-Rodriguez, Sandra and Gouy-Pailler, Cedric},
	month = nov,
	year = {2020},
	pages = {376--383},
}

@misc{kravchik_detecting_2018,
	title = {Detecting {Cyberattacks} in {Industrial} {Control} {Systems} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1806.08110},
	abstract = {This paper presents a study on detecting cyberattacks on industrial control systems (ICS) using unsupervised deep neural networks, specifically, convolutional neural networks. The study was performed on a Secure Water Treatment testbed (SWaT) dataset, which represents a scaled-down version of a real-world industrial water treatment plant. e suggest a method for anomaly detection based on measuring the statistical deviation of the predicted value from the observed value. We applied the proposed method by using a variety of deep neural networks architectures including different variants of convolutional and recurrent networks. The test dataset from SWaT included 36 different cyberattacks. The proposed method successfully detects the vast majority of the attacks with a low false positive rate thus improving on previous works based on this data set. The results of the study show that 1D convolutional networks can be successfully applied to anomaly detection in industrial control systems and outperform more complex recurrent networks while being much smaller and faster to train.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Kravchik, Moshe and Shabtai, Asaf},
	month = dec,
	year = {2018},
	note = {arXiv:1806.08110 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{altin_exploring_2024,
	title = {Exploring the {Influence} of {Dimensionality} {Reduction} on {Anomaly} {Detection} {Performance} in {Multivariate} {Time} {Series}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10559232/},
	doi = {10.1109/ACCESS.2024.3415088},
	abstract = {This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models. The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models’ capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying highdimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\% and 650\% when dimensionality was halved and minimized to the lowest dimensions, respectively. This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency. The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the Anomaly-Transformer demonstrates versatility across various reduction techniques. These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and anomaly detection, contributing valuable perspectives to the field of time series analysis. The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in anomaly detection.},
	language = {en},
	urldate = {2024-08-25},
	journal = {IEEE Access},
	author = {Altin, Mahsun and Cakir, Altan},
	year = {2024},
	pages = {85783--85794},
}

@article{lee_impact_nodate,
	title = {Impact of {Recurrent} {Neural} {Networks} and {Deep} {Learning} {Frameworks} on {Real}-time {Lightweight} {Time} {Series} {Anomaly} {Detection}},
	abstract = {Real-time lightweight time series anomaly detection has become increasingly crucial in cybersecurity and many other domains. Its ability to adapt to unforeseen pattern changes and swiftly identify anomalies enables prompt responses and critical decision-making. While several such anomaly detection approaches have been introduced in recent years, they primarily utilize a single type of recurrent neural networks (RNNs) and have been implemented in only one deep learning framework. It is unclear how the use of different types of RNNs available in various deep learning frameworks affects the performance of these anomaly detection approaches due to the absence of comprehensive evaluations. Arbitrarily choosing a RNN variant and a deep learning framework to implement an anomaly detection approach may not reflect its true performance and could potentially mislead users into favoring one approach over another. In this paper, we aim to study the influence of various types of RNNs available in popular deep learning frameworks on real-time lightweight time series anomaly detection. We reviewed several state-of-the-art approaches and implemented a representative anomaly detection approach using well-known RNN variants supported by three widely recognized deep learning frameworks. A comprehensive evaluation is then conducted to analyze the performance of each implementation across real-world, open-source time series datasets. The evaluation results provide valuable guidance for selecting the appropriate RNN variant and deep learning framework for real-time, lightweight time series anomaly detection.},
	language = {en},
	author = {Lee’, Ming-Chang and Lin, Jia-Chun and Katsikas, Sokratis},
}

@inproceedings{li_ips_2022,
	title = {{IPS}: {Instance} {Profile} for {Shapelet} {Discovery} for {Time} {Series} {Classification}},
	shorttitle = {{IPS}},
	url = {https://ieeexplore.ieee.org/document/9835498/?arnumber=9835498},
	doi = {10.1109/ICDE53745.2022.00179},
	abstract = {Time series classification (TSC) has been one of the most fundamental problems of time series data. Time series shapelets (or simply, shapelets) are discriminative subsequences that have been recently found both effective and interpretable for solving TSC. However, shapelet discovery is known to be computationally costly. Meanwhile, matrix profile has been recently proposed for efficient motif discovery and anomaly detection. Our preliminary experiment shows that a direct adoption of the matrix profile on TSC does not bring superior classification accuracy. We have identified two main issues of such an adoption: 1) discords as “shapelets”, and 2) lack of shapelet diversity. In response to these issues, we propose instance profile for shapelets, called IPS, for shapelet discovery for TSC. The main challenge is to utilize the instance profile (IP) to capture the characteristics of shapelets in a robust manner and then to discover high-quality shapelets efficiently. First, we use our IP to generate abundant shapelet candidates. We next efficiently prune candidates that do not align with the definition of shapelets using a novel distribution-aware bloom filter (DABF). Three utility functions are proposed to measure the shapelet candidates and DABF is used to efficiently compute the functions. We have conducted comprehensive experiments on IPS with 12 competitive state-of-the-art methods using UCR Archive datasets. The efficiency is on average 25 times faster than that of BSPCOVER (the current state-of-the-art method). The accuracy of IPS is comparable to or higher than that of existing work. Furthermore, we select one case study to illustrate the interpretability of the shapelets.},
	urldate = {2024-09-07},
	booktitle = {2022 {IEEE} 38th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Li, Guozhong and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S and Mah, Daphne Ngar-yin and Wong, Grace L.H.},
	month = may,
	year = {2022},
	note = {ISSN: 2375-026X},
	keywords = {Accuracy, Conferences, Current measurement, Data engineering, Distributed databases, Distribution-aware bloom filter, Efficiency, IP networks, Instance profile, Shape measurement, Time series analysis, Time series classification},
	pages = {1781--1793},
}

@misc{su_large_2024,
	title = {Large {Language} {Models} for {Forecasting} and {Anomaly} {Detection}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Large {Language} {Models} for {Forecasting} and {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2402.10350},
	abstract = {This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Su, Jing and Jiang, Chufeng and Jin, Xin and Qiao, Yuxin and Xiao, Tingsong and Ma, Hongda and Wei, Rong and Jing, Zhi and Xu, Jiajun and Lin, Junhong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10350 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{pereira_learning_2019,
	address = {Kyoto, Japan},
	title = {Learning {Representations} from {Healthcare} {Time} {Series} {Data} for {Unsupervised} {Anomaly} {Detection}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-5386-7789-6},
	url = {https://ieeexplore.ieee.org/document/8679157/},
	doi = {10.1109/BIGCOMP.2019.8679157},
	abstract = {The amount of time series data generated in Healthcare is growing very fast and so is the need for methods that can analyse these data, detect anomalies and provide meaningful insights. However, most of the data available is unlabelled and, therefore, anomaly detection in this scenario has been a great challenge for researchers and practitioners.},
	language = {en},
	urldate = {2024-07-30},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	publisher = {IEEE},
	author = {Pereira, Joao and Silveira, Margarida},
	month = feb,
	year = {2019},
	pages = {1--7},
}

@inproceedings{han_learning_2022,
	address = {Washington DC USA},
	title = {Learning {Sparse} {Latent} {Graph} {Representations} for {Anomaly} {Detection} in {Multivariate} {Time} {Series}},
	isbn = {978-1-4503-9385-0},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539117},
	doi = {10.1145/3534678.3539117},
	abstract = {Anomaly detection in high-dimensional time series is typically tackled using either reconstruction- or forecasting-based algorithms due to their abilities to learn compressed data representations and model temporal dependencies, respectively. However, most existing methods disregard the relationships between features, information that would be extremely useful when incorporated into a model. In this work, we introduce Fused Sparse Autoencoder and Graph Net (FuSAGNet), which jointly optimizes reconstruction and forecasting while explicitly modeling the relationships within multivariate time series. Our approach combines Sparse Autoencoder and Graph Neural Network, the latter of which predicts future time series behavior from sparse latent representations learned by the former as well as graph structures learned through recurrent feature embedding. Experimenting on three real-world cyber-physical system datasets, we empirically demonstrate that the proposed method enhances the overall anomaly detection performance, outperforming baseline approaches. Moreover, we show that mining sparse latent patterns from high-dimensional time series improves the robustness of the graph-based forecasting model. Lastly, we conduct visual analyses to investigate the interpretability of both recurrent feature embeddings and sparse latent representations.},
	language = {en},
	urldate = {2024-09-06},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Han, Siho and Woo, Simon S.},
	month = aug,
	year = {2022},
	pages = {2977--2986},
}

@inproceedings{gangloff_leveraging_2022,
	address = {Montreal, QC, Canada},
	title = {Leveraging {Vector}-{Quantized} {Variational} {Autoencoder} {Inner} {Metrics} for {Anomaly} {Detection}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-062-7},
	url = {https://ieeexplore.ieee.org/document/9956102/},
	doi = {10.1109/ICPR56361.2022.9956102},
	abstract = {Anomaly Detection (AD) is an important research topic, with very diverse applications such as industrial defect detection, medical diagnosis, fraud detection, intrusion detection, etc. Within the last few years, deep learning-based methods have become the standard approach for AD. In many practical cases, the anomalies are unknown in advance. Therefore, most of challenging AD problems need to be addressed in an unsupervised or weakly supervised framework. In this context, deep generative models are widely used, in particular Variational Autoencoder (VAE) models. VAEs have been extended to Vector-Quantized VAEs (VQ-VAEs), a model increasingly popular because of its versatility enabled by the discrete latent space. We present for the first time a robust approach which takes advantage of the inner metrics of VQ-VAEs for AD. We show that the distance between the output of the encoder and the codebook vectors of a VQ-VAE provides a valuable information which can be used to localize the anomalies. In our approach, this metric complements a reconstruction-based metric to improve AD results. We compare our model with state-of-the-art AD models on three standards datasets, including the MVTec, UCSD-Ped1 and CIFAR-10 datasets. Experiments show that the proposed method yields high competitive results.},
	language = {en},
	urldate = {2024-07-15},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Gangloff, Hugo and Pham, Minh-Tan and Courtrai, Luc and Lefevre, Sebastien},
	month = aug,
	year = {2022},
	pages = {435--441},
}

@article{niu_lstm-based_2020,
	title = {{LSTM}-{Based} {VAE}-{GAN} for {Time}-{Series} {Anomaly} {Detection}},
	volume = {20},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/13/3738},
	doi = {10.3390/s20133738},
	abstract = {Time series anomaly detection is widely used to monitor the equipment sates through the data collected in the form of time series. At present, the deep learning method based on generative adversarial networks (GAN) has emerged for time series anomaly detection. However, this method needs to ﬁnd the best mapping from real-time space to the latent space at the anomaly detection stage, which brings new errors and takes a long time. In this paper, we propose a long short-term memory-based variational autoencoder generation adversarial networks (LSTM-based VAE-GAN) method for time series anomaly detection, which eﬀectively solves the above problems. Our method jointly trains the encoder, the generator and the discriminator to take advantage of the mapping ability of the encoder and the discrimination ability of the discriminator simultaneously. The long short-term memory (LSTM) networks are used as the encoder, the generator and the discriminator. At the anomaly detection stage, anomalies are detected based on reconstruction diﬀerence and discrimination results. Experimental results show that the proposed method can quickly and accurately detect anomalies.},
	language = {en},
	number = {13},
	urldate = {2024-08-24},
	journal = {Sensors},
	author = {Niu, Zijian and Yu, Ke and Wu, Xiaofei},
	month = jul,
	year = {2020},
	pages = {3738},
}

@misc{schwartz_maeday_2024,
	title = {{MAEDAY}: {MAE} for few and zero shot {AnomalY}-{Detection}},
	shorttitle = {{MAEDAY}},
	url = {http://arxiv.org/abs/2211.14307},
	abstract = {We propose using Masked Auto-Encoder (MAE), a transformer model self-supervisedly trained on image inpainting, for anomaly detection (AD). Assuming anomalous regions are harder to reconstruct compared with normal regions. MAEDAY is the first image-reconstruction-based anomaly detection method that utilizes a pre-trained model, enabling its use for Few-Shot Anomaly Detection (FSAD). We also show the same method works surprisingly well for the novel tasks of Zero-Shot AD (ZSAD) and Zero-Shot Foreign Object Detection (ZSFOD), where no normal samples are available.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Schwartz, Eli and Arbelle, Assaf and Karlinsky, Leonid and Harary, Sivan and Scheidegger, Florian and Doveh, Sivan and Giryes, Raja},
	month = feb,
	year = {2024},
	note = {arXiv:2211.14307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fung_model_2024,
	title = {Model {Selection} of {Zero}-shot {Anomaly} {Detectors} in the {Absence} of {Labeled} {Validation} {Data}},
	url = {http://arxiv.org/abs/2310.10461},
	abstract = {Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data—without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors with a generated synthetic validation set. Our proposed anomaly generation method assumes access to only a small support set of normal images and requires no training or finetuning. Once generated, our synthetic validation set is used to create detection tasks that compose a validation framework for model selection. In an empirical study, we find that SWSA often selects models that match selections made with a groundtruth validation set, resulting in higher AUROCs than baseline methods. We also find that SWSA selects prompts for CLIP-based anomaly detection that outperform baseline prompt selection strategies on all datasets, including the challenging MVTec-AD and VisA datasets.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Fung, Clement and Qiu, Chen and Li, Aodong and Rudolph, Maja},
	month = feb,
	year = {2024},
	note = {arXiv:2310.10461 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{goswami_moment_2024,
	title = {{MOMENT}: {A} {Family} of {Open} {Time}-series {Foundation} {Models}},
	shorttitle = {{MOMENT}},
	url = {http://arxiv.org/abs/2402.03885},
	abstract = {We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pretraining. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and taskspecific fine-tuning. Finally, we present several interesting empirical observations about large pretrained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on https://huggingface.co/AutonLab.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
	month = may,
	year = {2024},
	note = {arXiv:2402.03885 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{pham_mst-vae_2022,
	title = {{MST}-{VAE}: {Multi}-{Scale} {Temporal} {Variational} {Autoencoder} for {Anomaly} {Detection} in {Multivariate} {Time} {Series}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {{MST}-{VAE}},
	url = {https://www.mdpi.com/2076-3417/12/19/10078},
	doi = {10.3390/app121910078},
	abstract = {In IT monitoring systems, anomaly detection plays a vital role in detecting and alerting unexpected behaviors timely to system operators. With the growth of signal data in both volumes and dimensions during operation, unsupervised learning turns out to be a great solution to trigger anomalies thanks to the feasibility of working well with unlabeled data. In recent years, autoencoder, an unsupervised learning technique, has gained much attention because of its robustness. Autoencoder first compresses input data to lower-dimensional latent representation, which obtains normal patterns, then the compressed data are reconstructed back to the input form to detect abnormal data. In this paper, we propose a practical unsupervised learning approach using Multi-Scale Temporal convolutional kernels with Variational AutoEncoder (MST-VAE) for anomaly detection in multivariate time series data. Our key observation is that combining short-scale and long-scale convolutional kernels to extract various temporal information of the time series can enhance the model performance. Extensive empirical studies on five real-world datasets demonstrate that MST-VAE can outperform baseline methods in effectiveness and efficiency.},
	language = {en},
	number = {19},
	urldate = {2024-09-06},
	journal = {Applied Sciences},
	author = {Pham, Tuan-Anh and Lee, Jong-Hoon and Park, Choong-Shik},
	month = oct,
	year = {2022},
	pages = {10078},
}

@article{nivarthi_multi-task_2023,
	title = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}: {A} {Comparative} {Analysis} of {Unified} {Autoencoder} {Variants} and {Task}-{Embedding} {Dimensions}},
	volume = {5},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2504-4990},
	shorttitle = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}},
	url = {https://www.mdpi.com/2504-4990/5/3/62},
	doi = {10.3390/make5030062},
	abstract = {Typically, renewable-power-generation forecasting using machine learning involves creating separate models for each photovoltaic or wind park, known as single-task learning models. However, transfer learning has gained popularity in recent years, as it allows for the transfer of knowledge from source parks to target parks. Nevertheless, determining the most similar source park(s) for transfer learning can be challenging, particularly when the target park has limited or no historical data samples. To address this issue, we propose a multi-task learning architecture that employs a Uniﬁed Autoencoder (UAE) to initially learn a common representation of input weather features among tasks and then utilizes a Task-Embedding layer in a Neural Network (TENN) to learn task-speciﬁc information. This proposed UAE-TENN architecture can be easily extended to new parks with or without historical data. We evaluate the performance of our proposed architecture and compare it to single-task learning models on six photovoltaic and wind farm datasets consisting of a total of 529 parks. Our results show that the UAE-TENN architecture signiﬁcantly improves power-forecasting performance by 10 to 19\% for photovoltaic parks and 5 to 15\% for wind parks compared to baseline models. We also demonstrate that UAE-TENN improves forecast accuracy for a new park by 19\% for photovoltaic parks, even in a zero-shot learning scenario where there is no historical data. Additionally, we propose variants of the Uniﬁed Autoencoder with convolutional and LSTM layers, compare their performance, and provide a comparison among architectures with different numbers of task-embedding dimensions. Finally, we demonstrate the utility of trained task embeddings for interpretation and visualization purposes.},
	language = {en},
	number = {3},
	urldate = {2024-06-27},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = sep,
	year = {2023},
	pages = {1214--1233},
}

@article{wang_multiscale_2023,
	title = {Multiscale {Wavelet} {Graph} {AutoEncoder} for {Multivariate} {Time}-{Series} {Anomaly} {Detection}},
	volume = {72},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/document/9954430/?arnumber=9954430},
	doi = {10.1109/TIM.2022.3223142},
	abstract = {In industrial facilities or IT systems, there are lots of multivariate time series generated from various metrics. Anomaly detection in multivariate time series is of great importance in applications such as fault diagnosis and root cause discovery. Recently, some unsupervised methods have made great progress in this task, especially the reconstruction architecture of autoencoders (AEs), learning normal distribution, and producing a significant error for anomalies. Although AEs can reconstruct subtle abnormal patterns well with the powerful generalization ability, it also leads to a high false negative. Moreover, these AE-based models ignore the dependence among variables at different time scales. In this article, we propose a novel anomaly detection framework named Multiscale wavElet Graph AE (MEGA). The main idea is to integrate discrete wavelet transform (DWT) into AE to decompose multivariate time series into multifrequency components and then reconstruct them, thereby highlighting various anomalies in specific frequency bands. Meanwhile, for anomalies caused by the changes of intervariable dependence, we introduce a dynamic graph module to capture such dependence on the decomposed multiscale frequency components. Experiments have been carried out on three public multivariate time-series anomaly detection datasets. The results demonstrate that the MEGA outperforms the state-of-the-art baselines.},
	urldate = {2024-09-06},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Wang, Jing and Shao, Shikuan and Bai, Yunfei and Deng, Jiaoxue and Lin, Youfang},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {Anomaly detection, Data models, Discrete wavelet transforms, Neural networks, Task analysis, Time series analysis, Time-frequency analysis, discrete wavelet transform (DWT), graph convolution, multiscale, multivariate time series},
	pages = {1--11},
}

@article{ye_multivariate_2023,
	title = {Multivariate {Time} {Series} {Anomaly} {Detection} with {Fourier} {Time} {Series} {Transformer}},
	abstract = {Anomaly detection in time series data plays a key role in automatic industrial operations. Due to the intricate temporal dependencies within time series data and the difficulty in obtaining labeled data, recent anomaly detection methods have primarily focused on the temporal domain features of time series data, neglecting the frequency domain features. However, spectral analysis can better utilize periodic information within time series data such as seasonal patterns, which helps capturing multiscale and multiple frequency features. In this paper, we present a Fourier Time Series Transformer model (FTST for short), which combines the features of both time and frequency domains for time series anomaly detection. Specifically, the attention mechanism is utilized for modeling the temporal domain, while the Fourier Transform is employed to transform time series data into frequency domain data. The frequency domain features are then modeled using a Temporal Convolutional Network. By making full use of the temporal and frequency domains of time series data, FTST can significantly enhance the performance of time series anomaly detection. Experimental results on popular benchmark datasets demonstrate the anomaly detection performance of the proposed method.},
	language = {en},
	author = {Ye, Yufeng and He, Qichao and Zhang, Peng and Xiao, Jie and Li, Zhao},
	year = {2023},
}

@article{qin_multiview_2023,
	title = {Multiview {Graph} {Contrastive} {Learning} for {Multivariate} {Time}-{Series} {Anomaly} {Detection} in {IoT}},
	volume = {10},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/10214266/},
	doi = {10.1109/JIOT.2023.3303946},
	abstract = {Internet of Things (IoT) systems typically generate large amounts of sensory signals that get involved to represent the states of the systems. Most existing methods focus on learning the temporal patterns of the signals to detect anomalies. However, the performance is limited due to two critical problems. First, the relationships between different signals are rarely considered, resulting in missing important information for representation. Second, the high sensitivity of time series makes it difﬁcult to use conventional methods for data augmentation, which limits the improvement of representation and generalization capabilities. In this work, we propose a novel reconstruction-based framework with contrastive learning from multiple views to address these two issues. Speciﬁcally, intrasignal and intersignal graph structures are formed and learned in parallel to model the temporal context and capture the dependency relationships between signals, respectively. Multiview graph contrastive learning strategy is designed to improve the graph representation. We also provide an adaptive data augmentation method to generate graph views for contrastive learning, which helps to accurately capture valuable intrinsic patterns from two different perspectives. Finally, the contrastive learning task and the reconstruction task are jointly trained. Extensive experiments on four real-world data sets demonstrate that our method outperforms the existing state-of-the-art baselines.},
	language = {en},
	number = {24},
	urldate = {2024-08-27},
	journal = {IEEE Internet of Things Journal},
	author = {Qin, Shuxin and Chen, Lin and Luo, Yongcan and Tao, Gaofeng},
	month = dec,
	year = {2023},
	pages = {22401--22414},
}

@article{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	language = {en},
	author = {Nielsen, Michael},
	year = {2015},
	pages = {224},
}

@article{gruhl_novelty_2022,
	title = {Novelty {Detection} for {Multivariate} {Data} {Streams} with {Probalistic} {Models}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, open access},
	url = {https://kobra.uni-kassel.de/handle/123456789/13902},
	doi = {10.17170/KOBRA-202205106160},
	language = {en},
	urldate = {2024-07-15},
	author = {Gruhl, Christian M.},
	collaborator = {{Universität Kassel}},
	year = {2022},
	note = {Publisher: Universität Kassel},
	keywords = {620, Anomalieerkennung, Autonomer Agent, Daten, Datenstrom, Maschinelles Lernen, Sensor, Wahrscheinlichkeitsmaß, anomalies, anomaly detection, concept drift, data streams, machine learning, novelties, novelty detection, outliers, probabilistic modeling},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{abdulaal_practical_2021,
	title = {Practical {Approach} to {Asynchronous} {Multivariate} {Time} {Series} {Anomaly} {Detection} and {Localization}},
	abstract = {Engineers at eBay utilize robust methods in monitoring IT system signals for anomalies. However, the growing scale of signals, both in volumes and dimensions, overpowers traditional statistical state-space or supervised learning tools. Thus, state-of-the-art methods based on unsupervised deep learning are sought in recent research. However, we experienced flaws when implementing those methods, such as requiring partial supervision and weaknesses to high dimensional datasets, among other reasons discussed in this paper. We propose a practical approach for inferring anomalies from large multivariate sets. We observe an abundance of time series in real-world applications, which exhibit asynchronous and consistent repetitive variations, such as IT, weather, utility, and transportation. Our solution is designed to leverage this behavior. The solution utilizes spectral analysis on the latent representation of a pre-trained autoencoder to extract dominant frequencies across the signals, which are then used in a subsequent network that learns the phase shifts across the signals and produces a synchronized representation of the raw multivariate. Random subsets of the synchronous multivariate are then fed into an array of autoencoders learning to minimize the quantile reconstruction losses, which are then used to infer and localize anomalies based on a majority vote. We benchmark this method against state-of-the-art approaches on public datasets and eBay’s data using their referenced evaluation methods. Furthermore, we address the limitations of the referenced evaluation methods and propose a more realistic evaluation method.},
	language = {en},
	author = {Abdulaal, Ahmed},
	year = {2021},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292},
	shorttitle = {Representation {Learning}},
	url = {http://ieeexplore.ieee.org/document/6472238/},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	language = {en},
	number = {8},
	urldate = {2024-07-03},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Y. and Courville, A. and Vincent, P.},
	month = aug,
	year = {2013},
	pages = {1798--1828},
}

@book{lavrac_representation_2021,
	address = {Cham},
	title = {Representation {Learning}: {Propositionalization} and {Embeddings}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-030-68816-5 978-3-030-68817-2},
	shorttitle = {Representation {Learning}},
	url = {https://link.springer.com/10.1007/978-3-030-68817-2},
	language = {en},
	urldate = {2024-07-09},
	publisher = {Springer International Publishing},
	author = {Lavrač, Nada and Podpečan, Vid and Robnik-Šikonja, Marko},
	year = {2021},
	doi = {10.1007/978-3-030-68817-2},
}

@inproceedings{su_robust_2019,
	address = {Anchorage AK USA},
	title = {Robust {Anomaly} {Detection} for {Multivariate} {Time} {Series} through {Stochastic} {Recurrent} {Neural} {Network}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330672},
	doi = {10.1145/3292500.3330672},
	abstract = {Industry devices (i.e., entities) such as server machines, spacecrafts, engines, etc., are typically monitored with multivariate time series, whose anomaly detection is critical for an entity’s service quality management. However, due to the complex temporal dependence and stochasticity of multivariate time series, their anomaly detection remains a big challenge. This paper proposes OmniAnomaly, a stochastic recurrent neural network for multivariate time series anomaly detection that works well robustly for various devices. Its core idea is to capture the normal patterns of multivariate time series by learning their robust representations with key techniques such as stochastic variable connection and planar normalizing flow, reconstruct input data by the representations, and use the reconstruction probabilities to determine anomalies. Moreover, for a detected entity anomaly, OmniAnomaly can provide interpretations based on the reconstruction probabilities of its constituent univariate time series. The evaluation experiments are conducted on two public datasets from aerospace and a new server machine dataset (collected and released by us) from an Internet company. OmniAnomaly achieves an overall F1-Score of 0.86 in three real-world datasets, significantly outperforming the best performing baseline method by 0.09. The interpretation accuracy for OmniAnomaly is up to 0.89.},
	language = {en},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Su, Ya and Zhao, Youjian and Niu, Chenhao and Liu, Rong and Sun, Wei and Pei, Dan},
	month = jul,
	year = {2019},
	pages = {2828--2837},
}

@article{kitchenham_systematic_2009,
	title = {Systematic literature reviews in software engineering – {A} systematic literature review},
	volume = {51},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584908001390},
	doi = {10.1016/j.infsof.2008.09.009},
	abstract = {Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.
Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.
Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.},
	language = {en},
	number = {1},
	urldate = {2024-08-19},
	journal = {Information and Software Technology},
	author = {Kitchenham, Barbara and Pearl Brereton, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	month = jan,
	year = {2009},
	pages = {7--15},
}

@article{peng_tcf-trans_2023,
	title = {{TCF}-{Trans}: {Temporal} {Context} {Fusion} {Transformer} for {Anomaly} {Detection} in {Time} {Series}},
	volume = {23},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	shorttitle = {{TCF}-{Trans}},
	url = {https://www.mdpi.com/1424-8220/23/20/8508},
	doi = {10.3390/s23208508},
	abstract = {Anomaly detection tasks involving time-series signal processing have been important research topics for decades. In many real-world anomaly detection applications, no speciﬁc distributions ﬁt the data, and the characteristics of anomalies are different. Under these circumstances, the detection algorithm requires excellent learning ability of the data features. Transformers, which apply the self-attention mechanism, have shown outstanding performances in modelling long-range dependencies. Although Transformer based models have good prediction performance, they may be inﬂuenced by noise and ignore some unusual details, which are signiﬁcant for anomaly detection. In this paper, a novel temporal context fusion framework: Temporal Context Fusion Transformer (TCF-Trans), is proposed for anomaly detection tasks with applications to time series. The original feature transmitting structure in the decoder of Informer is replaced with the proposed feature fusion decoder to fully utilise the features extracted from shallow and deep decoder layers. This strategy prevents the decoder from missing unusual anomaly details while maintaining robustness from noises inside the data. Besides, we propose the temporal context fusion module to adaptively fuse the generated auxiliary predictions. Extensive experiments on public and collected transportation datasets validate that the proposed framework is effective for anomaly detection in time series. Additionally, the ablation study and a series of parameter sensitivity experiments show that the proposed method maintains high performance under various experimental settings.},
	language = {en},
	number = {20},
	urldate = {2024-09-07},
	journal = {Sensors},
	author = {Peng, Xinggan and Li, Hanhui and Lin, Yuxuan and Chen, Yongming and Fan, Peng and Lin, Zhiping},
	month = oct,
	year = {2023},
	pages = {8508},
}

@article{thill_temporal_2021,
	title = {Temporal convolutional autoencoder for unsupervised anomaly detection in time series},
	volume = {112},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494621006724},
	doi = {10.1016/j.asoc.2021.107751},
	abstract = {Learning temporal patterns in time series remains a challenging task up until today. Particularly for anomaly detection in time series, it is essential to learn the underlying structure of a system’s normal behavior. Periodic or quasiperiodic signals with complex temporal patterns make the problem even more challenging: Anomalies may be a hard-to-detect deviation from the normal recurring pattern. In this paper, we present TCN-AE, a temporal convolutional network autoencoder based on dilated convolutions. Contrary to many other anomaly detection algorithms, TCN-AE is trained in an unsupervised manner. The algorithm demonstrates its efficacy on a comprehensive real-world anomaly benchmark comprising electrocardiogram (ECG) recordings of patients with cardiac arrhythmia. TCNAE significantly outperforms several other unsupervised state-of-the-art anomaly detection algorithms. Moreover, we investigate the contribution of the individual enhancements and show that each new ingredient improves the overall performance on the investigated benchmark.},
	language = {en},
	urldate = {2024-09-06},
	journal = {Applied Soft Computing},
	author = {Thill, Markus and Konen, Wolfgang and Wang, Hao and Bäck, Thomas},
	month = nov,
	year = {2021},
	pages = {107751},
}

@article{he_temporal_2019,
	title = {Temporal {Convolutional} {Networks} for {Anomaly} {Detection} in {Time} {Series}},
	abstract = {Convolutional Networks have been demonstrated to be particularly useful for extracting high level feature in structural data. Temporal convolutional network (TCN) is a framework which employs casual convolutions and dilations so that it is adaptive for sequential data with its temporality and large receptive fields. In this paper, we apply TCN for anomaly detection in time series. We train the TCN on normal sequences and use it to predict trend in a number of time steps. Prediction errors are fitted by a multivariate Gaussian distribution and used to calculate the anomaly scores of points. In addition, a multi-scale feature mixture method is raised to promote performance. The validity of this method is confirmed on three real-world datasets.},
	language = {en},
	journal = {Journal of Physics},
	author = {He, Yangdong and Zhao, Jiabao},
	year = {2019},
}

@misc{shi_trade-off_2023,
	title = {The {Trade}-off between {Universality} and {Label} {Efficiency} of {Representations} from {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2303.00106},
	abstract = {Pre-training representations (a.k.a. foundation models) has recently become a prevalent learning paradigm, where one ﬁrst pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efﬁciency (the ability to learn an accurate classiﬁer on top of the representation with a small amount of labeled data) and universality (usefulness across a wide range of downstream tasks). In this paper, we focus on one of the most popular instantiations of this paradigm: contrastive learning with linear probing, i.e., learning a linear predictor on the representation pre-trained by contrastive learning. We show that there exists a trade-off between the two desiderata so that one may not be able to achieve both simultaneously. Speciﬁcally, we provide analysis using a theoretical data model and show that, while more diverse pre-training data result in more diverse features for different tasks (improving universality), it puts less emphasis on task-speciﬁc features, giving rise to larger sample complexity for down-stream supervised tasks, and thus worse prediction performance. Guided by this analysis, we propose a contrastive regularization method to improve the trade-off. We validate our analysis and method empirically with systematic experiments using real-world datasets and foundation models.},
	language = {en},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Shi, Zhenmei and Chen, Jiefeng and Li, Kunyang and Raghuram, Jayaram and Wu, Xi and Liang, Yingyu and Jha, Somesh},
	month = feb,
	year = {2023},
	note = {arXiv:2303.00106 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{dau_ucr_2019,
	title = {The {UCR} {Time} {Series} {Archive}},
	url = {http://arxiv.org/abs/1810.07758},
	abstract = {The UCR Time Series Archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1-nearest neighbor classiﬁcation), a large fraction may be mis-attributing the reasons for their improvement. Moreover, they may have been able to achieve the same improvement with a much simpler modiﬁcation, requiring just a single line of code.},
	language = {en},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
	month = sep,
	year = {2019},
	note = {arXiv:1810.07758 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kang_tictok_2023,
	title = {{TiCTok}: {Time}-{Series} {Anomaly} {Detection} {With} {Contrastive} {Tokenization}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {{TiCTok}},
	url = {https://ieeexplore.ieee.org/document/10201844/},
	doi = {10.1109/ACCESS.2023.3301140},
	abstract = {Detecting anomalies in multivariate time-series data is an important task in various real world applications. Recent advances using deep learning have shown promising results in this area. Nowadays, Transformer-based models have shown outstanding performance and contrastive learning has emerged as a powerful technique for representation learning, however, it may not be directly applicable to the time-series domain. Here, we propose a time-series anomaly detection model with contrastive tokenization (TiCTok). We propose a time-series token encoder to transform raw time-series data into latent embeddings containing high-level wide-range temporal information. We exploit both token encoder and contrastive learning to produce high quality latent embeddings. In addition, we propose a novel anomaly scoring method simply utilizing the contrastive loss used in the training phase. According to our experimental results, the proposed model achieved better or comparable performance compared to the previous state-of-the-art on five widely used benchmark datasets in terms of F1-score.},
	language = {en},
	urldate = {2024-08-27},
	journal = {IEEE Access},
	author = {Kang, Minseo and Lee, Byunghan},
	year = {2023},
	pages = {81011--81020},
}

@article{beggel_time_2019,
	title = {Time series anomaly detection based on shapelet learning},
	volume = {34},
	issn = {0943-4062, 1613-9658},
	url = {http://link.springer.com/10.1007/s00180-018-0824-9},
	doi = {10.1007/s00180-018-0824-9},
	language = {en},
	number = {3},
	urldate = {2024-07-30},
	journal = {Computational Statistics},
	author = {Beggel, Laura and Kausler, Bernhard X. and Schiegg, Martin and Pfeiffer, Michael and Bischl, Bernd},
	month = sep,
	year = {2019},
	pages = {945--976},
}

@inproceedings{lee_time_2023,
	address = {Bali, Indonesia},
	title = {Time {Series} {Anomaly} {Detection} {Using} {Contrastive} {Learning} based {One}-{Class} {Classification}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66545-645-6},
	url = {https://ieeexplore.ieee.org/document/10067089/},
	doi = {10.1109/ICAIIC57133.2023.10067089},
	abstract = {Time series anomaly detection in industrial processes has recently attracted attention. However, since there are no labels in the manufacturing process data collected in real time, there are limitations in using supervised learning-based classification models. Therefore, the proposed method newly defines an objective function that simultaneously learns the OCC model and contrastive learning. In addition, by applying data augmentation techniques suitable for periodic time series data in contrastive learning, feature extraction that preserves temporal characteristics is possible. The effectiveness of representation extraction was verified by showing high anomaly detection performance even in datasets with similar normal and anomaly data forms.},
	language = {en},
	urldate = {2024-08-01},
	booktitle = {2023 {International} {Conference} on {Artificial} {Intelligence} in {Information} and {Communication} ({ICAIIC})},
	publisher = {IEEE},
	author = {Lee, Yeseul and Byun, Yunseon and Baek, Jun-Geol},
	month = feb,
	year = {2023},
	pages = {330--335},
}

@article{shin_time_2023,
	title = {Time {Series} {Anomaly} {Detection} {Using} {Transformer}-{Based} {GAN} {With} {Two}-{Step} {Masking}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10164104/},
	doi = {10.1109/ACCESS.2023.3289921},
	abstract = {Time series anomaly detection is a task that determines whether an unseen signal is normal or abnormal, and it is a crucial function in various real-world applications. Typical approach is to learn normal data representation using generative models, like Generative Adversarial Network (GAN), to discriminate between normal and abnormal signals. Recently, a few studies actively adopt Transformer to model time series data, but there is no pure Transformer-based GAN framework for time series anomaly detection. As a pioneer work, we propose a new pure Transformer-based GAN framework, called AnoFormer, and its effective training strategy for better representation learning. Specifically, we improve the detection ability of our model by introducing two-step masking strategies. The first step is Random masking: we design a random mask pool to hide parts of the signal randomly. This allows our model to learn the representation of normal data. The second step is Exclusive and Entropy-based Re-masking: we propose a novel refinement step to provide feedback to accurately model the exclusive and uncertain parts in the first step. We empirically demonstrate the effectiveness of re-masking step that generates more normal-like signals robustly. Extensive experiments on various datasets show that AnoFormer significantly outperforms the state-of-the-art methods in time series anomaly detection.},
	language = {en},
	urldate = {2024-09-08},
	journal = {IEEE Access},
	author = {Shin, Ah-Hyung and Kim, Seong Tae and Park, Gyeong-Moon},
	year = {2023},
	pages = {74035--74047},
}

@misc{chen_time-series_2023,
	title = {Time-series {Anomaly} {Detection} via {Contextual} {Discriminative} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2304.07898},
	abstract = {Detecting anomalies in temporal data is challenging due to anomalies being dependent on temporal dynamics. One-class classiﬁcation methods are commonly used for anomaly detection tasks, but they have limitations when applied to temporal data. In particular, mapping all normal instances into a single hypersphere to capture their global characteristics can lead to poor performance in detecting context-based anomalies where the abnormality is deﬁned with respect to local information. To address this limitation, we propose a novel approach inspired by the loss function of DeepSVDD. Instead of mapping all normal instances into a single hypersphere center, each normal instance is pulled toward a recent context window. However, this approach is prone to a representation collapse issue where the neural network that encodes a given instance and its context is optimized towards a constant encoder solution. To overcome this problem, we combine our approach with a deterministic contrastive loss from Neutral AD, a promising self-supervised learning anomaly detection approach. We provide a theoretical analysis to demonstrate that the incorporation of the deterministic contrastive loss can eﬀectively prevent the occurrence of a constant encoder solution. Experimental results show superior performance of our model over various baselines and model variants on real-world industrial datasets.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Chen, Katrina and Feng, Mingbin and Wirjanto, Tony S.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07898 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{jeong_time-series_2022,
	title = {Time-{Series} {Anomaly} {Detection} with {Implicit} {Neural} {Representation}},
	url = {http://arxiv.org/abs/2201.11950},
	abstract = {Detecting anomalies in multivariate time-series data is essential in many real-world applications. Recently, various deep learning-based approaches have shown considerable improvements in timeseries anomaly detection. However, existing methods still have several limitations, such as long training time due to their complex model designs or costly tuning procedures to ﬁnd optimal hyperparameters (e.g., sliding window length) for a given dataset. In our paper, we propose a novel method called Implicit Neural Representationbased Anomaly Detection (INRAD). Speciﬁcally, we train a simple multi-layer perceptron that takes time as input and outputs corresponding values at that time. Then we utilize the representation error as an anomaly score for detecting anomalies. Experiments on ﬁve real-world datasets demonstrate that our proposed method outperforms other state-of-the-art methods in performance, training speed, and robustness.},
	language = {en},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Jeong, Kyeong-Joong and Shin, Yong-Min},
	month = jan,
	year = {2022},
	note = {arXiv:2201.11950 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{jiao_timeautoad_2022,
	title = {{TimeAutoAD}: {Autonomous} {Anomaly} {Detection} {With} {Self}-{Supervised} {Contrastive} {Loss} for {Multivariate} {Time} {Series}},
	volume = {9},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4697, 2334-329X},
	shorttitle = {{TimeAutoAD}},
	url = {https://ieeexplore.ieee.org/document/9705079/},
	doi = {10.1109/TNSE.2022.3148276},
	abstract = {Multivariate time series (MTS) data are becoming increasingly ubiquitous in networked systems, e.g., IoT systems and 5G networks. Anomaly detection in MTS refers to identifying time series which exhibit different behaviors from normal status. Building such a system, however, is challenging due to a few reasons: i) labels for anomaly cases are usually unavailable or very rare; ii) most existing approaches rely on manual model-design and hyperparameter tuning, which may cost a huge amount of labor effort. To this end, we propose an autonomous anomaly detection technique for multivariate time series data (TimeAutoAD) based on a novel self-supervised contrastive loss. Speciﬁcally, we ﬁrst present an automatic anomaly detection pipeline to optimize the model conﬁguration and hyperparameters automatically. Next, we introduce three different strategies to augment the training data for generating pseudo negative time series and employ a self-supervised contrastive loss to distinguish the original time series and the generated time series. In this way, the representation learning capability of TimeAutoAD can be greatly enhanced and the anomaly detection performance can thus be improved. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoAD not only outperforms state-of-the-art anomaly detection approaches but also exhibits robustness when training data are contaminated.},
	language = {en},
	number = {3},
	urldate = {2024-08-01},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Jiao, Yang and Yang, Kai and Song, Dongjing and Tao, Dacheng},
	month = may,
	year = {2022},
	pages = {1604--1619},
}

@misc{doshi_tisat_2022,
	title = {{TiSAT}: {Time} {Series} {Anomaly} {Transformer}},
	shorttitle = {{TiSAT}},
	url = {http://arxiv.org/abs/2203.05167},
	abstract = {While anomaly detection in time series has been an active area of research for several years, most recent approaches employ an inadequate evaluation criterion leading to an inﬂated F1 score. We show that a rudimentary Random Guess method can outperform state-of-the-art detectors in terms of this popular but faulty evaluation criterion. In this work, we propose a proper evaluation metric that measures the timeliness and precision of detecting sequential anomalies. Moreover, most existing approaches are unable to capture temporal features from long sequences. Self-attention based approaches, such as transformers, have been demonstrated to be particularly efﬁcient in capturing long-range dependencies while being computationally efﬁcient during training and inference. We also propose an efﬁcient transformer approach for anomaly detection in time series and extensively evaluate our proposed approach on several popular benchmark datasets.},
	language = {en},
	urldate = {2024-09-07},
	publisher = {arXiv},
	author = {Doshi, Keval and Abudalou, Shatha and Yilmaz, Yasin},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05167 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@inproceedings{nivarthi_towards_2023,
	address = {Jacksonville, FL, USA},
	title = {Towards {Few}-{Shot} {Time} {Series} {Anomaly} {Detection} with {Temporal} {Attention} and {Dynamic} {Thresholding}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350345346},
	url = {https://ieeexplore.ieee.org/document/10459893/},
	doi = {10.1109/ICMLA58977.2023.00218},
	abstract = {Anomaly detection plays a pivotal role in diverse realworld applications such as cybersecurity, fault detection, network monitoring, predictive maintenance, and highly automated driving. However, obtaining labeled anomalous data can be a formidable challenge, especially when anomalies exhibit temporal evolution. This paper introduces LATAM (Long short-term memory Autoencoder with Temporal Attention Mechanism) for few-shot anomaly detection, with the aim of enhancing detection performance in scenarios with limited labeled anomaly data. LATAM effectively captures temporal dependencies and emphasizes signiﬁcant patterns in multivariate time series data. In our investigation, we comprehensively evaluate LATAM against other anomaly detection models, particularly assessing its capability in few-shot learning scenarios where we have minimal examples from the normal class and none from the anomalous class in the training data. Our experimental results, derived from real-world photovoltaic inverter data, highlight LATAM’s superiority, showcasing a substantial 27\% mean F1 score improvement, even when trained on a mere two-week dataset. Furthermore, LATAM demonstrates remarkable results on the open-source SWaT dataset, achieving a 12\% boost in accuracy with only two days of training data. Moreover, we introduce a simple yet effective dynamic thresholding mechanism, further enhancing the anomaly detection capabilities of LATAM. This underscores LATAM’s efﬁcacy in addressing the challenges posed by limited labeled anomalies in practical scenarios and it proves valuable for downstream tasks involving temporal representation and time series prediction, extending its utility beyond anomaly detection applications.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2023 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Sick, Bernhard},
	month = dec,
	year = {2023},
	pages = {1444--1450},
}

@misc{tuli_tranad_2022,
	title = {{TranAD}: {Deep} {Transformer} {Networks} for {Anomaly} {Detection} in {Multivariate} {Time} {Series} {Data}},
	shorttitle = {{TranAD}},
	url = {http://arxiv.org/abs/2201.07284},
	abstract = {Efficient anomaly detection and diagnosis in multivariate timeseries data is of great importance for modern industrial applications. However, building a system that is able to quickly and accurately pinpoint anomalous observations is a challenging problem. This is due to the lack of anomaly labels, high data volatility and the demands of ultra-low inference times in modern applications. Despite the recent developments of deep learning approaches for anomaly detection, only a few of them can address all of these challenges. In this paper, we propose TranAD, a deep transformer network based anomaly detection and diagnosis model which uses attentionbased sequence encoders to swiftly perform inference with the knowledge of the broader temporal trends in the data. TranAD uses focus score-based self-conditioning to enable robust multi-modal feature extraction and adversarial training to gain stability. Additionally, model-agnostic meta learning (MAML) allows us to train the model using limited data. Extensive empirical studies on six publicly available datasets demonstrate that TranAD can outperform state-of-the-art baseline methods in detection and diagnosis performance with data and time-efficient training. Specifically, TranAD increases F1 scores by up to 17\%, reducing training times by up to 99\% compared to the baselines.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Tuli, Shreshth and Casale, Giuliano and Jennings, Nicholas R.},
	month = may,
	year = {2022},
	note = {arXiv:2201.07284 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{nivarthi_transfer_2022,
	title = {Transfer {Learning} as an {Essential} {Tool} for {Digital} {Twins} in {Renewable} {Energy} {Systems}},
	url = {http://arxiv.org/abs/2203.05026},
	abstract = {Transfer learning (TL), the next frontier in machine learning (ML), has gained much popularity in recent years, due to the various challenges faced in ML, like the requirement of vast amounts of training data, expensive and time-consuming labelling processes for data samples, and long training duration for models. TL is useful in tackling these problems, as it focuses on transferring knowledge from previously solved tasks to new tasks. Digital twins and other intelligent systems need to utilise TL to use the previously gained knowledge and solve new tasks in a more self-reliant way, and to incrementally increase their knowledge base. Therefore, in this article, the critical challenges in power forecasting and anomaly detection in the context of renewable energy systems are identiﬁed, and a potential TL framework to meet these challenges is proposed. This article also proposes a feature embedding approach to handle the missing sensors data. The proposed TL methods help to make a system more autonomous in the context of organic computing.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Nivarthi, Chandana Priya},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05026 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{yue_ts2vec_2022,
	title = {{TS2Vec}: {Towards} {Universal} {Representation} of {Time} {Series}},
	shorttitle = {{TS2Vec}},
	url = {http://arxiv.org/abs/2106.10466},
	abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classiﬁcation tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves signiﬁcant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
	language = {en},
	urldate = {2024-08-03},
	publisher = {arXiv},
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	month = feb,
	year = {2022},
	note = {arXiv:2106.10466 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{gao_tsmae_2023,
	title = {{TSMAE}: {A} {Novel} {Anomaly} {Detection} {Approach} for {Internet} of {Things} {Time} {Series} {Data} {Using} {Memory}-{Augmented} {Autoencoder}},
	volume = {10},
	issn = {2327-4697},
	shorttitle = {{TSMAE}},
	url = {https://ieeexplore.ieee.org/document/9744555/?arnumber=9744555},
	doi = {10.1109/TNSE.2022.3163144},
	abstract = {With the development of communication, the Internet of Things (IoT) has been widely deployed and used in industrial manufacturing, intelligent transportation, and healthcare systems. The time-series feature of the IoT increases the data density and the data dimension, where anomaly detection is important to ensure hardware and software security. However, for the general anomaly detection methods, the anomaly may be well-reconstructed with tiny differences that are hard to discover. Measuring model complexity and the dataset feature space is a long and inefficient process. In this paper, we propose a memory-augmented autoencoder approach for detecting anomalies in IoT data, which is unsupervised, end-to-end, and not easily overgeneralized. First, a memory mechanism is introduced to suppress the generalization ability of the model, and a memory-augmented time-series autoencoder (TSMAE) is designed. Each memory item is encoded and recombined according to the similarity with the latent representation. Then, the new representation is decoded to generate the reconstructed sample, based on which the anomaly score can be obtained. Second, the addressing vector tends to be sparse by adding penalties and rectification functions to the loss. Memory modules are encouraged to extract typical normal patterns, thus inhibiting model generalization ability. Long short-term memory (LSTM) is introduced for decoding and encoding time-series data to obtain the contextual characteristics of time-series data. Finally, through experiments on the ECG and Wafer datasets, the validity of the TSMAE is verified. The rationality of the hyperparameter setting is discussed by visualizing the memory module addressing vector.},
	number = {5},
	urldate = {2024-09-06},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Gao, Honghao and Qiu, Binyang and Barroso, Ramón J. Durán and Hussain, Walayat and Xu, Yueshen and Wang, Xinheng},
	month = sep,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Network Science and Engineering},
	keywords = {Anomaly detection, Complexity theory, Data models, Decoding, Feature extraction, Internet of Things, Task analysis, autoencoder, memory augmentation, time-series classification},
	pages = {2978--2990},
}

@inproceedings{nivarthi_unified_2022,
	address = {Nassau, Bahamas},
	title = {Unified {Autoencoder} with {Task} {Embeddings} for {Multi}-{Task} {Learning} in {Renewable} {Power} {Forecasting}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-283-9},
	url = {https://ieeexplore.ieee.org/document/10068974/},
	doi = {10.1109/ICMLA55696.2022.00240},
	abstract = {Renewable power generation forecasts using machine learning are typically implemented as single-task learning models, where a separate model is trained for each photovoltaic or wind park. In recent years, transfer learning is gaining popularity in these systems, as it can be used to transfer the knowledge gained from source parks to a target park. However, for transferring the knowledge to a target park, there is a need to determine the most similar source park(s) among the existing parks. This similarity determination using historical power measurements is challenging when the target park has limited to no historical data samples. Therefore, we propose a simple multi-task learning architecture that initially learns a common representation of input weather features among the tasks, using a Unified Autoencoder (UAE) and then learns the task specific information utilizing a Task Embedding layer in a Neural Network (TENN). This proposed architecture, UAE-TENN, can be easily extended to new parks with or without historical data. An elaborate performance comparison of single and multi-task learning models is performed on six photovoltaic and wind farm datasets comprising a total of 529 parks. UAE-TENN significantly improves the performance of power forecasting by 10 to 19\% for photovoltaic parks and 5 to 22\% for wind parks compared to the baseline models. Even in the zero-shot learning scenario, when there is no historical data, we successfully demonstrate that the UAE-TENN improves the forecast accuracy for a new park by 19\% for photovoltaic parks.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2022 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = dec,
	year = {2022},
	pages = {1530--1536},
}

@article{zhang_unknown_2020,
	title = {Unknown {Attack} {Detection} {Based} on {Zero}-{Shot} {Learning}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9239385/},
	doi = {10.1109/ACCESS.2020.3033494},
	abstract = {In recent years, due to the frequent occurrence of network intrusions, more and more researchers have begun to focus on network intrusion detection. However, it is still a challenge to detect unknown attacks. Currently, there are two main methods of unknown attack detection: clustering and honeypot. But they still have unsolved problems such as difﬁculty in collecting unknown attack samples and failure to detect on time. Zero-Shot learning is proposed to deal with the problem in this article, which can recognize unknown attacks by learning the mapping relations between feature space and semantic space (such as attribute space). When the semantic descriptions of all attacks (including known and unknown attacks) are provided, the classiﬁer built by Zero-Shot learning can extract common semantic information among all attacks and construct connections between known and unknown attacks. The classiﬁer then utilizes the connections to classify unknown attacks although there are no samples for unknown attacks. In this article, we ﬁrst propose to use Zero-Shot learning to overcome the challenge of unknown attack detection and illustrate the feasibility of this method. Secondly, we then propose a novel method of Zero-Shot learning based on sparse autoencoder for unknown attack detection. This method maps the feature of known attacks to the semantic space, and restores the semantic space to the feature space by constrains of reconstruction error, and establishes the feature to semantic mapping, which is used to detect unknown attacks. Veriﬁcation tests have been carried out by using the public dataset NSL\_KDD. From the experiments conducted in this work, the results show that the average accuracy reaches 88.3\%, which performs better than other methods.},
	language = {en},
	urldate = {2024-08-25},
	journal = {IEEE Access},
	author = {Zhang, Zhun and Liu, Qihe and Qiu, Shilin and Zhou, Shijie and Zhang, Cheng},
	year = {2020},
	pages = {193981--193991},
}

@misc{sun_unraveling_2023,
	title = {Unraveling the "{Anomaly}" in {Time} {Series} {Anomaly} {Detection}: {A} {Self}-supervised {Tri}-domain {Solution}},
	shorttitle = {Unraveling the "{Anomaly}" in {Time} {Series} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2311.11235},
	abstract = {The ongoing challenges in time series anomaly detection (TSAD), including the scarcity of anomaly labels and the variability in anomaly lengths and shapes, have led to the need for a more robust and efficient solution. As limited anomaly labels hinder traditional supervised models in anomaly detection, various state-of-the-art (SOTA) deep learning techniques (e.g., self-supervised learning) are introduced to tackle this issue. However, they encounter difficulties handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of having explicit anomalies that even random functions can detect. This problem is exacerbated by an illposed evaluation metric, known as point adjustment (PA), which results in inflated model performance. In this context, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which addresses these challenges by modeling features across three aspects - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, our approach can detect anomalies of varying lengths by integrating with a discord discovery algorithm. It is worth noting that this study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA\%K and affiliation). Experimental results demonstrate that TriAD achieves an impressive three-fold increase in PA\%K based F1 scores over SOTA deep learning models. Moreover, in comparison to SOTA discord discovery algorithms, TriAD improves anomaly detection accuracy by 50\% while cutting the inference time down to just one-tenth. Illuminating the significance of rigorous datasets and evaluation metrics, this paper offers a new direction for addressing the multifaceted challenges of time series anomaly detection. The source code is publicly available at https://github.com/pseudo-Skye/TriAD.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Sun, Yuting and Pang, Guansong and Ye, Guanhua and Chen, Tong and Hu, Xia and Yin, Hongzhi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11235 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{provotar_unsupervised_2019,
	title = {Unsupervised {Anomaly} {Detection} in {Time} {Series} {Using} {LSTM}-{Based} {Autoencoders}},
	url = {https://ieeexplore.ieee.org/document/9030505/?arnumber=9030505},
	doi = {10.1109/ATIT49449.2019.9030505},
	abstract = {Automatic anomaly detection in data mining has a wide range of applications such as fraud detection, system health monitoring, fault detection, event detection systems in sensor networks, and so on. The main challenge related to such problem is unknown nature of the anomaly. Therefore, it is impossible to use classical machine learning techniques to train the model, as we don’t have labels of time series with anomaly. For periodic time series it is advisable to use STL decomposition of the signal. In such case anomaly detection task is reduced to residuals peak detection. If time series is not periodic (for example, forex price or sound) the only way is using machine learning methods. One of the best machine learning methods is autoencoder-based anomaly detection. An autoencoder is a type of artificial neural network used to learn efficient data encodings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Unsupervised anomaly detection method based on autoencoders was tested on two types of data: various artificial signal datasets and detection of rare sound events dataset. (Abstract)},
	urldate = {2024-09-06},
	booktitle = {2019 {IEEE} {International} {Conference} on {Advanced} {Trends} in {Information} {Theory} ({ATIT})},
	author = {Provotar, Oleksandr I. and Linder, Yaroslav M. and Veres, Maksym M.},
	month = dec,
	year = {2019},
	keywords = {Anomaly detection, Biological neural networks, Machine learning, Market research, Support vector machines, Time series analysis, Training, anomaly detection, autoencoders, machine learning, neural network},
	pages = {513--517},
}

@inproceedings{gangloff_unsupervised_2023,
	address = {Kuala Lumpur, Malaysia},
	title = {Unsupervised {Anomaly} {Detection} {Using} {Variational} {Autoencoder} with {Gaussian} {Random} {Field} {Prior}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72819-835-4},
	url = {https://ieeexplore.ieee.org/document/10222900/},
	doi = {10.1109/ICIP49359.2023.10222900},
	abstract = {We propose a new model of Variational Autoencoder (VAE) for Anomaly Detection (AD) with improved modeling power. More precisely, we introduce a VAE model with a Gaussian Random Field (GRF) prior, namely VAE-GRF, which generalizes the classical VAE model. We show that, under some assumptions, the VAE-GRF largely outperforms the traditional VAE and some other probabilistic models developed for AD. Our experimental results suggest that the VAE-GRF could be used as a relevant VAE baseline in place of the traditional VAE with very limited additional computational cost. We provide competitive results on the public MVTec benchmark dataset for visual inspection, as well as on the public Livestock dataset dedicated to the task of unsupervised animal detection from aerial images.},
	language = {en},
	urldate = {2024-07-15},
	booktitle = {2023 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Gangloff, Hugo and Pham, Minh-Tan and Courtrai, Luc and Lefèvre, Sébastien},
	month = oct,
	year = {2023},
	pages = {1620--1624},
}

@inproceedings{duan_unsupervised_2021,
	address = {Shanghai, China},
	title = {Unsupervised {Time} {Series} {Anomaly} {Detection} using {Moving} {Memorial} {Dynamic} {Filter}},
	isbn = {978-988-15638-0-4},
	url = {https://ieeexplore.ieee.org/document/9549428/},
	doi = {10.23919/CCC52363.2021.9549428},
	abstract = {Anomaly detection on time series data is something of great importance nowadays. From the unsupervised machine learning method like K-Means to newly developed artificial neural networks, people try very hard to give a precise detection of where the anomalies locate. In this paper, a new method, the moving memorial dynamic filter, which combines Fourier seasonality decomposition, distance transformation and anomaly determination is introduced. First, the seasonal component is split off based on Fourier transformation. Then the time series data is mapped to a center-to-center distance subsequence using a dynamic circumscribed circle filter, which could use a low dimensional series to represent high dimensional information. At last, moving memorial value at risk is introduced and serves as a threshold to determine whether a certain data point should be labeled as an anomaly. Our method is tested and evaluated using the Numenta Anomaly Benchmark, an open-sourced dataset for time series anomaly detection. The result shows that our method outperforms the competing algorithms in terms of accuracy on most profiles of the NAB with a shorter latency in time consumption.},
	language = {en},
	urldate = {2024-09-08},
	booktitle = {2021 40th {Chinese} {Control} {Conference} ({CCC})},
	publisher = {IEEE},
	author = {Duan, Jufang and Xu, Xiangyang and Wang, Yi},
	month = jul,
	year = {2021},
	pages = {3403--3408},
}

@article{kutbi_zero-shot_2021,
	title = {Zero-shot {Deep} {Domain} {Adaptation} with {Common} {Representation} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9361131/},
	doi = {10.1109/TPAMI.2021.3061204},
	abstract = {Domain Adaptation aims at adapting the knowledge learned from a domain (source-domain) to another (target-domain). Existing approaches typically require a portion of task-relevant target-domain data a priori. We propose an approach, zero-shot deep domain adaptation (ZDDA), which uses paired dual-domain task-irrelevant data to eliminate the need for task-relevant target-domain training data. ZDDA learns to generate common representations for source and target domains data. Then, either domain representation is used later to train a system that works on both domains or having the ability to eliminate the need to either domain in sensor fusion settings. Two variants of ZDDA have been developed: ZDDA for classiﬁcation task (ZDDA-C) and ZDDA for metric learning task (ZDDAML). Another limitation in Existing approaches is that most of them are designed for the closed-set classiﬁcation task, i.e., the sets of classes in both the source and target domains are “known.” However, ZDDA-C is also applicable to the open-set classiﬁcation task where not all classes are “known” during training. Moreover, the effectiveness of ZDDA-ML shows ZDDA’s applicability is not limited to classiﬁcation tasks. ZDDA-C and ZDDA-ML are tested on classiﬁcation and metric-learning tasks, respectively. Under most experimental conditions, ZDDA outperforms the baseline without using task-relevant target-domain-training data.},
	language = {en},
	urldate = {2024-06-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kutbi, Mohammed and Peng, Kuan-Chuan and Wu, Ziyan},
	year = {2021},
	pages = {1--1},
}

@misc{socher_zero-shot_2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	url = {http://arxiv.org/abs/1301.3666},
	abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by ﬁrst using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually deﬁned semantic features for either words or images.},
	language = {en},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
	month = mar,
	year = {2013},
	note = {arXiv:1301.3666 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{palatucci_zero-shot_2009,
	title = {Zero-shot {Learning} with {Semantic} {Output} {Codes}},
	abstract = {We consider the problem of zero-shot learning, where the goal is to learn a classiﬁer f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we deﬁne the notion of a semantic output code classiﬁer (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classiﬁer and study its theoretical properties in a PAC framework, showing conditions under which the classiﬁer can accurately predict novel classes. As a case study, we build a SOC classiﬁer for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.},
	language = {en},
	author = {Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
	year = {2009},
}

@inproceedings{aota_zero-shot_2023,
	address = {Waikoloa, HI, USA},
	title = {Zero-shot versus {Many}-shot: {Unsupervised} {Texture} {Anomaly} {Detection}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-346-8},
	shorttitle = {Zero-shot versus {Many}-shot},
	url = {https://ieeexplore.ieee.org/document/10030870/},
	doi = {10.1109/WACV56688.2023.00552},
	abstract = {Research on unsupervised anomaly detection (AD) has recently progressed, significantly increasing detection accuracy. This paper focuses on texture images and considers how few normal samples are needed for accurate AD. We first highlight the critical nature of the problem that previous studies have overlooked: accurate detection gets harder for anisotropic textures when image orientations are not aligned between inputs and normal samples. We then propose a zero-shot method, which detects anomalies without using a normal sample. The method is free from the issue of unaligned orientation between input and normal images. It assumes the input texture to be homogeneous, detecting image regions that break the homogeneity as anomalies. We present a quantitative criterion to judge whether this assumption holds for an input texture. Experimental results show the broad applicability of the proposed zero-shot method and its good performance comparable to or even higher than the state-of-the-art methods using hundreds of normal samples. The code and data are available from https://drive.google.com/drive/folders/ 10OyPzvI3H6llCZBxKxFlKWt1Pw1tkMK1.},
	language = {en},
	urldate = {2024-08-01},
	booktitle = {2023 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Aota, Toshimichi and Tong, Lloyd Teh Tzer and Okatani, Takayuki},
	month = jan,
	year = {2023},
	pages = {5553--5561},
}
