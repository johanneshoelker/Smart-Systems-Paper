
@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	file = {Vaswani et al. - Attention is All you Need.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/LKH78TLW/Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@article{nivarthi_multi-task_2023,
	title = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}: {A} {Comparative} {Analysis} of {Unified} {Autoencoder} {Variants} and {Task}-{Embedding} {Dimensions}},
	volume = {5},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2504-4990},
	shorttitle = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}},
	url = {https://www.mdpi.com/2504-4990/5/3/62},
	doi = {10.3390/make5030062},
	abstract = {Typically, renewable-power-generation forecasting using machine learning involves creating separate models for each photovoltaic or wind park, known as single-task learning models. However, transfer learning has gained popularity in recent years, as it allows for the transfer of knowledge from source parks to target parks. Nevertheless, determining the most similar source park(s) for transfer learning can be challenging, particularly when the target park has limited or no historical data samples. To address this issue, we propose a multi-task learning architecture that employs a Uniﬁed Autoencoder (UAE) to initially learn a common representation of input weather features among tasks and then utilizes a Task-Embedding layer in a Neural Network (TENN) to learn task-speciﬁc information. This proposed UAE-TENN architecture can be easily extended to new parks with or without historical data. We evaluate the performance of our proposed architecture and compare it to single-task learning models on six photovoltaic and wind farm datasets consisting of a total of 529 parks. Our results show that the UAE-TENN architecture signiﬁcantly improves power-forecasting performance by 10 to 19\% for photovoltaic parks and 5 to 15\% for wind parks compared to baseline models. We also demonstrate that UAE-TENN improves forecast accuracy for a new park by 19\% for photovoltaic parks, even in a zero-shot learning scenario where there is no historical data. Additionally, we propose variants of the Uniﬁed Autoencoder with convolutional and LSTM layers, compare their performance, and provide a comparison among architectures with different numbers of task-embedding dimensions. Finally, we demonstrate the utility of trained task embeddings for interpretation and visualization purposes.},
	language = {en},
	number = {3},
	urldate = {2024-06-27},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = sep,
	year = {2023},
	pages = {1214--1233},
	file = {Nivarthi et al. - 2023 - Multi-Task Representation Learning for Renewable-P.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/KTLAM734/Nivarthi et al. - 2023 - Multi-Task Representation Learning for Renewable-P.pdf:application/pdf},
}

@misc{nivarthi_transfer_2022,
	title = {Transfer {Learning} as an {Essential} {Tool} for {Digital} {Twins} in {Renewable} {Energy} {Systems}},
	url = {http://arxiv.org/abs/2203.05026},
	abstract = {Transfer learning (TL), the next frontier in machine learning (ML), has gained much popularity in recent years, due to the various challenges faced in ML, like the requirement of vast amounts of training data, expensive and time-consuming labelling processes for data samples, and long training duration for models. TL is useful in tackling these problems, as it focuses on transferring knowledge from previously solved tasks to new tasks. Digital twins and other intelligent systems need to utilise TL to use the previously gained knowledge and solve new tasks in a more self-reliant way, and to incrementally increase their knowledge base. Therefore, in this article, the critical challenges in power forecasting and anomaly detection in the context of renewable energy systems are identiﬁed, and a potential TL framework to meet these challenges is proposed. This article also proposes a feature embedding approach to handle the missing sensors data. The proposed TL methods help to make a system more autonomous in the context of organic computing.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Nivarthi, Chandana Priya},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05026 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Nivarthi - 2022 - Transfer Learning as an Essential Tool for Digital.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/Y7VY76H3/Nivarthi - 2022 - Transfer Learning as an Essential Tool for Digital.pdf:application/pdf},
}

@inproceedings{nivarthi_unified_2022,
	address = {Nassau, Bahamas},
	title = {Unified {Autoencoder} with {Task} {Embeddings} for {Multi}-{Task} {Learning} in {Renewable} {Power} {Forecasting}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-283-9},
	url = {https://ieeexplore.ieee.org/document/10068974/},
	doi = {10.1109/ICMLA55696.2022.00240},
	abstract = {Renewable power generation forecasts using machine learning are typically implemented as single-task learning models, where a separate model is trained for each photovoltaic or wind park. In recent years, transfer learning is gaining popularity in these systems, as it can be used to transfer the knowledge gained from source parks to a target park. However, for transferring the knowledge to a target park, there is a need to determine the most similar source park(s) among the existing parks. This similarity determination using historical power measurements is challenging when the target park has limited to no historical data samples. Therefore, we propose a simple multi-task learning architecture that initially learns a common representation of input weather features among the tasks, using a Unified Autoencoder (UAE) and then learns the task specific information utilizing a Task Embedding layer in a Neural Network (TENN). This proposed architecture, UAE-TENN, can be easily extended to new parks with or without historical data. An elaborate performance comparison of single and multi-task learning models is performed on six photovoltaic and wind farm datasets comprising a total of 529 parks. UAE-TENN significantly improves the performance of power forecasting by 10 to 19\% for photovoltaic parks and 5 to 22\% for wind parks compared to the baseline models. Even in the zero-shot learning scenario, when there is no historical data, we successfully demonstrate that the UAE-TENN improves the forecast accuracy for a new park by 19\% for photovoltaic parks.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2022 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = dec,
	year = {2022},
	pages = {1530--1536},
	file = {Nivarthi et al. - 2022 - Unified Autoencoder with Task Embeddings for Multi.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/BLGPDV3R/Nivarthi et al. - 2022 - Unified Autoencoder with Task Embeddings for Multi.pdf:application/pdf},
}

@inproceedings{nivarthi_towards_2023,
	address = {Jacksonville, FL, USA},
	title = {Towards {Few}-{Shot} {Time} {Series} {Anomaly} {Detection} with {Temporal} {Attention} and {Dynamic} {Thresholding}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350345346},
	url = {https://ieeexplore.ieee.org/document/10459893/},
	doi = {10.1109/ICMLA58977.2023.00218},
	abstract = {Anomaly detection plays a pivotal role in diverse realworld applications such as cybersecurity, fault detection, network monitoring, predictive maintenance, and highly automated driving. However, obtaining labeled anomalous data can be a formidable challenge, especially when anomalies exhibit temporal evolution. This paper introduces LATAM (Long short-term memory Autoencoder with Temporal Attention Mechanism) for few-shot anomaly detection, with the aim of enhancing detection performance in scenarios with limited labeled anomaly data. LATAM effectively captures temporal dependencies and emphasizes signiﬁcant patterns in multivariate time series data. In our investigation, we comprehensively evaluate LATAM against other anomaly detection models, particularly assessing its capability in few-shot learning scenarios where we have minimal examples from the normal class and none from the anomalous class in the training data. Our experimental results, derived from real-world photovoltaic inverter data, highlight LATAM’s superiority, showcasing a substantial 27\% mean F1 score improvement, even when trained on a mere two-week dataset. Furthermore, LATAM demonstrates remarkable results on the open-source SWaT dataset, achieving a 12\% boost in accuracy with only two days of training data. Moreover, we introduce a simple yet effective dynamic thresholding mechanism, further enhancing the anomaly detection capabilities of LATAM. This underscores LATAM’s efﬁcacy in addressing the challenges posed by limited labeled anomalies in practical scenarios and it proves valuable for downstream tasks involving temporal representation and time series prediction, extending its utility beyond anomaly detection applications.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2023 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Sick, Bernhard},
	month = dec,
	year = {2023},
	pages = {1444--1450},
	file = {Nivarthi und Sick - 2023 - Towards Few-Shot Time Series Anomaly Detection wit.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/DXMT68I3/Nivarthi und Sick - 2023 - Towards Few-Shot Time Series Anomaly Detection wit.pdf:application/pdf},
}

@misc{schwartz_maeday_2024,
	title = {{MAEDAY}: {MAE} for few and zero shot {AnomalY}-{Detection}},
	shorttitle = {{MAEDAY}},
	url = {http://arxiv.org/abs/2211.14307},
	abstract = {We propose using Masked Auto-Encoder (MAE), a transformer model self-supervisedly trained on image inpainting, for anomaly detection (AD). Assuming anomalous regions are harder to reconstruct compared with normal regions. MAEDAY is the first image-reconstruction-based anomaly detection method that utilizes a pre-trained model, enabling its use for Few-Shot Anomaly Detection (FSAD). We also show the same method works surprisingly well for the novel tasks of Zero-Shot AD (ZSAD) and Zero-Shot Foreign Object Detection (ZSFOD), where no normal samples are available.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Schwartz, Eli and Arbelle, Assaf and Karlinsky, Leonid and Harary, Sivan and Scheidegger, Florian and Doveh, Sivan and Giryes, Raja},
	month = feb,
	year = {2024},
	note = {arXiv:2211.14307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Schwartz et al. - 2024 - MAEDAY MAE for few and zero shot AnomalY-Detectio.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/6K3S2KM6/Schwartz et al. - 2024 - MAEDAY MAE for few and zero shot AnomalY-Detectio.pdf:application/pdf},
}

@article{kutbi_zero-shot_2021,
	title = {Zero-shot {Deep} {Domain} {Adaptation} with {Common} {Representation} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9361131/},
	doi = {10.1109/TPAMI.2021.3061204},
	abstract = {Domain Adaptation aims at adapting the knowledge learned from a domain (source-domain) to another (target-domain). Existing approaches typically require a portion of task-relevant target-domain data a priori. We propose an approach, zero-shot deep domain adaptation (ZDDA), which uses paired dual-domain task-irrelevant data to eliminate the need for task-relevant target-domain training data. ZDDA learns to generate common representations for source and target domains data. Then, either domain representation is used later to train a system that works on both domains or having the ability to eliminate the need to either domain in sensor fusion settings. Two variants of ZDDA have been developed: ZDDA for classiﬁcation task (ZDDA-C) and ZDDA for metric learning task (ZDDAML). Another limitation in Existing approaches is that most of them are designed for the closed-set classiﬁcation task, i.e., the sets of classes in both the source and target domains are “known.” However, ZDDA-C is also applicable to the open-set classiﬁcation task where not all classes are “known” during training. Moreover, the effectiveness of ZDDA-ML shows ZDDA’s applicability is not limited to classiﬁcation tasks. ZDDA-C and ZDDA-ML are tested on classiﬁcation and metric-learning tasks, respectively. Under most experimental conditions, ZDDA outperforms the baseline without using task-relevant target-domain-training data.},
	language = {en},
	urldate = {2024-06-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kutbi, Mohammed and Peng, Kuan-Chuan and Wu, Ziyan},
	year = {2021},
	pages = {1--1},
	file = {Kutbi et al. - 2021 - Zero-shot Deep Domain Adaptation with Common Repre.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/EN22WD8C/Kutbi et al. - 2021 - Zero-shot Deep Domain Adaptation with Common Repre.pdf:application/pdf},
}

@article{zhuang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	volume = {109},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9134370/},
	doi = {10.1109/JPROC.2020.3004555},
	language = {en},
	number = {1},
	urldate = {2024-07-03},
	journal = {Proceedings of the IEEE},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jan,
	year = {2021},
	pages = {43--76},
	file = {Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/4PCQVBQQ/Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf:application/pdf},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a deﬁnition of MTL and then classify different MTL algorithms into ﬁve categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	language = {en},
	number = {12},
	urldate = {2024-07-03},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	pages = {5586--5609},
	file = {Zhang und Yang - 2022 - A Survey on Multi-Task Learning.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/T4XAIZXR/Zhang und Yang - 2022 - A Survey on Multi-Task Learning.pdf:application/pdf},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292},
	shorttitle = {Representation {Learning}},
	url = {http://ieeexplore.ieee.org/document/6472238/},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	language = {en},
	number = {8},
	urldate = {2024-07-03},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Y. and Courville, A. and Vincent, P.},
	month = aug,
	year = {2013},
	pages = {1798--1828},
	file = {Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/3XCB43PP/Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf:application/pdf},
}

@book{lavrac_representation_2021,
	address = {Cham},
	title = {Representation {Learning}: {Propositionalization} and {Embeddings}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-030-68816-5 978-3-030-68817-2},
	shorttitle = {Representation {Learning}},
	url = {https://link.springer.com/10.1007/978-3-030-68817-2},
	language = {en},
	urldate = {2024-07-09},
	publisher = {Springer International Publishing},
	author = {Lavrač, Nada and Podpečan, Vid and Robnik-Šikonja, Marko},
	year = {2021},
	doi = {10.1007/978-3-030-68817-2},
	file = {Lavrač et al. - 2021 - Representation Learning Propositionalization and .pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/PDJ5CR72/Lavrač et al. - 2021 - Representation Learning Propositionalization and .pdf:application/pdf},
}

@article{zhang_debiased_2024,
	title = {Debiased {Contrastive} {Learning} for {Time}-{Series} {Representation} {Learning} and {Fault} {Detection}},
	volume = {20},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/10443248/},
	doi = {10.1109/TII.2024.3359409},
	abstract = {Building reliable fault detection systems through deep neural networks is an appealing topic in industrial scenarios. In these contexts, the representations extracted by neural networks on available labeled timeseries data can reﬂect system states. However, this endeavor remains challenging due to the necessity of labeled data. Self-supervised contrastive learning (SSCL) is one of the effective approaches to deal with this challenge, but existing SSCL-based models suffer from sampling bias and representation bias problems. This article introduces a debiased contrastive learning framework for time-series data and applies it to industrial fault detection tasks. This framework ﬁrst develops the multigranularity augmented view generation method to generate augmented views at different granularities. It then introduces the momentum clustering contrastive learning strategy and the expert knowledge guidance mechanism to mitigate sampling bias and representation bias, respectively. Finally, the experiments on a public bearing fault detection dataset and a widely used valve stiction detection dataset show the effectiveness of the proposed feature learning framework.},
	language = {en},
	number = {5},
	urldate = {2024-07-09},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Zhang, Kexin and Cai, Rongyao and Zhou, Chunlin and Liu, Yong},
	month = may,
	year = {2024},
	pages = {7641--7653},
	file = {Zhang et al. - 2024 - Debiased Contrastive Learning for Time-Series Repr.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/PR3HC2LC/Zhang et al. - 2024 - Debiased Contrastive Learning for Time-Series Repr.pdf:application/pdf},
}

@article{zhong_overview_2016,
	title = {An overview on data representation learning: {From} traditional feature learning to recent deep learning},
	volume = {2},
	issn = {2405-9188},
	shorttitle = {An overview on data representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S2405918816300459},
	doi = {10.1016/j.jfds.2017.05.001},
	abstract = {Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, either linear or nonlinear, either supervised or unsupervised, either “shallow” or “deep”. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available online resources (e.g., courses, tutorials and books) and toolboxes are provided. At the end, we give a few remarks on the development of data representation learning and suggest some interesting research directions in this area.},
	number = {4},
	urldate = {2024-07-14},
	journal = {The Journal of Finance and Data Science},
	author = {Zhong, Guoqiang and Wang, Li-Na and Ling, Xiao and Dong, Junyu},
	month = dec,
	year = {2016},
	keywords = {Deep learning, Feature learning, Representation learning},
	pages = {265--278},
	file = {Eingereichte Version:/home/jo/snap/zotero-snap/common/Zotero/storage/VZYYTIXA/Zhong et al. - 2016 - An overview on data representation learning From .pdf:application/pdf;ScienceDirect Snapshot:/home/jo/snap/zotero-snap/common/Zotero/storage/DLTLLNZL/S2405918816300459.html:text/html},
}

@article{sun_survey_2021,
	title = {A {Survey} on {Deep} {Learning} for {Data}-{Driven} {Soft} {Sensors}},
	volume = {17},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9329169/},
	doi = {10.1109/TII.2021.3053128},
	abstract = {Soft sensors are widely constructed in process industry to realize process monitoring, quality prediction, and many other important applications. With the development of hardware and software, industrial processes have embraced new characteristics, which lead to the poor performance of traditional soft sensor modeling methods. Deep learning, as a kind of data-driven approach, shows its great potential in many ﬁelds, as well as in soft sensing scenarios. After a period of development, especially in the last ﬁve years, many new issues have emerged that need to be investigated. Therefore, in this article, the necessity and signiﬁcance of deep learning for soft sensor applications are demonstrated ﬁrst by analyzing the merits of deep learning and the trends of industrial processes. Next, mainstream deep learning models, tricks, and frameworks/toolkits are summarized and discussed to help designers propel the developing progress of soft sensors. Then, existing works are reviewed and analyzed to discuss the demands and problems occurred in practical applications. Finally, outlook and conclusions are given.},
	language = {en},
	number = {9},
	urldate = {2024-07-14},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Sun, Qingqiang and Ge, Zhiqiang},
	month = sep,
	year = {2021},
	pages = {5853--5866},
	file = {Sun und Ge - 2021 - A Survey on Deep Learning for Data-Driven Soft Sen.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/A2TFT88H/Sun und Ge - 2021 - A Survey on Deep Learning for Data-Driven Soft Sen.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@misc{socher_zero-shot_2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	url = {http://arxiv.org/abs/1301.3666},
	abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by ﬁrst using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually deﬁned semantic features for either words or images.},
	language = {en},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
	month = mar,
	year = {2013},
	note = {arXiv:1301.3666 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/WSUT6K6M/Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer.pdf:application/pdf},
}

@article{palatucci_zero-shot_2009,
	title = {Zero-shot {Learning} with {Semantic} {Output} {Codes}},
	abstract = {We consider the problem of zero-shot learning, where the goal is to learn a classiﬁer f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we deﬁne the notion of a semantic output code classiﬁer (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classiﬁer and study its theoretical properties in a PAC framework, showing conditions under which the classiﬁer can accurately predict novel classes. As a case study, we build a SOC classiﬁer for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.},
	language = {en},
	author = {Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
	year = {2009},
	file = {Palatucci et al. - Zero-shot Learning with Semantic Output Codes.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/6LBXIG8N/Palatucci et al. - Zero-shot Learning with Semantic Output Codes.pdf:application/pdf},
}

@article{faerman_representation_nodate,
	title = {Representation {Learning} on {Relational} {Data}},
	language = {en},
	author = {Faerman, Evgeniy},
	file = {Faerman - Representation Learning on Relational Data.pdf:/home/jo/snap/zotero-snap/common/Zotero/storage/XPKMZ2SD/Faerman - Representation Learning on Relational Data.pdf:application/pdf},
}
