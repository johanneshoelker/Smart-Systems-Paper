
@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	file = {Vaswani et al. - Attention is All you Need.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\LKH78TLW\\Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@misc{schwartz_maeday_2024,
	title = {{MAEDAY}: {MAE} for few and zero shot {AnomalY}-{Detection}},
	shorttitle = {{MAEDAY}},
	url = {http://arxiv.org/abs/2211.14307},
	abstract = {We propose using Masked Auto-Encoder (MAE), a transformer model self-supervisedly trained on image inpainting, for anomaly detection (AD). Assuming anomalous regions are harder to reconstruct compared with normal regions. MAEDAY is the first image-reconstruction-based anomaly detection method that utilizes a pre-trained model, enabling its use for Few-Shot Anomaly Detection (FSAD). We also show the same method works surprisingly well for the novel tasks of Zero-Shot AD (ZSAD) and Zero-Shot Foreign Object Detection (ZSFOD), where no normal samples are available.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Schwartz, Eli and Arbelle, Assaf and Karlinsky, Leonid and Harary, Sivan and Scheidegger, Florian and Doveh, Sivan and Giryes, Raja},
	month = feb,
	year = {2024},
	note = {arXiv:2211.14307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Schwartz et al. - 2024 - MAEDAY MAE for few and zero shot AnomalY-Detectio.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\6K3S2KM6\\Schwartz et al. - 2024 - MAEDAY MAE for few and zero shot AnomalY-Detectio.pdf:application/pdf},
}

@inproceedings{nivarthi_towards_2023,
	address = {Jacksonville, FL, USA},
	title = {Towards {Few}-{Shot} {Time} {Series} {Anomaly} {Detection} with {Temporal} {Attention} and {Dynamic} {Thresholding}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350345346},
	url = {https://ieeexplore.ieee.org/document/10459893/},
	doi = {10.1109/ICMLA58977.2023.00218},
	abstract = {Anomaly detection plays a pivotal role in diverse realworld applications such as cybersecurity, fault detection, network monitoring, predictive maintenance, and highly automated driving. However, obtaining labeled anomalous data can be a formidable challenge, especially when anomalies exhibit temporal evolution. This paper introduces LATAM (Long short-term memory Autoencoder with Temporal Attention Mechanism) for few-shot anomaly detection, with the aim of enhancing detection performance in scenarios with limited labeled anomaly data. LATAM effectively captures temporal dependencies and emphasizes signiﬁcant patterns in multivariate time series data. In our investigation, we comprehensively evaluate LATAM against other anomaly detection models, particularly assessing its capability in few-shot learning scenarios where we have minimal examples from the normal class and none from the anomalous class in the training data. Our experimental results, derived from real-world photovoltaic inverter data, highlight LATAM’s superiority, showcasing a substantial 27\% mean F1 score improvement, even when trained on a mere two-week dataset. Furthermore, LATAM demonstrates remarkable results on the open-source SWaT dataset, achieving a 12\% boost in accuracy with only two days of training data. Moreover, we introduce a simple yet effective dynamic thresholding mechanism, further enhancing the anomaly detection capabilities of LATAM. This underscores LATAM’s efﬁcacy in addressing the challenges posed by limited labeled anomalies in practical scenarios and it proves valuable for downstream tasks involving temporal representation and time series prediction, extending its utility beyond anomaly detection applications.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2023 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Sick, Bernhard},
	month = dec,
	year = {2023},
	pages = {1444--1450},
	file = {Nivarthi und Sick - 2023 - Towards Few-Shot Time Series Anomaly Detection wit.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\DXMT68I3\\Nivarthi und Sick - 2023 - Towards Few-Shot Time Series Anomaly Detection wit.pdf:application/pdf},
}

@inproceedings{nivarthi_unified_2022,
	address = {Nassau, Bahamas},
	title = {Unified {Autoencoder} with {Task} {Embeddings} for {Multi}-{Task} {Learning} in {Renewable} {Power} {Forecasting}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-283-9},
	url = {https://ieeexplore.ieee.org/document/10068974/},
	doi = {10.1109/ICMLA55696.2022.00240},
	abstract = {Renewable power generation forecasts using machine learning are typically implemented as single-task learning models, where a separate model is trained for each photovoltaic or wind park. In recent years, transfer learning is gaining popularity in these systems, as it can be used to transfer the knowledge gained from source parks to a target park. However, for transferring the knowledge to a target park, there is a need to determine the most similar source park(s) among the existing parks. This similarity determination using historical power measurements is challenging when the target park has limited to no historical data samples. Therefore, we propose a simple multi-task learning architecture that initially learns a common representation of input weather features among the tasks, using a Unified Autoencoder (UAE) and then learns the task specific information utilizing a Task Embedding layer in a Neural Network (TENN). This proposed architecture, UAE-TENN, can be easily extended to new parks with or without historical data. An elaborate performance comparison of single and multi-task learning models is performed on six photovoltaic and wind farm datasets comprising a total of 529 parks. UAE-TENN significantly improves the performance of power forecasting by 10 to 19\% for photovoltaic parks and 5 to 22\% for wind parks compared to the baseline models. Even in the zero-shot learning scenario, when there is no historical data, we successfully demonstrate that the UAE-TENN improves the forecast accuracy for a new park by 19\% for photovoltaic parks.},
	language = {en},
	urldate = {2024-06-27},
	booktitle = {2022 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = dec,
	year = {2022},
	pages = {1530--1536},
	file = {Nivarthi et al. - 2022 - Unified Autoencoder with Task Embeddings for Multi.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\BLGPDV3R\\Nivarthi et al. - 2022 - Unified Autoencoder with Task Embeddings for Multi.pdf:application/pdf},
}

@misc{nivarthi_transfer_2022,
	title = {Transfer {Learning} as an {Essential} {Tool} for {Digital} {Twins} in {Renewable} {Energy} {Systems}},
	url = {http://arxiv.org/abs/2203.05026},
	abstract = {Transfer learning (TL), the next frontier in machine learning (ML), has gained much popularity in recent years, due to the various challenges faced in ML, like the requirement of vast amounts of training data, expensive and time-consuming labelling processes for data samples, and long training duration for models. TL is useful in tackling these problems, as it focuses on transferring knowledge from previously solved tasks to new tasks. Digital twins and other intelligent systems need to utilise TL to use the previously gained knowledge and solve new tasks in a more self-reliant way, and to incrementally increase their knowledge base. Therefore, in this article, the critical challenges in power forecasting and anomaly detection in the context of renewable energy systems are identiﬁed, and a potential TL framework to meet these challenges is proposed. This article also proposes a feature embedding approach to handle the missing sensors data. The proposed TL methods help to make a system more autonomous in the context of organic computing.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Nivarthi, Chandana Priya},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05026 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Nivarthi - 2022 - Transfer Learning as an Essential Tool for Digital.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\Y7VY76H3\\Nivarthi - 2022 - Transfer Learning as an Essential Tool for Digital.pdf:application/pdf},
}

@article{nivarthi_multi-task_2023,
	title = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}: {A} {Comparative} {Analysis} of {Unified} {Autoencoder} {Variants} and {Task}-{Embedding} {Dimensions}},
	volume = {5},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2504-4990},
	shorttitle = {Multi-{Task} {Representation} {Learning} for {Renewable}-{Power} {Forecasting}},
	url = {https://www.mdpi.com/2504-4990/5/3/62},
	doi = {10.3390/make5030062},
	abstract = {Typically, renewable-power-generation forecasting using machine learning involves creating separate models for each photovoltaic or wind park, known as single-task learning models. However, transfer learning has gained popularity in recent years, as it allows for the transfer of knowledge from source parks to target parks. Nevertheless, determining the most similar source park(s) for transfer learning can be challenging, particularly when the target park has limited or no historical data samples. To address this issue, we propose a multi-task learning architecture that employs a Uniﬁed Autoencoder (UAE) to initially learn a common representation of input weather features among tasks and then utilizes a Task-Embedding layer in a Neural Network (TENN) to learn task-speciﬁc information. This proposed UAE-TENN architecture can be easily extended to new parks with or without historical data. We evaluate the performance of our proposed architecture and compare it to single-task learning models on six photovoltaic and wind farm datasets consisting of a total of 529 parks. Our results show that the UAE-TENN architecture signiﬁcantly improves power-forecasting performance by 10 to 19\% for photovoltaic parks and 5 to 15\% for wind parks compared to baseline models. We also demonstrate that UAE-TENN improves forecast accuracy for a new park by 19\% for photovoltaic parks, even in a zero-shot learning scenario where there is no historical data. Additionally, we propose variants of the Uniﬁed Autoencoder with convolutional and LSTM layers, compare their performance, and provide a comparison among architectures with different numbers of task-embedding dimensions. Finally, we demonstrate the utility of trained task embeddings for interpretation and visualization purposes.},
	language = {en},
	number = {3},
	urldate = {2024-06-27},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Nivarthi, Chandana Priya and Vogt, Stephan and Sick, Bernhard},
	month = sep,
	year = {2023},
	pages = {1214--1233},
	file = {Nivarthi et al. - 2023 - Multi-Task Representation Learning for Renewable-P.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\KTLAM734\\Nivarthi et al. - 2023 - Multi-Task Representation Learning for Renewable-P.pdf:application/pdf},
}

@article{kutbi_zero-shot_2021,
	title = {Zero-shot {Deep} {Domain} {Adaptation} with {Common} {Representation} {Learning}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9361131/},
	doi = {10.1109/TPAMI.2021.3061204},
	abstract = {Domain Adaptation aims at adapting the knowledge learned from a domain (source-domain) to another (target-domain). Existing approaches typically require a portion of task-relevant target-domain data a priori. We propose an approach, zero-shot deep domain adaptation (ZDDA), which uses paired dual-domain task-irrelevant data to eliminate the need for task-relevant target-domain training data. ZDDA learns to generate common representations for source and target domains data. Then, either domain representation is used later to train a system that works on both domains or having the ability to eliminate the need to either domain in sensor fusion settings. Two variants of ZDDA have been developed: ZDDA for classiﬁcation task (ZDDA-C) and ZDDA for metric learning task (ZDDAML). Another limitation in Existing approaches is that most of them are designed for the closed-set classiﬁcation task, i.e., the sets of classes in both the source and target domains are “known.” However, ZDDA-C is also applicable to the open-set classiﬁcation task where not all classes are “known” during training. Moreover, the effectiveness of ZDDA-ML shows ZDDA’s applicability is not limited to classiﬁcation tasks. ZDDA-C and ZDDA-ML are tested on classiﬁcation and metric-learning tasks, respectively. Under most experimental conditions, ZDDA outperforms the baseline without using task-relevant target-domain-training data.},
	language = {en},
	urldate = {2024-06-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kutbi, Mohammed and Peng, Kuan-Chuan and Wu, Ziyan},
	year = {2021},
	pages = {1--1},
	file = {Kutbi et al. - 2021 - Zero-shot Deep Domain Adaptation with Common Repre.pdf:C\:\\Users\\HOELKER\\Zotero\\storage\\EN22WD8C\\Kutbi et al. - 2021 - Zero-shot Deep Domain Adaptation with Common Repre.pdf:application/pdf},
}
